{"componentChunkName":"component---src-templates-blog-post-js","path":"/mukund","result":{"data":{"markdownRemark":{"html":"<h1>A Gentle Introduction to Word Embeddings</h1>\n<h2>Fun with Natural  Language Processing’s “Secret Sauce“</h2>\n<p>Computers don’t understand the nuances of language. It’s because they only understand numbers and, as you can imagine, it’s impossible for us to enumerate every single nuance in understanding human language (let alone as numbers). As a result, some might say that even humans don’t understand language well! But, we’ve seen a lot of progress in recent years of computers understanding language. So how do these work?  More specifically, how are these system representing words as <em>numbers</em>? </p>\n<p>In this post, we’re going to talk about one of the biggest advances in natural language processing: <em>word embeddings</em>.</p>\n<h2>Representation</h2>\n<p>So how do we represent one of the most basic units of natural language, <strong>words</strong>? If this is the first time you’ve thought about this, you might be tempted to say the appropriate data structure is obviously a string! However, this comes with some design considerations:</p>\n<ol>\n<li>Your compiler doesn’t understand the contents of the string - it just recognizes the ASCII values that correspond to each letter. This representation is as useful to us as recognizing the word <code>cat</code> as being <code>[3, 1, 20]</code> - not very useful.</li>\n<li>Strings can have a variable length. If we wanted to give a machine learning model a string, it might truncate the ending values for consistency’s sake and we would lose important information.</li>\n</ol>\n<p>For a long time, one of best approaches we could do was representing a word as an array with all 0s that had 1 in the index corresponding to a specific word. For example, in the vocabulary <code>{\"I\", \"am\", \"a\", \"cat\"}</code>, the word <code>I</code> would correspond to <code>[1, 0, 0, 0]</code>, the word <code>am</code> as <code>[0, 1, 0, 0]</code>, and so forth. </p>\n<h3>Better Representation</h3>\n<p>So what can we do? Turns out, distributional semantics holds the key:</p>\n<blockquote>\n<p>You shall know a word by the company it keeps <a href=\"#\">Firth, J. R. 1957:11</a></p>\n</blockquote>\n<p>In essence, we should be defining words in relation to other words. Intuitively, this makes sense. If I asked you to describe the word “avocado”, you would probably define it in terms of other words like “fruit” and “green”. </p>\n<p>This idea of word similarity gives rise to the notion of placing our words in <em>vector spaces</em>. This may seem scary, but in essence want to mathematically determine the “closeness” between two words. We could define a one dimensional scalar value named <code>distance</code> that tracks this, but the nuances of language extend far beyond a single dimension. That’s why we place them in higher order dimensions - typically in the tens or hundreds. Our brains can’t really imagine dimensions higher than three, so for the sake of visualization we’ll work in two dimensions for now: the classic x-y plane.</p>\n<p>Imagine we have the string <code>“cat”</code>. Let’s say we have a perfect word embedding fairy that gives us the vector <code>[1, 4]</code> to represent the word <strong>cat</strong>.</p>\n<h2>Word Embeddings</h2>\n<p>Why do we want to represent a three-letter word as a vector with potentially a vector of hundreds of values? In short, we want to create the embeddings such that the vectors <strong>capture the meaning of a given word</strong>. This can intuitively be visualized as the vectors for similar words being group together. For example, if the vector for <code>\"cat\"</code> is <code>[1, 4]</code>, the vector for <code>\"kitten\"</code> would be something like <code>[2, 4]</code> whereas the vector for <code>\"dog\"</code> would be close by, for example <code>[1, 5]</code>. </p>\n<p>Since words are now vectors, we are also able to perform linear algebra operations on the given language. Although it may feel weird to subtract <code>dog</code> from <code>cat</code>, it turns out performing such operations tends to be useful for a variety of tasks. Calculating the cosine distance (which encodes similarity) between two words is a powerful feature that makes tasks involving natural language a lot easier.</p>\n<p>As a result, something interesting we can do is train our word embeddings to create analogies. For example, a classic example in the field is using word embeddings to see that  <code>\"king\" - \"man\" = \"queen\" - \"woman\"</code>.  We can even generalize this to fill-in-the-blanks for sentences like <code>“Bill Gates is to Microsoft as Steve Jobs is to _____”</code> by predicting <code>Apple</code>.</p>\n<p>While these ad-hoc analyses are interesting to think about, the real use of word embeddings is to provide computers a semantically-aware representation of words. This is done by providing the word embeddings as <em>features</em> to a neural network that performs other “downstream“ tasks. For example, providing word embeddings to a neural network that powers a chatbot will let it generate sentences that make more sense than if we represented words using a string-to-index mapping.</p>\n<h3>How Do We Create Word Embeddings?</h3>\n<p>It seems like perfect word embeddings are too specific and are therefore good to be true. However, we can actually create very powerful word embeddings that capture a lot of semantic meaning using neural networks. Why neural networks? As we’ll see, one previous method of word embedding generation was to perform dimensionality reduction on word co-occurrence matrices (which doesn’t involve deep learning). This procedure captures the intuition behind distributional semantics, but doesn’t have the powerful non-linearity capabilities of neural networks. However, it’s still useful to think about since certain methods of generation word embeddings draw upon this as reference.</p>\n<p>However, I would actually like you to forget about pre-neural network methods, for now. It turns out that NLP can be implemented “from scratch“, i.e. purely through statistical and neural means.</p>\n<p>A commonly used implementation to generate word embeddings is <code>word2vec</code>, which is what we will use as reference in this guide.<code>word2vec</code> generates word embeddings through one of two related models. Both models are be trained using different objectives and as such, we can build two simple neural networks that performs the following tasks:</p>\n<ol>\n<li><strong>Continuous Bag Of Word</strong>: predicts a given missing word in a sentence/phrase based on context (faster but less specific)</li>\n<li><strong>Skip Gram</strong>: given a word, predicts the words that will co-appear near it (slower but works better for infrequent words)</li>\n</ol>\n<p>If you notice, they are in essence the inverse of the other. This is good for our intuition of how <code>word2vec</code> works to generate word embeddings as both are really good examples of the <em>distributional hypothesis</em> from earlier!</p>\n<p>You might be wondering: how do we get the word vectors from this process? Turns out the task we’re making the neural network do is a <em>fake task</em> that we training the network off of - we actually won’t use the model that’s trained. Instead, the goodies are encoded in the parameters of the neural network layers: the weights and biases of each neuron.</p>\n<h3>Implementation</h3>\n<p>Training word embeddings with a given dataset is easy using <code>gensim</code>, a Python package that abstracts the implementation of the <code>word2vec</code> neural network. This is the most commonly used Python package for generating word embeddings. </p>\n<p>Some example code:</p>\n<blockquote>\n<p>todo</p>\n</blockquote>\n<h2>Using Word Embeddings</h2>\n<p>Let’s use <strong>pre-trained word embeddings</strong> from Google (trained by reading through Google News). Using trusted pre-trained models will allow us to quickly play with word vectors as well as prototype with deep learning faster since such models already been worked well in practice.</p>\n<blockquote>\n<p>todo </p>\n</blockquote>\n<h3>Extra: Visualizing Word Embeddings</h3>\n<p>It would be cool to visualize the word vectors. Sadly, we humans are mostly incapable of visualizing in the 300th dimension.</p>\n<p>Instead, we can use a process called <strong>dimensionality reduction</strong> which will allow us to turn our 300 dimensions into regular 2D vectors (without losing too much information) that we can visualize.</p>\n<h3>Extra: Gensim Compatibility with Gensim</h3>\n<p>A lot of natural language processing might use the <code>gensim</code> package, which has a different API as the faster <code>pymagnitude</code> package we’ve been using. In order to interface with the <code>pymagnitude</code> model, we can write a wrapper class to use the same API as the <code>gensim</code> model:</p>\n<pre><code class=\"language-python\">class Word2Vec:\n    def __init__(self, vectors):\n        self.vectors = vectors\n        self.layer1_size = self.vectors.dim\n\n    def __getitem__(self, word):\n        return self.vectors.query(word)\n    \n    def __contains__(self, word):\n        return word in self.vectors\n    \n    def dim(self):\n        return self.vectors.dim \n</code></pre>\n<p>Using this, we can wrap our magnitude model as follows:</p>\n<pre><code class=\"language-python\">vectors = Magnitude('vectors.magnitude')\nw2v = Word2Vec(vectors)\n</code></pre>\n<p>And we can access the vector exactly the same as we would with <code>gensim</code> as follows:</p>\n<pre><code class=\"language-python\">cat_vector = w2v['cat']\n</code></pre>\n<p>This should resolve a lot of compatibility issues if you choose to leverage faster Magnitude embeddings with an existing Gensim codebase. </p>\n<h2>Epilogue: why do I think this is cool?</h2>\n<p>I ready about word embeddings sometime during my freshman year of university. I’m not really sure where I learned about it, but I found the idea really enchanting.</p>\n<p>Word embeddings are a good introduction to neural networks as well as computational linguistics, and it’s what eventually put me on a path through machine learning academia - which has made me a better developer and computer scientist. I decided to pay homage by writing this tutorial. I know lots of good resources about word embeddings exist already, but I wanted to help introduce other burgeoning computer scientists to the wonders of NLP!</p>","frontmatter":{"title":"mukund reads my blog"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"pathSlug":"/mukund"}}}