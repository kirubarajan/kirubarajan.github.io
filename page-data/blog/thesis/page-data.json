{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/thesis","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n<div class=\"notification is-link\">\n  This post will be under construction until I graduate, and is subject to (probably a lot of) change. For any errata, feel free to reach out to me!\n</div>\n\n# My Undergraduate Thesis\n> Extracting Knowledge From Transformer Language Models\n\nAfter (almost) 4 long years at the University of Pennsylvania, I'm finally a senior approaching my final few weeks as a student. Sentiments aside, this means that I'm also wrapping up a lot of the research on machine learning and natural language processing I've done during my time at Penn. For my program, this culminates in a senior thesis (on a topic of my choosing) during my final semester. A single semester isn't typically enough time to make a significant contribution to the field, so I'm hoping to use this thesis as an opportunity to explore new areas of machine intelligence. I'm really excited to shift some of my focus from my previous research from generative models (i.e. language models) towards knowledge representation.\n\nI'll be using this part of my website as a public workspace for my foray into **information extraction, knowledge graph construction, and machine reasoning**. This will probably include (but isn't limited to) code snippets, equations/derivations, background reading, literature review, and personal hot takes.\n\n---\n\n## Introduction\n\nIn recent years, there has been an explosion of data of various formats (e.g. video, text, sensor), which are not always immediately useful for completing tasks. An overarching goal in the field of machine intelligence is the problem of **knowledge representation**, a field dedicated to designing *efficient* representations of data that capture their respective data.\n\nThis problem is not just a question of information theory, but also relies on biological inspiration. When collecting new information, humans are able to make connections between related data, which enables not only efficient recall but also provides a framework for synthesizing new thoughts or beliefs. Providing such a framework for machines to perform such types of reasoning over previously given data is a goal for the field and, in turn, this paper.\n\nThe approach that this paper is primarily interested in is the **knowledge graph** model, which models real life (or abstract) entities and their relationships as a graph $G = (V, E)$. Google aptly describes this approach as \"things, not strings\", referring to the notion that these graphs are often mined from pure language but contain a far more powerful semantic meaning.\n\nIn this paper, we explore various knowledge graph construction methods, both from their theoretical merits, as well as their empirical performance provided real natural language data as source material. We also aim to provide an unopinionated framework for expressing declarative knowledge over mined knowledge graphs to aid in logical reasoning. In addition, an engineering goal for this thesis is to make such knowledge graphs accessible to the general public, by offering an open source front-end application that is self-hostable for researchers.\n\n## Background\n\n### Natural Language Processing\nWith the recent implosion of textual data as well as advances in machine learning, the field of natural language processing has been galvanized like few other disciplines in computer science. Natural language processing is the computational task of processing and analyzing text written in natural language. This manifests itself as being able to read and write human language with strong confidence and understanding of the language. Natural language processing is an umbrella term used to describe the field, and includes a variety of subtasks such as speech recognition, text generation, and information extraction. The computational challenge of natural language processing comes from the lack of structure within language. As a result, text can often be ambiguous or unclear regarding how to interpet its higher-level semantics. For example, the statement \"Call me a taxi, please\" would be difficult to develop an algorithm to parse, since it's unclear (arguably even to humans) whether or not the intent is to hail a taxi or to be referred to as a taxi. Alan Turing, in his paper [Computing Machinery and Intelligence](), hails natural language understanding as a criterion of intelligence, and a plethora of research has been devoted to analyzing the requirement (e.g. [Dugan et. al](https://roft.io)).\n\nOriginally, natural language processing involved the symbolic manupulation of language, which lacks semantic understanding (a necessary component of true artificial intelligence). Regarding machine translation (one of NLP's most coveted subtasks), [The Chinese Room]() philosophical thought experiment suggests that such symbolic systems arguably weren't capable of true understanding. During this era of NLP, most approaches involved some form of human guidance in addition to a strong reliance on Chomskyan theories of linguistics (e.g. formal languages, context-free grammars etc.). In the 1980s to 1990s, these human guided efforts were superceded by statistical NLP, where data and machine learning enabled results unseen before. Datasets such as multilingual corpora from Canadian parliament hearings for translating between English and French, as well as the Penn Treebank dataset for statistical parsing ushered in a new paradigm for language understanding that drifted from its original linguistic roots. In recent years, the current state-of-the-art approaches for natural language processing ubiquitously involve neural networks and representation learning. The deep learning paradigm offers impressive results on a variety of NLP tasks; however, the lack of transparency for these models is prone to criticism, and can often make it difficult to deploy such models to real-world scenarios due to unpredictability. \n\n### Machine Learning\n\nIn recent years, researchers have made incredible progress towards the goal of artificial intelligence through the field of machine learning. For tasks that cannot be completed with imperative rules (e.g. image classification, text generation), traditional algorithms and software fail to produce adequate results. Machine learning serves to remedy this by automatically extracting such patterns and rules from data, and applying them at algorithm runtime to make decisions.\n\n<br />\n\n![](https://drek4537l1klr.cloudfront.net/chollet2/v-3/Figures/ch01-a-new-programming-paradigm.png)\n\n<small> Source: François Chollet </small>\n\n<br />\n\nAlthough algorithms to make predictions from data have existed for many decades, the recent explosion in machine learning progress can be attributed to two key developments: more computing resources and more data. The first development is a byproduct of both Moore's Law, as well as readily available compute being made more prevelant by cloud providers such as Amazon's AWS or Microsoft's Azure. The latter development in machine learning progress is partly brought about by the former (i.e. more storage availability for big data), but is also made possible by an increased number of devices connected to the internet, providing troves of data to analyze.\n\nRegarding this paper on knowledge graphs, machine learning has made it possible to automatically mine such graphical representations, instead of hand-annotating potentially billions of entities and their relations (as is the case with Google's Knowledge Graph, presumably the largest in existence).\n\n#### Neural Networks\n\nThe most prevalent model in the machine learning renaissance is the **neural network**, which also goes by the name of **deep learning**. As the name implies, neural networks are machine learning models with a biological inspiration of neurons in animal brains. In particular, neurons can fire in accordance with other \"similarly wired\" neurons, where the strength between neurons (or synapses) can be modelled using continuous weights. \n\nIn **feed-forward** neural networks (pictured below), we can often stack neurons in order to create a longer synapse path between the input layer and the output layer, thus creating the need for additional learnable model parameters. This increase in model capacity provides representational power to the neural network, thus giving the name **deep learning** (due to networks with hidden layers being \"deep\").\n\n![](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png)\n<small> Source: IBM </small>\n\nNeural networks offer a primary advantage of not requiring hand-picked or discrete features during inference. This is especially important when working when modelling data types that do not have explicitly identifiable features, such as language or image data. The downside to this power is that neural networks require orders of magnitude more training data than classical machine learning models (such as random forests or linear regression).\n\nNeural Network parameters are generally stochastically optimized using **gradient descent**. The process for doing this is to first define an auxillary **loss function** that measures the \"correctness\" of the model's output with respect to some training data, known as a *forward pass*. Next, the gradients of the model parameters with respect to the loss function are computed using the chain rule of calculus in a dynamic programming algorithm named **backpropagation**, also known as the *backward pass*. Finally, the model parameters are updated in accordance with their gradients in order to minimize the loss function. The stochasticity in the optimization process arrives from sampling a **batch** of training examples at a time in order to leverage hardware parallelization (particularly in GPUs). \n\nThe gradient descent update rule is given by:\n\n$$\nw_{t + 1} = w_t - \\alpha \\nabla w_t\n$$  \n\nwhere $\\alpha$ is the **learning rate**, often an integer in the range [0.01, 0.05].\n\n### Transformers\n\nThe current state-of-the-art in various natural language processing tasks (particularly related to information extraction and text generation) are **transformer** models, which was published by Vatswani et. al in 2017. These models are largely extensions of feed-forward neural networks, and mark a departure from the previous generation of language models that were variations of **recurrent neural networks**. Due to their feed-forward nature, transformer networks are highly parallelizable, and thereby making it feasible to train of vast quantities of language data. As a result, these models have achieved widespread state-of-the-art results on many tasks in natural language processing, often achieving human-level performance.\n\n![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n<small> Source: Jay Allamar </small>\n\nA recent development in the machine learning landscape named **transfer learning** has also assisted in the rise of transformer models. Transfer learning involves utilizing features for a model of one specific problem, and re-applying them towards another problem. The intuititon behind this approach is that problems often require overlapping knowledge, and speed as well as performance boosts are possible by relying on a pre-trained model. In the natural language processing world, this has manifested through fine-tuning, where a pre-trained model is re-trained with additional examples on a more specific task, often with a smaller learning rate. Transformers are natural models to fine-tune with since they are often trained on swaths of high-quality language data with a relatively generalizable task of language modelling. \n\n#### Architecture\n\nTransformer models are sequence-to-sequence (Seq2Seq) models consisting of an encoder and a decoder that rely heavily on the **attention mechanism**. Attention aims to remedy the bottleneck in previous approaches of sequence-to-sequence tasks (e.g. chatbots, question answering, machine translation) by not having to encode the entirety of a given input sequence into a fixed context vector. Attention forgoes this by allowing a model to \"softly\" search for the parts of the source sentence that are semantically or structurally relevant to aid in the task of prediction. Mechanically, instead of the encoder passing only a single context vector to the decoder, every context vector is additionally provided to the decoder. Then, the attention decoder scores the hidden states and uses these attention-scored hidden states for prediction. Typically, these attention scores are concatenated with the hidden states in order for the decoder to make a prediction. This mechanism allows for the model to \"attend\" to the input sequence when generating the output sequence, effectively allowing the model to query for sections of the input that are relevant. Conversely, the model may also attend to the input sequence when further analyzing the input sequence itself in a process known as **self-attention**, which is a crucial mechanism for transformer networks.\n\nThe Transformer architecture is similar to traditional seq2seq architectures by virtue of having an encoder and decoder. The encoder consists of six identical neural networks, which first apply self-attention to the input sequence and is then followed by a simple feed-forward neural network, where the embedding layers use a dimensionality $d$ of 512. The decoder of Transformer models also consist of six stacked neural networks with the same architecture of the encoder, but with an additional multi-headed attention layer. The model also masks inputs beyond the current position to avoid \"looking ahead\" at the input sequence. The authors dub their particular use of attention to be \"Scaled Dot-Product Attention\" due to a scaling factor that allows for stable gradients during model which can be highly optimized using hardware acceleration. The matrix of outputs for the Scaled Dot-Product Attention layer is computed as:\n\n$$\n\\operatorname*{Attention}(Q, K, V) = \\operatorname*{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n$$\n\nwhere $Q$ and $K$ are abstractions over the attention mechanism that are applied to $V$ and $d_k$ represents the dimensionality of the embedding representations. Due to a lack of convolution or recurrence in the model architecture, information about the order of words is \"injected\" into the model's knowledge using a **positional encoding** that is incorporated into the representation of the input sequence.\n\n#### Performance\n\nA benefit of the transformer architecture is that reliance on the attention mechanism allows for faster computation in practice, which is essential when considering training sets that are sized at orders of magnitudes greater than previous state-of-the-art approaches. Whereas the runtime for convolutional neural networks (CNNs) are in the order of $O(l * d^2 * w)$ (where $l$ equals the length of the input, $d$ equals the dimensionality of the representation and $w$ represents the width of the kernel), recurrent neural networks are marginally more peformant by virtue of the lack of kernal with a runtime of $O(l*d)$. However, transformer networks are able to perform at a much faster speed due to its runtime only being quadratic in the length of the input, and not of the dimensionality with a runtime of $O(l^2 * 2)$. In recent years, the rise of increasing dimensionality for representation, whereas input sequences have remained largedly the same has proved that the transformer's approach to computational effiency is one of the reasons behind the architecture by permitting new datasets that precluded previous approaches.\n\n#### Connection to Graph Neural Networks\n\nSince the attention mechanism can be interpreted as a connected graph, with edged annotated with their various weights (or attention strengths), transformer models are closely related to neural networks that operate over graphs, known as **graph neural networks**. In particular, a challenge in graph neural networks is the requirement that nodes are encoded in a representation that captures information about the local structure, namely to derive semantic value from a given node's neighbourhood. In transformer models, this same requirement is challenge as well, since the feed-forward nature of the neural network prohibits the model from learning the temporal ordering aspect of the input words. In transformer networks, this is solved by introducing a positional encoding (often a sinusoidal function) for each token, whose embeddings are an additional input to the multiple attention heads of the transformer model. Their connection to graph neural networks would suggest that transformers would be a natural choice for incorporating graphical data into a language setting.\n\n### Language Models\n\nWe can use the chain rule in probability to break down a sequence (or sentence) of words into step-by-step calculations. For example, if we are considering the probability of the phrase \"cat in the hat\":\n\n$$\nP(\\text{cat in the hat})\n$$\n\nWe can break this value down into the product of the following terms (where $\\text{<s>}$ denotes the starting token):\n\n$$\nP(\\text{<s>})\n\\newline\nP(\\text{cat} ~|~ \\text{<s>})\n\\newline \nP(\\text{in} ~|~ \\text{<s> cat}) \n\\newline \nP(\\text{the} ~|~ \\text{<s> cat in}) \n\\newline \nP(\\text{hat} ~|~ \\text{<s> cat in the}) \n$$\n\nThese probabilities are provided by a **language model**. In essence, a language model’s purpose is to provide $P(w ~|~ c)$, where $w$ is a particular target word (i.e. the next word) and $c$ is the context that precedes the target word. Using a trained model, we can use $P(w ~|~ c)$ to create a distribution of the likelihood for the next word. A common method to estimate this probbaility is using a neural network:\n\n$$\nP(w ~|~ c) = \\operatorname*{softmax}(Wh_t + b)\n$$\n\nwhere $W$ represents a parameter matrix of the neural network weights, $h_t$ consists of a representation of the preceeding language (either the encoding of natural language string or the previous output of a neural network in the case of a recurrent architecture), and $b$ represents the bias of the prediction. The $\\operatorname*{softmax}$ operation produces an distribution over possible outputs $i$ through the equation:\n\n$$\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j{e^{z_j}}}\n$$\n\nNote that the output distribution is a valid probability distribution by the normalization of the denominator term, which allows the use of stochastic sampling for various applications (including text generation).\n\n#### Text Generation\n\nNow, we turn our focus to using these probabilities to **create** text. We first determine what the first word of our generation would be. Similar to the previous example, where we have a $\\text{<s>}$ token to signify the beginning of a sequence, we can ask the model what the value of $P(w ~|~ \\text{<s>})$ for a variety of different values of $w$.\n\nMore broadly, our goal is to select the words that maximize:\n\n$$\n\\prod _{i = 0} ^{n} P(w_i ~ | ~ c_0 ~ ... ~ c_{i - 1})\n$$\n\nNote that we can model this problem as a graph with layers for each timestep *t* in the sentence comprised of nodes representing words that are fully connected to the nodes in the following layer. Thus, we can prove that this problem is equivalent to the **longest path problem**, which has been shown to be NP-complete. As such, generating text through deterministic combinatorial optimization is far too slow, both for real-world applications and for model training.\n\n### Machine Reasoning\n\nMachine reasoning a subfield of artificial intelligence regarding the ability to \"think\" in logical or plausible terms. Bottou considers a plausible definition of reasoning to be \"algebraically manipulating previously acquired knowledge in order to answer a new question\" [(Bouttou, 2011)](https://arxiv.org/ftp/arxiv/papers/1102/1102.1808.pdf). This notion is linked with the goal of understanding, which has implications in artificial intelligence's ability to perform problem solving. Whereas much of the hype in artificial intelligence has been attributed to learning, systems that can perform true logical reasoning is arguably a more elusive goal for the field. Recent reseach has shown that by framing reasoning as a learning problem, it is possible for machine learning models to perform some forms of reasoning (e.g. abductive, inductive). Additional research has shown that even without expressing reasoning as a learning objective, machine learning models are still able to indirectly learn various concepts related to reasoning through the hidden representation mechanisms of neural networks.\n\n#### Commonsense Reasoning\n\nTraditional forms of artificial intelligence and machine learning rely on data or other explicit knowledge in order to make decisions and perform judgement. However, much of the cognitive intelligence that humans exercise is through the form of general purpose knowledge, that is often unobserved or implicit. This type of reasoning is known as **commonsense reasoning**, and as the name implies it concerns judgement regarding information about the real world that goes unspoken. Commonsense reasoning is an umbrella term including (but not limited to) situational awareness, an understanding of human motivation, intuitive physics, and general purpose knowledge that an average human (any age) would know. An interesting attribute of common sense reasoning is that it is often multimodal, relying on both computer vision and natural language processing in order to solve problems. Modern neural network architectures are able to solve these multi-modal problems with much better relative performence than traditional models due to their ability to represent the encodings of different input formats as hidden representations, which are used downstream in the network to generate predictions. For example, the Compositional Attention Network architecture presented by [Hudson et. al](https://arxiv.org/pdf/1803.03067.pdf) decomposes multi-modal problems into a series of sub-problems which is end-to-end differentiable. \n\nThere is also reason to believe that modern neural network representations are able to learn commonsense reasoning as a byproduct of another training objective. Commonsense reasoning is often a missing attribute from language models, as shown in [Zhou et. al](https://arxiv.org/pdf/1911.11931.pdf) due to their lack of an explicit training objective for reasoning. However, with the rise of training data volume for large neural language models (i.e. transformers), and the current state-of-the-art's reliance on contexualized language, there have been trends to indicate that such models are able to contain representations of commonsense reasoning. In the paper \"Extracting Commonsense Properties from Embeddings with Limited Human Guidance\" by [Yang et. al](https://www.aclweb.org/anthology/P18-2102.pdf), it was possible to extract commonsense knowledge from pre-trained word embeddings, which are representations generated by means of another dummy task (e.g. skip-gram, or continuous bag-of-words). As a result, there exists some latent commonsense knowledge that can be probed with human guidance (as in Yang et. al), or possibly even without human annotation through the means of a generation task.\n\n#### Knowledge Graphs\n\nFor this paper, the knowledge representation scheme that we are concerned with are **knowledge graphs**. As the name implies, these are graphical data models that collect a set of *entities*, as well as various relationships between them. The set of entities forms the nodes for the knowledge graph and their relations form the set of annotated edges. For example, Google Search incorporates a knowledge graph of various entities from the results of web searches (e.g. President Barack Obama), which allows it to provide information about the entity without performing additional web crawls. Using knowledge graphs as intermediary knowledge representation for reasoning tasks can serve to be a more efficient manner than analyzing raw text, since a single-pass of logical inference over the knowledge can be performed in $O(|V| + |E|)$ time and space, where $V$ contains the entities of the represented knowledge and $E$ contains the annotated relations between them.\n\n![](https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/5ff89cbc-ea1e-4ab0-b646-877369cad553/File/99553b788b5a24eb03c3e35e6917c008/disease_symptom_healthcare_knowledge_graph.png)\n\n<small> Source: Oracle </small>\n\nFor many years, knowledge bases were only developed through human annotation. Collaborative human approaches to knowledge graph constructions have existed as volunteer efforts, namely through [Wikidata]() and [Freebase](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&rep=rep1&type=pdf). The limitations to human approaches are (by nature) their inability to automatically adapt to new raw text, as well as their reliance on humans, which are either costly or time intensive (or both). A long-standing goal of artificial intelligence is knowledge graph construction, where such a graph represents a variety of high-precision knowledge on a variety of concepts that rivals human performance. As such, a variety of research has been conducted on novel methods of constructing knowledge graphs, ranging from a pure information extraction approach (e.g. OpenIE) to generative approaches (e.g. COMET). In this paper, both approaches will be examined, and we also present evidence that pre-trained Transformer models share some semantics with their OpenIE counterparts.\n\n### Information Extraction\n\nRaw text in natural language form is often filled with useful semantic information. However, this information is usually in the form of unstructured text that is difficult to analyze. Information extraction is the task of transforming unstructured information embedded in texts into structured data [Jurafsky et. al](https://web.stanford.edu/~jurafsky/slp3/17.pdf), which can later be used by some independent data structure or algorithms. With increases in volume for natural language data, information extraction has been a rapidly evolving field in natural language processing. Furthermore, with increased modelling performance made possible by neural networks, new state-of-the-art models are developing at a faster rate than previous paradigms allowed for.\n\n#### Named Entity Recognition\n\nNamed Entity Recognition (NER) is a subtask of information extraction that is concerned with identifying named entities from raw text. Named entities are real-world objects that are often embedded in natural language. For example, named entities can include people, places, products, brands, or countries from real life. Since named entities often exist in natural language in an unstructured manner, it presents a significant computational challenge to extract named entities from text. Often, Named Entity Recognition is decomposed into two subtasks: 1) identifying named entities 2) classifying the named entities (i.e. person, product, place etc.) that were previously identified. For example, the statement \"Steve Jobs founded Apple\" would include the named entities of \"Steve Jobs\" (note that named entities are not limited to a single token) and \"Apple\" (note the correct identification of \"Apple\" being a company and not a fruit). \n\nThe current state-of-the-art approaches for Named Entity Recognition rely on machine learning, particularly neural network model. In recent years, fine-tuning of large transformer models (typically with over a billion parameters) achieves impressive results on top leaderboards. This is in part due to due the importance of bi-directionality in modern language models, as well as the impact of contexualization, which has improved a variety of other tasks in natural language processing, namely Word Sense Disambiguation (WSD). Popular implementations of NER include Explosion AI's spaCy and Stanford NLP's OpenNLP.\n\n#### Dependency Parsing\n\nDependency grammars are a family of grammar formalisms that allow for a discrete representation of language. Such representations are useful for natural language processing applications due to their structure, which also provide semantic value. Since natural languages are morphologically rich and have a free word order, dependency grammars are well suited to provide semantic relationships between words in a given sentence. In linguistics, there is a notion of a **grammatical relationship**, which provide the approximation for semantic relationships. A grammatical relationship consists of a head and a dependent, where the relationship can also be annotated for further semantic value. For example, typical relations are nominal subjects, direct objects, indirect objects, clausal complements, nominal modifiers, or adjectival modifiers. Having a set of words in a sentence and a collection of their relationships between each other naturally gives rise to a graph structure for the grammatical relations, known as a **dependency tree**. Producing a dependency tree for a specific sentence is a considerable computational task due to the inherent difficulties in working with unstructured natural language. The Penn Treebank dataset, one of the research efforts in the 1990s that galvanized the field of natural language processing, is one of the standards for dependency parsing. In particular, it includes around 2,500 stores from the Wall Street Journal along with their syntactic annotations. \n\nThe current state-of-the-art approaches for dependency parsing, like many other tasks in natural language processing, involve fine-tuning a transformer model to predict the relationships between constituents in a given text. The impact of contexualization and deeper semantic representation has allowed for models in this task to flourish.\n\n## Related Work\n\n### Knowledge Extraction\n\n#### [Neural Open Information Extraction](https://arxiv.org/pdf/1805.04270.pdf)\n##### Summary\n*Neural Open Information Extraction* is a paper written by researchers at Microsoft Research Asia in 2018 regarding improving Open Information Extraction (OpenIE). OpenIE is a natural language processing task of extracting a structured representation of knowledge in text. Typically, the extracted information is in the form of a knowledge tuple of the form (subject, relation, object), or another $n$-ary logical proposition. These knowledge tuples can be used for downstream tasks such as question answering (QA), knowledge graph construction, or natural language understanding (NLU). Since traditional OpenIE engines depend on hand-designed schemas, the approach outlined by the paper instead highly confident arguments and relationship tuples bootstrapped from a state-of-the-art OpenIE system, a similar method to Bosselut et al. The authors develop a novel neural network architecture that is able to generate knowledge tuples, rivalling the predictive performance of traditional OpenIE systems, while still maintaining computational efficency.\n\n##### Analysis\nThe Neural Open Information Extraction system is based on an encoder-decoder neural network architecture, often known as Seq2Seq in the NLP field due to its objective of accepting a sequence as an input and producing a sequence as output. Since these architectures can be trained end-to-end, it also serves to produce an intermediary input representation, often as an embedding of the input sequence. This also has the added benefit of the architecture not needing to rely on hand-crafted patterns, and removing error propagation through the extracted knowledge. In particular, given input word sequence $X = (x_1, x_2, ..., x_m)$ and knowledge tuple sequence $Y = (y_1, y_2, ..., y_n)$, the neural network predicts the conditional distribution $P(Y | X)$ through the chain-rule decomposition:\n\n$$\nP(Y | X) = P(Y ~|~ x_1, x_2 ..., x_m)\n= \\prod _{i = 0} ^{n} P(y_i ~ | ~ y_0 ~ ... ~ y_{i - 1} ~ ; ~ x_0 ~ ... ~ x_n)\n$$\n\nAlthough OpenIE is able to capture structured information in a consistent manner, it is ill-suited to higher level reasoning. For example, if an entity is extracted and known \"to be an adult\", OpenIE will not represent the fact that the entity is not \"known to be a child\". This is by definition, since OpenIE is extractive, and thereby only captures knowledge that is explicilty represented in the text. Other papers related to using neural networks for knowledge graph construction serve to remedy this problem, and often rely on OpenIE as an extractive baseline.\n\n#### [Language Models as Knowledge Bases?](https://www.aclweb.org/anthology/D19-1250.pdf)\n##### Summary\n*Language Models as Knowledge Bases* is a paper written in 2019 by various researchers at Facebook AI Research (FAIR) and University College London. The paper is inspired by the notion that in the process of learning linguistic knowledge (i.e. language model training), models are possibly able to learn relational data as well, and that such relational data can be probed via cloze statements (or other fill-in-the-blank tasks). The goal of the paper is to explain that language models themselves contain relational data similar to how knowledge bases do, without the need for extensive manual annotating of a structured knowledge graph. The authors show this by evaluating the ability of BERT (without fine-tuning) to complete cloze tasks for factual data, as well as BERT's ability to perform question answering (QA) tasks. In the process, the authors develop a probe named *LAMA* (LAnguage Model Analysis), which is a set of subject-relation-object triples or question-answer pairs from various disparate sources.\n\n##### Analysis\nThe intuition for the claims about language models made by the paper stem from the modern training process of large language models. In particular, the ability of modern language models to contain such linguistic and relational knowledge is two-fold. The first improvement that such models have to obtain excess factual knowledge than expected is due to the amount of training data used in the model training process. Since transformer language models do not have the bottle-necks in parallelization that recursive models do (e.g. recurrent neural networks), it is possible to train them on troves more data, which often include internet data containing a plethora of declarative/factual information such as Wikipedia. The second recent improvement in language modelling is sheer representational power, as transformer models have a representational capacity order of magnitudes larger than their predecessors, as it is common to see state-of-the-art models with billions of model parameters. The generation process itself may be a contributor to this, as language models tend to exihibit some sort of memorization when generating text, as well as information obfuscation or retrieval itself.\n\nIn addition, the method of information extraction put forth by the paper is particularly interesting to this research. Instead of producing declarative knowledge in a structured manner, the generative approach indirectly probes the model for answers via clever question/prompt formulation, relying on implicit knowledge representation peformed by the model that is hidden to observers. This is in contrast to a more direct approach of extracting knowledge, like the previous gold-standard information extraction approach of OpenIE.\n\nThe authors also note that certain types of factual knowledge are more easily acquired by language models than others. As a result, they argue that they are not measuring the average empirical performance of language models on this task, but are instead measuring a **lower bound** on the ability for language models to acquire knowledge.\n\n### Knowledge Graph Tasks\n\n#### [GraphWriter](https://arxiv.org/pdf/1904.02342.pdf)\n##### Summary\n*Text Generation from Knowledge Graphs with Graph Transformers* is a paper written in 2019 by researchers at the University of Washington, the University of Edinburgh, and the Allen Institute for Artificial Intelligence (AI2). The paper aims to improve the generational abilities of modern transformer language models by augmenting their knowledge abilities through declarative knowledge, in particular knowledge graphs. The paper introduces a novel graph encoding neural network architecture that is capable of understanding the relational structures that comprise knowledge graphs. This encoding architecture is then incorporated into an end-to-end trainable system that is capable of generating downstream text given a knowledge graph as input. The authors show that this architecture is superior than competing architectures through automatic evaluations (e.g. BLEU, METEOR), as well as human annotators agreeing that the produced text is more informative and captures more of the source information. In addition, the authors introduce the AGENDA (Abstract GENeration DAtaset) dataset as their encoder-decoder architecture is trained to generate scientific paper abstracts from their knowledge graph representations.\n\n##### Analysis\n\nBroadly, this paper is an advancement of the \"concept-to-text\" model of generation. One powerful result of this paper is showing that it is possible to provide neural language models with the context necessary for generating longer text that still maintains the logical integrity of the provided declarative knowledge. The AGENDA dataset also serves as a good evaluation benchmark for this task due to its generation domain of scientific abstracts since it contains 40,000 examples which all provide clear and consistent text with a heavy emphasis on declarative facts and knowledge. The strong performance on this benchmark may be an artifact of the domain itself, and further research could be conducted to show whether this task is feasible on more free-formed or longer text, such as narrative writing with summarizations.\n\nThe GraphWriter architecture introduced in the paper, which includes a graph transformer neural network, is a highly performant and task-dependent approach to encoding graphical data for use in a neural language model. It is worth noting that decoding strategy used in the paper differs from \"pure generation\" that other papers use (i.e. directly from the model's vocabulary) as the decoding process will additionally sample from the entities contained in the knowledge graph's set of labelled nodes. In particular, the architecture incorporates the two next-token distributions of sampling from the vocabulary (denoted $\\text{vocab})$ or copying the text from a sample over the entities (denoted $\\text{copy}$). This final probability is calculated as:\n\n$$\np * \\alpha^\\text{copy} + (1 - p) * \\alpha^\\text{vocab}\n$$\n\nPossibly the most important advance in the paper is its attention to information flow. An interesting implementation detail is the inclusion of a \"global vertex\", which is connected to all other vertices in the knowledge graph. The goal of this vertex is to introduce less restricted information flow between all vertices, presumably due to it being otherwise difficult to provide an adequete initialization sequence. Although the paper's novel architecture bears a strong resemblance to the Graph Attention Network (GAT) paper (Velickovic et al. 2018), which uses self-attention over a node's neighbourhood to compute a hidden representation, there exist architecture differences regarding the attention mechanism's use towards incorporating declarative knowledge. These changes related to information flow provide a statistically significant performance increase on the AGENDA task.\n\n\n#### [COMET](https://arxiv.org/pdf/1906.05317.pdf)\n##### Summary\n*COMET: Commensense Transformers for Automatic Knowledge Graph Construction* is a paper published by researchers at the Allen Institute for Artificial Intelligence (AI2), the University of Washington, and Microsoft Research in 2019. The paper is primarily concerned with progressing standards in knowledge graph construction, particularly making a paradigm shift from knowledge graph construction as an information task towards a generative task, moving away from the structured schemas of traditional knowledge bases. The paper is also primarily concerned with **commonsense knowledge**, as opposed to the traditional knowledge graph approach of representing domain-specific information about a particular narrative. In doing this, the authors propose a novel transformer architecture named COMET, which is able to augment an existing knowledge graph by generating commonsense descriptions for entities in natural language. The architecture also approaches human performance on the evaluation dataset of ATOMIC and ConceptNet (existing declarative knowledge datasets). As a result, the findings of this paper would indicate that generative models for commonsense may be a path forward to developing knowledge graphs.\n\nThis paper introduces a promising alternative to information extraction through the lens of generative modelling of language. In particular, the crux of the paper lies in the reduction of commonsense acquisition to knowledge base construction. Since previous work on large language models have shown that transformers are able to be useful in tasks related to declarative knowledge, this paper is able to rely on such findings to utilize such models for knowledge graph generation. An interesting caveat is that traditional knowledge graph construction is primarily concerned with representing over a well-defined space of entities and relations that can be directly modelled. In contrast, commensense knowledge is a reasoning task over an ill-defined space of possible entities and relations, and over an unrestricted vocabulary. As a result, it poses a considerably different problem than the tasks that transformers have been known to excel at (i.e. fine-tuning over a defined task space). However, this problem is arguably more well suited to transformers than traditional OpenIE approaches since contextualized language models are able to represent implicit knowledge, whereas extractive methods only consider knowledge that by definition is explicit.\n\n##### Analysis\nThe COMET architecture is a performant framework for constructing commonsense knowledge graphs. COMET first relies on a seed set of knowledge tuples of the form $\\{s, r, o\\}$, where $s$ is the subject of the knowledge tuple, $r$ is the relation of the tuple, and $o$ is the object of the phrase. This seed set of knowledge is represented by concatenating the tokens of each part of the tuple into a single tensor:\n\n$$\nX = \\{X^s, X^r, X^o\\}\n$$\n\nCOMET is then trained end-to-end to predict $X^o$ given  $\\{X^s, X^o\\}$ using the conditional loglikelihood loss function:\n\n$$\nL = - \\sum^{|s| + |r| + |o|} _{t = |s| + |r|} \\operatorname*{log} P(x_t | x_{< t})\n$$\n\nIn addition, due to the lack of positional knowledge in transformer networks, each word representation $e_t$ is summed with a positional encoding for the position $t$ of the word:\n\n$$\nh_t = e_t + p_t\n$$\n\nThe performance of this representation may suggest that locational information about knowledge may be of importance, since a similar notion of neighbourhood encoding is often a prevalent feature of graph neural networks and even convolutional neural networks (i.e. receptive fields). It may also be loosely related to the notion of contextualization in language models, where consideration of the positional structure of certain semantic entities is able to provide additional representation power.\n\nGiven the generation nature of the task of generating natural language descriptions of commonsense descriptions, the paper uses BLEU-2 as an evaluation metric. BLEU (Bilingual Evaluation Understudy) measures the quality of a text given a reference baseline using a modified form of precision between $n$-grams, originally used for machine translation. In the case of COMET, the reference text is the original knowledge tuple's object, which is often represented using natural language text as opposed to a direct enumerable entity. The authors also verify the automatic evaluation methods of both BLEU and perplexity using human annotation (e.g. Mechanical Turk), by asking annotators whether the model's generation consitutes a plausible knowledge tuple. The authors found the evaluations to outperform not only the baselines, but also the state-of-the-art architectures on the same task by a 51% relative performance increase (with statistical significance).\n\nThe paradigm shift from knowledge graph construction from an information extraction task to a text generation task serves as inspiration for this paper. In particular, we leverage transformer models to generate knowledge graphs using a process similar to OpenIE, as well as further analysis into the performance of text generation using existing knowledge tuples.\n\n![](https://i.imgur.com/LN9NaNp.png)\n\n<small> An example generated knowledge graph from ATOMIC's seed knowledge tuples. </small>\n\n## Dependency-Based Construction Task\n\n### Experimental Design\nDataset: [CMU Book Summaries](http://www.cs.cmu.edu/~dbamman/booksummaries.html)\n\n1. Extract knowledge graph using OpenIE.\n2. Extract knowledge graph using NER + dependency parsing.\n3. Compare extraction similarities.\n\n### Pipeline Architecture\n\n#### SpaCy\n\nIt is often be a considerable engineering effort to develop an NLP workflow from scratch that is performant enough to work with large datasets. The open source Python package spaCy solves this issue by providing industry-strength and reproducible NLP workflows over a variety of NLP tasks. In this paper, we use spaCy's dependency parsing and named entity extraction pipelines to leverage reproducible pipelines, as well as clear and helpful abstractions over such NLP tasks.\n\n#### HuggingFace Transformers\n\nUtilizing pre-trained models can often be a complicated task due to the large number of different language models, as well as their differing input formats. In particular, large-scale language models with billions of parameters can be unwieldly due to their sheer size, making it difficult to implement in Python without GPU acceleration. The HuggingFace Transformers open source Python package solves this by providing a general purpose Python architecture for working with Transformer models, as well as a model registry for fine-tuning existing language models. We leverage the HuggingFace Transformers package for generating text from BERT-based language models.\n\n#### Implementation\n\nIn this prototype, we leverage Transformer neural network models (i.e. BERT-based fine-tuning) for named entity extraction and dependency parsing, which we use to generate nodes and edges, respectively, for the knowledge graph.\n\nThe core procedure (implemented in Python) is given as follows:\n\n```python\nfor token in doc:\n    if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n        # root verb (event) extraction\n        print(token.dep_, token.text, token.pos_)\n\n        for argument in token.children:\n            if argument.dep_ in {\"csubj\", \"nsubj\", \"nsubjpass\"} and argument.pos_ in {\"PRON\", \"NOUN\", \"PROPN\"}:\n                # named entity extraction\n                if argument.pos_ == \"PRON\" and argument._.in_coref:\n                    print(argument.text, \"=\", argument._.coref_clusters[0].main.text)\n\n                # subject extraction\n                print(argument.dep_, argument.text, argument.pos_)\n\n            if argument.dep_ in {\"dobj\", \"obj\", \"iobj\", \"pobj\"}:\n                # object extraction\n                print(argument.dep_, argument.text, argument.pos_)\n```\n\nThis generates a graph $G = (V, E)$ which we are able to export into a useable format for downstream tasks. We also develop auxillary functions for querying and interacting with the knowledge graph.\n\n#### Representation\n\nIn order to provide the knowledge graph as an input to standard language models, we must first generate a string representation for the graph. The simplest way to do this is to perform a string concatenation between the various nodes and edge annotations, ensuring that nodes and edges appear close-by in the representation.\n\nThis notes a more ambitious, but less robust departure from GraphWriter, which relies on a graph neural network to process the given knowledge graph. \n\n#### Usage\n\nAfter exporting the knowledge graph, we are able to load it into our open source knowledge graph browser, which is built in JavaScript and is self-hostable for researchers.\n\n## Transformer-based Knowledge Graph Construction/Bootstrapping\n\n### Experimental Design\n\n#### Generative Construction Probe\nDataset: [CMU Book Summaries](http://www.cs.cmu.edu/~dbamman/booksummaries.html)\n\n1. Extract knowledge graph using OpenIE + generative approach (OpenIE for subject and relation, generate object).\n2. Compare accuracy/embedding similarity.\n\n#### Knowledge Graph Reasoning Task\nDataset: [ATOMIC Dataset](https://homes.cs.washington.edu/~msap/atomic/)\n\n1. Generate knowledge graph with NER + dependency parsing approach.\n2. Feed knowledge graph representation to language model and evaluate on ATOMIC dataset.\n\nTODO\n\n### Results\n\nTODO\n\n### Comparison\n\nTODO\n\n## Future Work\n\nTODO\n\n## Conclusion\n\nTODO\n\n## Bibliography\n1. https://blog.google/products/search/introducing-knowledge-graph-things-not/\n2. https://arxiv.org/pdf/1906.05317.pdf\n3. https://arxiv.org/pdf/1610.08763.pdf\n4. https://arxiv.org/pdf/1904.02342.pdf\n5. https://www.cs.cmu.edu/~mg1/thesis.pdf\n6. https://www.aclweb.org/anthology/D19-1250.pdf","html":"<div class=\"notification is-link\">\n  This post will be under construction until I graduate, and is subject to (probably a lot of) change. For any errata, feel free to reach out to me!\n</div>\n<h1>My Undergraduate Thesis</h1>\n<blockquote>\n<p>Extracting Knowledge From Transformer Language Models</p>\n</blockquote>\n<p>After (almost) 4 long years at the University of Pennsylvania, I'm finally a senior approaching my final few weeks as a student. Sentiments aside, this means that I'm also wrapping up a lot of the research on machine learning and natural language processing I've done during my time at Penn. For my program, this culminates in a senior thesis (on a topic of my choosing) during my final semester. A single semester isn't typically enough time to make a significant contribution to the field, so I'm hoping to use this thesis as an opportunity to explore new areas of machine intelligence. I'm really excited to shift some of my focus from my previous research from generative models (i.e. language models) towards knowledge representation.</p>\n<p>I'll be using this part of my website as a public workspace for my foray into <strong>information extraction, knowledge graph construction, and machine reasoning</strong>. This will probably include (but isn't limited to) code snippets, equations/derivations, background reading, literature review, and personal hot takes.</p>\n<hr>\n<h2>Introduction</h2>\n<p>In recent years, there has been an explosion of data of various formats (e.g. video, text, sensor), which are not always immediately useful for completing tasks. An overarching goal in the field of machine intelligence is the problem of <strong>knowledge representation</strong>, a field dedicated to designing <em>efficient</em> representations of data that capture their respective data.</p>\n<p>This problem is not just a question of information theory, but also relies on biological inspiration. When collecting new information, humans are able to make connections between related data, which enables not only efficient recall but also provides a framework for synthesizing new thoughts or beliefs. Providing such a framework for machines to perform such types of reasoning over previously given data is a goal for the field and, in turn, this paper.</p>\n<p>The approach that this paper is primarily interested in is the <strong>knowledge graph</strong> model, which models real life (or abstract) entities and their relationships as a graph <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span>. Google aptly describes this approach as \"things, not strings\", referring to the notion that these graphs are often mined from pure language but contain a far more powerful semantic meaning.</p>\n<p>In this paper, we explore various knowledge graph construction methods, both from their theoretical merits, as well as their empirical performance provided real natural language data as source material. We also aim to provide an unopinionated framework for expressing declarative knowledge over mined knowledge graphs to aid in logical reasoning. In addition, an engineering goal for this thesis is to make such knowledge graphs accessible to the general public, by offering an open source front-end application that is self-hostable for researchers.</p>\n<h2>Background</h2>\n<h3>Natural Language Processing</h3>\n<p>With the recent implosion of textual data as well as advances in machine learning, the field of natural language processing has been galvanized like few other disciplines in computer science. Natural language processing is the computational task of processing and analyzing text written in natural language. This manifests itself as being able to read and write human language with strong confidence and understanding of the language. Natural language processing is an umbrella term used to describe the field, and includes a variety of subtasks such as speech recognition, text generation, and information extraction. The computational challenge of natural language processing comes from the lack of structure within language. As a result, text can often be ambiguous or unclear regarding how to interpet its higher-level semantics. For example, the statement \"Call me a taxi, please\" would be difficult to develop an algorithm to parse, since it's unclear (arguably even to humans) whether or not the intent is to hail a taxi or to be referred to as a taxi. Alan Turing, in his paper <a href=\"\">Computing Machinery and Intelligence</a>, hails natural language understanding as a criterion of intelligence, and a plethora of research has been devoted to analyzing the requirement (e.g. <a href=\"https://roft.io\">Dugan et. al</a>).</p>\n<p>Originally, natural language processing involved the symbolic manupulation of language, which lacks semantic understanding (a necessary component of true artificial intelligence). Regarding machine translation (one of NLP's most coveted subtasks), <a href=\"\">The Chinese Room</a> philosophical thought experiment suggests that such symbolic systems arguably weren't capable of true understanding. During this era of NLP, most approaches involved some form of human guidance in addition to a strong reliance on Chomskyan theories of linguistics (e.g. formal languages, context-free grammars etc.). In the 1980s to 1990s, these human guided efforts were superceded by statistical NLP, where data and machine learning enabled results unseen before. Datasets such as multilingual corpora from Canadian parliament hearings for translating between English and French, as well as the Penn Treebank dataset for statistical parsing ushered in a new paradigm for language understanding that drifted from its original linguistic roots. In recent years, the current state-of-the-art approaches for natural language processing ubiquitously involve neural networks and representation learning. The deep learning paradigm offers impressive results on a variety of NLP tasks; however, the lack of transparency for these models is prone to criticism, and can often make it difficult to deploy such models to real-world scenarios due to unpredictability. </p>\n<h3>Machine Learning</h3>\n<p>In recent years, researchers have made incredible progress towards the goal of artificial intelligence through the field of machine learning. For tasks that cannot be completed with imperative rules (e.g. image classification, text generation), traditional algorithms and software fail to produce adequate results. Machine learning serves to remedy this by automatically extracting such patterns and rules from data, and applying them at algorithm runtime to make decisions.</p>\n<br />\n<p><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-3/Figures/ch01-a-new-programming-paradigm.png\"></p>\n<p><small> Source: François Chollet </small></p>\n<br />\n<p>Although algorithms to make predictions from data have existed for many decades, the recent explosion in machine learning progress can be attributed to two key developments: more computing resources and more data. The first development is a byproduct of both Moore's Law, as well as readily available compute being made more prevelant by cloud providers such as Amazon's AWS or Microsoft's Azure. The latter development in machine learning progress is partly brought about by the former (i.e. more storage availability for big data), but is also made possible by an increased number of devices connected to the internet, providing troves of data to analyze.</p>\n<p>Regarding this paper on knowledge graphs, machine learning has made it possible to automatically mine such graphical representations, instead of hand-annotating potentially billions of entities and their relations (as is the case with Google's Knowledge Graph, presumably the largest in existence).</p>\n<h4>Neural Networks</h4>\n<p>The most prevalent model in the machine learning renaissance is the <strong>neural network</strong>, which also goes by the name of <strong>deep learning</strong>. As the name implies, neural networks are machine learning models with a biological inspiration of neurons in animal brains. In particular, neurons can fire in accordance with other \"similarly wired\" neurons, where the strength between neurons (or synapses) can be modelled using continuous weights. </p>\n<p>In <strong>feed-forward</strong> neural networks (pictured below), we can often stack neurons in order to create a longer synapse path between the input layer and the output layer, thus creating the need for additional learnable model parameters. This increase in model capacity provides representational power to the neural network, thus giving the name <strong>deep learning</strong> (due to networks with hidden layers being \"deep\").</p>\n<p><img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\">\n<small> Source: IBM </small></p>\n<p>Neural networks offer a primary advantage of not requiring hand-picked or discrete features during inference. This is especially important when working when modelling data types that do not have explicitly identifiable features, such as language or image data. The downside to this power is that neural networks require orders of magnitude more training data than classical machine learning models (such as random forests or linear regression).</p>\n<p>Neural Network parameters are generally stochastically optimized using <strong>gradient descent</strong>. The process for doing this is to first define an auxillary <strong>loss function</strong> that measures the \"correctness\" of the model's output with respect to some training data, known as a <em>forward pass</em>. Next, the gradients of the model parameters with respect to the loss function are computed using the chain rule of calculus in a dynamic programming algorithm named <strong>backpropagation</strong>, also known as the <em>backward pass</em>. Finally, the model parameters are updated in accordance with their gradients in order to minimize the loss function. The stochasticity in the optimization process arrives from sampling a <strong>batch</strong> of training examples at a time in order to leverage hardware parallelization (particularly in GPUs). </p>\n<p>The gradient descent update rule is given by:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mi>t</mi></msub><mo>−</mo><mi>α</mi><mi mathvariant=\"normal\">∇</mi><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{t + 1} = w_t - \\alpha \\nabla w_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord\">∇</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span> is the <strong>learning rate</strong>, often an integer in the range [0.01, 0.05].</p>\n<h3>Transformers</h3>\n<p>The current state-of-the-art in various natural language processing tasks (particularly related to information extraction and text generation) are <strong>transformer</strong> models, which was published by Vatswani et. al in 2017. These models are largely extensions of feed-forward neural networks, and mark a departure from the previous generation of language models that were variations of <strong>recurrent neural networks</strong>. Due to their feed-forward nature, transformer networks are highly parallelizable, and thereby making it feasible to train of vast quantities of language data. As a result, these models have achieved widespread state-of-the-art results on many tasks in natural language processing, often achieving human-level performance.</p>\n<p><img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\">\n<small> Source: Jay Allamar </small></p>\n<p>A recent development in the machine learning landscape named <strong>transfer learning</strong> has also assisted in the rise of transformer models. Transfer learning involves utilizing features for a model of one specific problem, and re-applying them towards another problem. The intuititon behind this approach is that problems often require overlapping knowledge, and speed as well as performance boosts are possible by relying on a pre-trained model. In the natural language processing world, this has manifested through fine-tuning, where a pre-trained model is re-trained with additional examples on a more specific task, often with a smaller learning rate. Transformers are natural models to fine-tune with since they are often trained on swaths of high-quality language data with a relatively generalizable task of language modelling. </p>\n<h4>Architecture</h4>\n<p>Transformer models are sequence-to-sequence (Seq2Seq) models consisting of an encoder and a decoder that rely heavily on the <strong>attention mechanism</strong>. Attention aims to remedy the bottleneck in previous approaches of sequence-to-sequence tasks (e.g. chatbots, question answering, machine translation) by not having to encode the entirety of a given input sequence into a fixed context vector. Attention forgoes this by allowing a model to \"softly\" search for the parts of the source sentence that are semantically or structurally relevant to aid in the task of prediction. Mechanically, instead of the encoder passing only a single context vector to the decoder, every context vector is additionally provided to the decoder. Then, the attention decoder scores the hidden states and uses these attention-scored hidden states for prediction. Typically, these attention scores are concatenated with the hidden states in order for the decoder to make a prediction. This mechanism allows for the model to \"attend\" to the input sequence when generating the output sequence, effectively allowing the model to query for sections of the input that are relevant. Conversely, the model may also attend to the input sequence when further analyzing the input sequence itself in a process known as <strong>self-attention</strong>, which is a crucial mechanism for transformer networks.</p>\n<p>The Transformer architecture is similar to traditional seq2seq architectures by virtue of having an encoder and decoder. The encoder consists of six identical neural networks, which first apply self-attention to the input sequence and is then followed by a simple feed-forward neural network, where the embedding layers use a dimensionality <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">d</span></span></span></span> of 512. The decoder of Transformer models also consist of six stacked neural networks with the same architecture of the encoder, but with an additional multi-headed attention layer. The model also masks inputs beyond the current position to avoid \"looking ahead\" at the input sequence. The authors dub their particular use of attention to be \"Scaled Dot-Product Attention\" due to a scaling factor that allows for stable gradients during model which can be highly optimized using hardware acceleration. The matrix of outputs for the Scaled Dot-Product Attention layer is computed as:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Attention</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>Q</mi><mo separator=\"true\">,</mo><mi>K</mi><mo separator=\"true\">,</mo><mi>V</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"normal\">softmax</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy=\"false\">)</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{Attention}(Q, K, V) = \\operatorname*{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">A</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">n</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">Q</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.448331em;vertical-align:-0.93em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">s</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\" style=\"margin-right:0.07778em;\">f</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">x</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5183309999999999em;\"><span style=\"top:-2.25278em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85722em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.81722em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.18278000000000005em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">Q</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.93em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span> are abstractions over the attention mechanism that are applied to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> represents the dimensionality of the embedding representations. Due to a lack of convolution or recurrence in the model architecture, information about the order of words is \"injected\" into the model's knowledge using a <strong>positional encoding</strong> that is incorporated into the representation of the input sequence.</p>\n<h4>Performance</h4>\n<p>A benefit of the transformer architecture is that reliance on the attention mechanism allows for faster computation in practice, which is essential when considering training sets that are sized at orders of magnitudes greater than previous state-of-the-art approaches. Whereas the runtime for convolutional neural networks (CNNs) are in the order of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>l</mi><mo>∗</mo><msup><mi>d</mi><mn>2</mn></msup><mo>∗</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(l * d^2 * w)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span> (where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span> equals the length of the input, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">d</span></span></span></span> equals the dimensionality of the representation and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> represents the width of the kernel), recurrent neural networks are marginally more peformant by virtue of the lack of kernal with a runtime of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>l</mi><mo>∗</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(l*d)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mclose\">)</span></span></span></span>. However, transformer networks are able to perform at a much faster speed due to its runtime only being quadratic in the length of the input, and not of the dimensionality with a runtime of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>l</mi><mn>2</mn></msup><mo>∗</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(l^2 * 2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span></span></span></span>. In recent years, the rise of increasing dimensionality for representation, whereas input sequences have remained largedly the same has proved that the transformer's approach to computational effiency is one of the reasons behind the architecture by permitting new datasets that precluded previous approaches.</p>\n<h4>Connection to Graph Neural Networks</h4>\n<p>Since the attention mechanism can be interpreted as a connected graph, with edged annotated with their various weights (or attention strengths), transformer models are closely related to neural networks that operate over graphs, known as <strong>graph neural networks</strong>. In particular, a challenge in graph neural networks is the requirement that nodes are encoded in a representation that captures information about the local structure, namely to derive semantic value from a given node's neighbourhood. In transformer models, this same requirement is challenge as well, since the feed-forward nature of the neural network prohibits the model from learning the temporal ordering aspect of the input words. In transformer networks, this is solved by introducing a positional encoding (often a sinusoidal function) for each token, whose embeddings are an additional input to the multiple attention heads of the transformer model. Their connection to graph neural networks would suggest that transformers would be a natural choice for incorporating graphical data into a language setting.</p>\n<h3>Language Models</h3>\n<p>We can use the chain rule in probability to break down a sequence (or sentence) of words into step-by-step calculations. For example, if we are considering the probability of the phrase \"cat in the hat\":</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>cat in the hat</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(\\text{cat in the hat})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">cat in the hat</span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>We can break this value down into the product of the following terms (where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>&lt;s&gt;</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{&lt;s&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span></span></span></span> denotes the starting token):</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>&lt;s&gt;</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>cat </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt;</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>in </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt; cat</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>the </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt; cat in</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>hat </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt; cat in the</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(\\text{&lt;s&gt;})\n\\newline\nP(\\text{cat} ~|~ \\text{&lt;s&gt;})\n\\newline \nP(\\text{in} ~|~ \\text{&lt;s&gt; cat}) \n\\newline \nP(\\text{the} ~|~ \\text{&lt;s&gt; cat in}) \n\\newline \nP(\\text{hat} ~|~ \\text{&lt;s&gt; cat in the}) </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">cat</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">in</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt; cat</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">the</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt; cat in</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">hat</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt; cat in the</span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>These probabilities are provided by a <strong>language model</strong>. In essence, a language model’s purpose is to provide <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span></span></span></span>, where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> is a particular target word (i.e. the next word) and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> is the context that precedes the target word. Using a trained model, we can use <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span></span></span></span> to create a distribution of the likelihood for the next word. A common method to estimate this probbaility is using a neural network:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><mi>c</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"normal\">softmax</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>W</mi><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ c) = \\operatorname*{softmax}(Wh_t + b)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">s</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\" style=\"margin-right:0.07778em;\">f</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">x</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\"><span class=\"mord mathdefault\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> represents a parameter matrix of the neural network weights, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">h_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> consists of a representation of the preceeding language (either the encoding of natural language string or the previous output of a neural network in the case of a recurrent architecture), and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">b</span></span></span></span> represents the bias of the prediction. The <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">softmax</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{softmax}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">s</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\" style=\"margin-right:0.07778em;\">f</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">x</span></span></span></span></span> operation produces an distribution over possible outputs <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> through the equation:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>z</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j{e^{z_j}}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.46321em;vertical-align:-1.1218180000000002em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.341392em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.16195399999999993em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.43581800000000004em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6064620000000001em;\"><span style=\"top:-3.0050700000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2818857142857143em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1218180000000002em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>Note that the output distribution is a valid probability distribution by the normalization of the denominator term, which allows the use of stochastic sampling for various applications (including text generation).</p>\n<h4>Text Generation</h4>\n<p>Now, we turn our focus to using these probabilities to <strong>create</strong> text. We first determine what the first word of our generation would be. Similar to the previous example, where we have a <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>&lt;s&gt;</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{&lt;s&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span></span></span></span> token to signify the beginning of a sequence, we can ask the model what the value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt;</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ \\text{&lt;s&gt;})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span><span class=\"mclose\">)</span></span></span></span> for a variety of different values of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span>.</p>\n<p>More broadly, our goal is to select the words that maximize:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><msub><mi>c</mi><mn>0</mn></msub><mtext> </mtext><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mtext> </mtext><msub><mi>c</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\prod _{i = 0} ^{n} P(w_i ~ | ~ c_0 ~ ... ~ c_{i - 1})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">0</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∏</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>Note that we can model this problem as a graph with layers for each timestep <em>t</em> in the sentence comprised of nodes representing words that are fully connected to the nodes in the following layer. Thus, we can prove that this problem is equivalent to the <strong>longest path problem</strong>, which has been shown to be NP-complete. As such, generating text through deterministic combinatorial optimization is far too slow, both for real-world applications and for model training.</p>\n<h3>Machine Reasoning</h3>\n<p>Machine reasoning a subfield of artificial intelligence regarding the ability to \"think\" in logical or plausible terms. Bottou considers a plausible definition of reasoning to be \"algebraically manipulating previously acquired knowledge in order to answer a new question\" <a href=\"https://arxiv.org/ftp/arxiv/papers/1102/1102.1808.pdf\">(Bouttou, 2011)</a>. This notion is linked with the goal of understanding, which has implications in artificial intelligence's ability to perform problem solving. Whereas much of the hype in artificial intelligence has been attributed to learning, systems that can perform true logical reasoning is arguably a more elusive goal for the field. Recent reseach has shown that by framing reasoning as a learning problem, it is possible for machine learning models to perform some forms of reasoning (e.g. abductive, inductive). Additional research has shown that even without expressing reasoning as a learning objective, machine learning models are still able to indirectly learn various concepts related to reasoning through the hidden representation mechanisms of neural networks.</p>\n<h4>Commonsense Reasoning</h4>\n<p>Traditional forms of artificial intelligence and machine learning rely on data or other explicit knowledge in order to make decisions and perform judgement. However, much of the cognitive intelligence that humans exercise is through the form of general purpose knowledge, that is often unobserved or implicit. This type of reasoning is known as <strong>commonsense reasoning</strong>, and as the name implies it concerns judgement regarding information about the real world that goes unspoken. Commonsense reasoning is an umbrella term including (but not limited to) situational awareness, an understanding of human motivation, intuitive physics, and general purpose knowledge that an average human (any age) would know. An interesting attribute of common sense reasoning is that it is often multimodal, relying on both computer vision and natural language processing in order to solve problems. Modern neural network architectures are able to solve these multi-modal problems with much better relative performence than traditional models due to their ability to represent the encodings of different input formats as hidden representations, which are used downstream in the network to generate predictions. For example, the Compositional Attention Network architecture presented by <a href=\"https://arxiv.org/pdf/1803.03067.pdf\">Hudson et. al</a> decomposes multi-modal problems into a series of sub-problems which is end-to-end differentiable. </p>\n<p>There is also reason to believe that modern neural network representations are able to learn commonsense reasoning as a byproduct of another training objective. Commonsense reasoning is often a missing attribute from language models, as shown in <a href=\"https://arxiv.org/pdf/1911.11931.pdf\">Zhou et. al</a> due to their lack of an explicit training objective for reasoning. However, with the rise of training data volume for large neural language models (i.e. transformers), and the current state-of-the-art's reliance on contexualized language, there have been trends to indicate that such models are able to contain representations of commonsense reasoning. In the paper \"Extracting Commonsense Properties from Embeddings with Limited Human Guidance\" by <a href=\"https://www.aclweb.org/anthology/P18-2102.pdf\">Yang et. al</a>, it was possible to extract commonsense knowledge from pre-trained word embeddings, which are representations generated by means of another dummy task (e.g. skip-gram, or continuous bag-of-words). As a result, there exists some latent commonsense knowledge that can be probed with human guidance (as in Yang et. al), or possibly even without human annotation through the means of a generation task.</p>\n<h4>Knowledge Graphs</h4>\n<p>For this paper, the knowledge representation scheme that we are concerned with are <strong>knowledge graphs</strong>. As the name implies, these are graphical data models that collect a set of <em>entities</em>, as well as various relationships between them. The set of entities forms the nodes for the knowledge graph and their relations form the set of annotated edges. For example, Google Search incorporates a knowledge graph of various entities from the results of web searches (e.g. President Barack Obama), which allows it to provide information about the entity without performing additional web crawls. Using knowledge graphs as intermediary knowledge representation for reasoning tasks can serve to be a more efficient manner than analyzing raw text, since a single-pass of logical inference over the knowledge can be performed in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><mi>V</mi><mi mathvariant=\"normal\">∣</mi><mo>+</mo><mi mathvariant=\"normal\">∣</mi><mi>E</mi><mi mathvariant=\"normal\">∣</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(|V| + |E|)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">∣</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord\">∣</span><span class=\"mclose\">)</span></span></span></span> time and space, where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> contains the entities of the represented knowledge and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> contains the annotated relations between them.</p>\n<p><img src=\"https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/5ff89cbc-ea1e-4ab0-b646-877369cad553/File/99553b788b5a24eb03c3e35e6917c008/disease_symptom_healthcare_knowledge_graph.png\"></p>\n<p><small> Source: Oracle </small></p>\n<p>For many years, knowledge bases were only developed through human annotation. Collaborative human approaches to knowledge graph constructions have existed as volunteer efforts, namely through <a href=\"\">Wikidata</a> and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&#x26;rep=rep1&#x26;type=pdf\">Freebase</a>. The limitations to human approaches are (by nature) their inability to automatically adapt to new raw text, as well as their reliance on humans, which are either costly or time intensive (or both). A long-standing goal of artificial intelligence is knowledge graph construction, where such a graph represents a variety of high-precision knowledge on a variety of concepts that rivals human performance. As such, a variety of research has been conducted on novel methods of constructing knowledge graphs, ranging from a pure information extraction approach (e.g. OpenIE) to generative approaches (e.g. COMET). In this paper, both approaches will be examined, and we also present evidence that pre-trained Transformer models share some semantics with their OpenIE counterparts.</p>\n<h3>Information Extraction</h3>\n<p>Raw text in natural language form is often filled with useful semantic information. However, this information is usually in the form of unstructured text that is difficult to analyze. Information extraction is the task of transforming unstructured information embedded in texts into structured data <a href=\"https://web.stanford.edu/~jurafsky/slp3/17.pdf\">Jurafsky et. al</a>, which can later be used by some independent data structure or algorithms. With increases in volume for natural language data, information extraction has been a rapidly evolving field in natural language processing. Furthermore, with increased modelling performance made possible by neural networks, new state-of-the-art models are developing at a faster rate than previous paradigms allowed for.</p>\n<h4>Named Entity Recognition</h4>\n<p>Named Entity Recognition (NER) is a subtask of information extraction that is concerned with identifying named entities from raw text. Named entities are real-world objects that are often embedded in natural language. For example, named entities can include people, places, products, brands, or countries from real life. Since named entities often exist in natural language in an unstructured manner, it presents a significant computational challenge to extract named entities from text. Often, Named Entity Recognition is decomposed into two subtasks: 1) identifying named entities 2) classifying the named entities (i.e. person, product, place etc.) that were previously identified. For example, the statement \"Steve Jobs founded Apple\" would include the named entities of \"Steve Jobs\" (note that named entities are not limited to a single token) and \"Apple\" (note the correct identification of \"Apple\" being a company and not a fruit). </p>\n<p>The current state-of-the-art approaches for Named Entity Recognition rely on machine learning, particularly neural network model. In recent years, fine-tuning of large transformer models (typically with over a billion parameters) achieves impressive results on top leaderboards. This is in part due to due the importance of bi-directionality in modern language models, as well as the impact of contexualization, which has improved a variety of other tasks in natural language processing, namely Word Sense Disambiguation (WSD). Popular implementations of NER include Explosion AI's spaCy and Stanford NLP's OpenNLP.</p>\n<h4>Dependency Parsing</h4>\n<p>Dependency grammars are a family of grammar formalisms that allow for a discrete representation of language. Such representations are useful for natural language processing applications due to their structure, which also provide semantic value. Since natural languages are morphologically rich and have a free word order, dependency grammars are well suited to provide semantic relationships between words in a given sentence. In linguistics, there is a notion of a <strong>grammatical relationship</strong>, which provide the approximation for semantic relationships. A grammatical relationship consists of a head and a dependent, where the relationship can also be annotated for further semantic value. For example, typical relations are nominal subjects, direct objects, indirect objects, clausal complements, nominal modifiers, or adjectival modifiers. Having a set of words in a sentence and a collection of their relationships between each other naturally gives rise to a graph structure for the grammatical relations, known as a <strong>dependency tree</strong>. Producing a dependency tree for a specific sentence is a considerable computational task due to the inherent difficulties in working with unstructured natural language. The Penn Treebank dataset, one of the research efforts in the 1990s that galvanized the field of natural language processing, is one of the standards for dependency parsing. In particular, it includes around 2,500 stores from the Wall Street Journal along with their syntactic annotations. </p>\n<p>The current state-of-the-art approaches for dependency parsing, like many other tasks in natural language processing, involve fine-tuning a transformer model to predict the relationships between constituents in a given text. The impact of contexualization and deeper semantic representation has allowed for models in this task to flourish.</p>\n<h2>Related Work</h2>\n<h3>Knowledge Extraction</h3>\n<h4><a href=\"https://arxiv.org/pdf/1805.04270.pdf\">Neural Open Information Extraction</a></h4>\n<h5>Summary</h5>\n<p><em>Neural Open Information Extraction</em> is a paper written by researchers at Microsoft Research Asia in 2018 regarding improving Open Information Extraction (OpenIE). OpenIE is a natural language processing task of extracting a structured representation of knowledge in text. Typically, the extracted information is in the form of a knowledge tuple of the form (subject, relation, object), or another <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span>-ary logical proposition. These knowledge tuples can be used for downstream tasks such as question answering (QA), knowledge graph construction, or natural language understanding (NLU). Since traditional OpenIE engines depend on hand-designed schemas, the approach outlined by the paper instead highly confident arguments and relationship tuples bootstrapped from a state-of-the-art OpenIE system, a similar method to Bosselut et al. The authors develop a novel neural network architecture that is able to generate knowledge tuples, rivalling the predictive performance of traditional OpenIE systems, while still maintaining computational efficency.</p>\n<h5>Analysis</h5>\n<p>The Neural Open Information Extraction system is based on an encoder-decoder neural network architecture, often known as Seq2Seq in the NLP field due to its objective of accepting a sequence as an input and producing a sequence as output. Since these architectures can be trained end-to-end, it also serves to produce an intermediary input representation, often as an embedding of the input sequence. This also has the added benefit of the architecture not needing to rely on hand-crafted patterns, and removing error propagation through the extracted knowledge. In particular, given input word sequence <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">X = (x_1, x_2, ..., x_m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">m</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> and knowledge tuple sequence <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Y = (y_1, y_2, ..., y_n)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, the neural network predicts the conditional distribution <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(Y | X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span> through the chain-rule decomposition:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><msub><mi>y</mi><mn>0</mn></msub><mtext> </mtext><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mtext> </mtext><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mtext> </mtext><mo separator=\"true\">;</mo><mtext> </mtext><msub><mi>x</mi><mn>0</mn></msub><mtext> </mtext><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mtext> </mtext><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(Y | X) = P(Y ~|~ x_1, x_2 ..., x_m)\n= \\prod _{i = 0} ^{n} P(y_i ~ | ~ y_0 ~ ... ~ y_{i - 1} ~ ; ~ x_0 ~ ... ~ x_n)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">m</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">0</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∏</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>Although OpenIE is able to capture structured information in a consistent manner, it is ill-suited to higher level reasoning. For example, if an entity is extracted and known \"to be an adult\", OpenIE will not represent the fact that the entity is not \"known to be a child\". This is by definition, since OpenIE is extractive, and thereby only captures knowledge that is explicilty represented in the text. Other papers related to using neural networks for knowledge graph construction serve to remedy this problem, and often rely on OpenIE as an extractive baseline.</p>\n<h4><a href=\"https://www.aclweb.org/anthology/D19-1250.pdf\">Language Models as Knowledge Bases?</a></h4>\n<h5>Summary</h5>\n<p><em>Language Models as Knowledge Bases</em> is a paper written in 2019 by various researchers at Facebook AI Research (FAIR) and University College London. The paper is inspired by the notion that in the process of learning linguistic knowledge (i.e. language model training), models are possibly able to learn relational data as well, and that such relational data can be probed via cloze statements (or other fill-in-the-blank tasks). The goal of the paper is to explain that language models themselves contain relational data similar to how knowledge bases do, without the need for extensive manual annotating of a structured knowledge graph. The authors show this by evaluating the ability of BERT (without fine-tuning) to complete cloze tasks for factual data, as well as BERT's ability to perform question answering (QA) tasks. In the process, the authors develop a probe named <em>LAMA</em> (LAnguage Model Analysis), which is a set of subject-relation-object triples or question-answer pairs from various disparate sources.</p>\n<h5>Analysis</h5>\n<p>The intuition for the claims about language models made by the paper stem from the modern training process of large language models. In particular, the ability of modern language models to contain such linguistic and relational knowledge is two-fold. The first improvement that such models have to obtain excess factual knowledge than expected is due to the amount of training data used in the model training process. Since transformer language models do not have the bottle-necks in parallelization that recursive models do (e.g. recurrent neural networks), it is possible to train them on troves more data, which often include internet data containing a plethora of declarative/factual information such as Wikipedia. The second recent improvement in language modelling is sheer representational power, as transformer models have a representational capacity order of magnitudes larger than their predecessors, as it is common to see state-of-the-art models with billions of model parameters. The generation process itself may be a contributor to this, as language models tend to exihibit some sort of memorization when generating text, as well as information obfuscation or retrieval itself.</p>\n<p>In addition, the method of information extraction put forth by the paper is particularly interesting to this research. Instead of producing declarative knowledge in a structured manner, the generative approach indirectly probes the model for answers via clever question/prompt formulation, relying on implicit knowledge representation peformed by the model that is hidden to observers. This is in contrast to a more direct approach of extracting knowledge, like the previous gold-standard information extraction approach of OpenIE.</p>\n<p>The authors also note that certain types of factual knowledge are more easily acquired by language models than others. As a result, they argue that they are not measuring the average empirical performance of language models on this task, but are instead measuring a <strong>lower bound</strong> on the ability for language models to acquire knowledge.</p>\n<h3>Knowledge Graph Tasks</h3>\n<h4><a href=\"https://arxiv.org/pdf/1904.02342.pdf\">GraphWriter</a></h4>\n<h5>Summary</h5>\n<p><em>Text Generation from Knowledge Graphs with Graph Transformers</em> is a paper written in 2019 by researchers at the University of Washington, the University of Edinburgh, and the Allen Institute for Artificial Intelligence (AI2). The paper aims to improve the generational abilities of modern transformer language models by augmenting their knowledge abilities through declarative knowledge, in particular knowledge graphs. The paper introduces a novel graph encoding neural network architecture that is capable of understanding the relational structures that comprise knowledge graphs. This encoding architecture is then incorporated into an end-to-end trainable system that is capable of generating downstream text given a knowledge graph as input. The authors show that this architecture is superior than competing architectures through automatic evaluations (e.g. BLEU, METEOR), as well as human annotators agreeing that the produced text is more informative and captures more of the source information. In addition, the authors introduce the AGENDA (Abstract GENeration DAtaset) dataset as their encoder-decoder architecture is trained to generate scientific paper abstracts from their knowledge graph representations.</p>\n<h5>Analysis</h5>\n<p>Broadly, this paper is an advancement of the \"concept-to-text\" model of generation. One powerful result of this paper is showing that it is possible to provide neural language models with the context necessary for generating longer text that still maintains the logical integrity of the provided declarative knowledge. The AGENDA dataset also serves as a good evaluation benchmark for this task due to its generation domain of scientific abstracts since it contains 40,000 examples which all provide clear and consistent text with a heavy emphasis on declarative facts and knowledge. The strong performance on this benchmark may be an artifact of the domain itself, and further research could be conducted to show whether this task is feasible on more free-formed or longer text, such as narrative writing with summarizations.</p>\n<p>The GraphWriter architecture introduced in the paper, which includes a graph transformer neural network, is a highly performant and task-dependent approach to encoding graphical data for use in a neural language model. It is worth noting that decoding strategy used in the paper differs from \"pure generation\" that other papers use (i.e. directly from the model's vocabulary) as the decoding process will additionally sample from the entities contained in the knowledge graph's set of labelled nodes. In particular, the architecture incorporates the two next-token distributions of sampling from the vocabulary (denoted <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>vocab</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{vocab})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">vocab</span></span><span class=\"mclose\">)</span></span></span></span> or copying the text from a sample over the entities (denoted <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>copy</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{copy}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">copy</span></span></span></span></span>). This final probability is calculated as:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo>∗</mo><msup><mi>α</mi><mtext>copy</mtext></msup><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>∗</mo><msup><mi>α</mi><mtext>vocab</mtext></msup></mrow><annotation encoding=\"application/x-tex\">p * \\alpha^\\text{copy} + (1 - p) * \\alpha^\\text{vocab}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6597200000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.797722em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.714392em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">copy</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8991079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">vocab</span></span></span></span></span></span></span></span></span></span></span></span></span>\n<p>Possibly the most important advance in the paper is its attention to information flow. An interesting implementation detail is the inclusion of a \"global vertex\", which is connected to all other vertices in the knowledge graph. The goal of this vertex is to introduce less restricted information flow between all vertices, presumably due to it being otherwise difficult to provide an adequete initialization sequence. Although the paper's novel architecture bears a strong resemblance to the Graph Attention Network (GAT) paper (Velickovic et al. 2018), which uses self-attention over a node's neighbourhood to compute a hidden representation, there exist architecture differences regarding the attention mechanism's use towards incorporating declarative knowledge. These changes related to information flow provide a statistically significant performance increase on the AGENDA task.</p>\n<h4><a href=\"https://arxiv.org/pdf/1906.05317.pdf\">COMET</a></h4>\n<h5>Summary</h5>\n<p><em>COMET: Commensense Transformers for Automatic Knowledge Graph Construction</em> is a paper published by researchers at the Allen Institute for Artificial Intelligence (AI2), the University of Washington, and Microsoft Research in 2019. The paper is primarily concerned with progressing standards in knowledge graph construction, particularly making a paradigm shift from knowledge graph construction as an information task towards a generative task, moving away from the structured schemas of traditional knowledge bases. The paper is also primarily concerned with <strong>commonsense knowledge</strong>, as opposed to the traditional knowledge graph approach of representing domain-specific information about a particular narrative. In doing this, the authors propose a novel transformer architecture named COMET, which is able to augment an existing knowledge graph by generating commonsense descriptions for entities in natural language. The architecture also approaches human performance on the evaluation dataset of ATOMIC and ConceptNet (existing declarative knowledge datasets). As a result, the findings of this paper would indicate that generative models for commonsense may be a path forward to developing knowledge graphs.</p>\n<p>This paper introduces a promising alternative to information extraction through the lens of generative modelling of language. In particular, the crux of the paper lies in the reduction of commonsense acquisition to knowledge base construction. Since previous work on large language models have shown that transformers are able to be useful in tasks related to declarative knowledge, this paper is able to rely on such findings to utilize such models for knowledge graph generation. An interesting caveat is that traditional knowledge graph construction is primarily concerned with representing over a well-defined space of entities and relations that can be directly modelled. In contrast, commensense knowledge is a reasoning task over an ill-defined space of possible entities and relations, and over an unrestricted vocabulary. As a result, it poses a considerably different problem than the tasks that transformers have been known to excel at (i.e. fine-tuning over a defined task space). However, this problem is arguably more well suited to transformers than traditional OpenIE approaches since contextualized language models are able to represent implicit knowledge, whereas extractive methods only consider knowledge that by definition is explicit.</p>\n<h5>Analysis</h5>\n<p>The COMET architecture is a performant framework for constructing commonsense knowledge graphs. COMET first relies on a seed set of knowledge tuples of the form <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>r</mi><mo separator=\"true\">,</mo><mi>o</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s, r, o\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord mathdefault\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">o</span><span class=\"mclose\">}</span></span></span></span>, where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span> is the subject of the knowledge tuple, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span></span></span></span> is the relation of the tuple, and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>o</mi></mrow><annotation encoding=\"application/x-tex\">o</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">o</span></span></span></span> is the object of the phrase. This seed set of knowledge is represented by concatenating the tokens of each part of the tuple into a single tensor:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy=\"false\">{</mo><msup><mi>X</mi><mi>s</mi></msup><mo separator=\"true\">,</mo><msup><mi>X</mi><mi>r</mi></msup><mo separator=\"true\">,</mo><msup><mi>X</mi><mi>o</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">X = \\{X^s, X^r, X^o\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">o</span></span></span></span></span></span></span></span><span class=\"mclose\">}</span></span></span></span></span>\n<p>COMET is then trained end-to-end to predict <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>X</mi><mi>o</mi></msup></mrow><annotation encoding=\"application/x-tex\">X^o</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">o</span></span></span></span></span></span></span></span></span></span></span> given  <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>X</mi><mi>s</mi></msup><mo separator=\"true\">,</mo><msup><mi>X</mi><mi>o</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{X^s, X^o\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">o</span></span></span></span></span></span></span></span><span class=\"mclose\">}</span></span></span></span> using the conditional loglikelihood loss function:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mi mathvariant=\"normal\">∣</mi><mi>s</mi><mi mathvariant=\"normal\">∣</mi><mo>+</mo><mi mathvariant=\"normal\">∣</mi><mi>r</mi><mi mathvariant=\"normal\">∣</mi></mrow><mrow><mi mathvariant=\"normal\">∣</mi><mi>s</mi><mi mathvariant=\"normal\">∣</mi><mo>+</mo><mi mathvariant=\"normal\">∣</mi><mi>r</mi><mi mathvariant=\"normal\">∣</mi><mo>+</mo><mi mathvariant=\"normal\">∣</mi><mi>o</mi><mi mathvariant=\"normal\">∣</mi></mrow></munderover><mi mathvariant=\"normal\">log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L = - \\sum^{|s| + |r| + |o|} _{t = |s| + |r|} \\operatorname*{log} P(x_t | x_{&lt; t})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.4770100000000004em;vertical-align:-1.5160049999999998em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.9610050000000006em;\"><span style=\"top:-1.8089950000000001em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mtight\">∣</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mtight\">∣</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.386005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mtight\">∣</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mtight\">∣</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mtight\">∣</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5160049999999998em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">l</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\" style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&lt;</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.17737em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>In addition, due to the lack of positional knowledge in transformer networks, each word representation <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>e</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">e_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is summed with a positional encoding for the position <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span> of the word:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><msub><mi>e</mi><mi>t</mi></msub><mo>+</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">h_t = e_t + p_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>The performance of this representation may suggest that locational information about knowledge may be of importance, since a similar notion of neighbourhood encoding is often a prevalent feature of graph neural networks and even convolutional neural networks (i.e. receptive fields). It may also be loosely related to the notion of contextualization in language models, where consideration of the positional structure of certain semantic entities is able to provide additional representation power.</p>\n<p>Given the generation nature of the task of generating natural language descriptions of commonsense descriptions, the paper uses BLEU-2 as an evaluation metric. BLEU (Bilingual Evaluation Understudy) measures the quality of a text given a reference baseline using a modified form of precision between <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span>-grams, originally used for machine translation. In the case of COMET, the reference text is the original knowledge tuple's object, which is often represented using natural language text as opposed to a direct enumerable entity. The authors also verify the automatic evaluation methods of both BLEU and perplexity using human annotation (e.g. Mechanical Turk), by asking annotators whether the model's generation consitutes a plausible knowledge tuple. The authors found the evaluations to outperform not only the baselines, but also the state-of-the-art architectures on the same task by a 51% relative performance increase (with statistical significance).</p>\n<p>The paradigm shift from knowledge graph construction from an information extraction task to a text generation task serves as inspiration for this paper. In particular, we leverage transformer models to generate knowledge graphs using a process similar to OpenIE, as well as further analysis into the performance of text generation using existing knowledge tuples.</p>\n<p><img src=\"https://i.imgur.com/LN9NaNp.png\"></p>\n<p><small> An example generated knowledge graph from ATOMIC's seed knowledge tuples. </small></p>\n<h2>Dependency-Based Construction Task</h2>\n<h3>Experimental Design</h3>\n<p>Dataset: <a href=\"http://www.cs.cmu.edu/~dbamman/booksummaries.html\">CMU Book Summaries</a></p>\n<ol>\n<li>Extract knowledge graph using OpenIE.</li>\n<li>Extract knowledge graph using NER + dependency parsing.</li>\n<li>Compare extraction similarities.</li>\n</ol>\n<h3>Pipeline Architecture</h3>\n<h4>SpaCy</h4>\n<p>It is often be a considerable engineering effort to develop an NLP workflow from scratch that is performant enough to work with large datasets. The open source Python package spaCy solves this issue by providing industry-strength and reproducible NLP workflows over a variety of NLP tasks. In this paper, we use spaCy's dependency parsing and named entity extraction pipelines to leverage reproducible pipelines, as well as clear and helpful abstractions over such NLP tasks.</p>\n<h4>HuggingFace Transformers</h4>\n<p>Utilizing pre-trained models can often be a complicated task due to the large number of different language models, as well as their differing input formats. In particular, large-scale language models with billions of parameters can be unwieldly due to their sheer size, making it difficult to implement in Python without GPU acceleration. The HuggingFace Transformers open source Python package solves this by providing a general purpose Python architecture for working with Transformer models, as well as a model registry for fine-tuning existing language models. We leverage the HuggingFace Transformers package for generating text from BERT-based language models.</p>\n<h4>Implementation</h4>\n<p>In this prototype, we leverage Transformer neural network models (i.e. BERT-based fine-tuning) for named entity extraction and dependency parsing, which we use to generate nodes and edges, respectively, for the knowledge graph.</p>\n<p>The core procedure (implemented in Python) is given as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> token<span class=\"token punctuation\">.</span>dep_ <span class=\"token operator\">==</span> <span class=\"token string\">\"ROOT\"</span> <span class=\"token keyword\">and</span> token<span class=\"token punctuation\">.</span>pos_ <span class=\"token operator\">==</span> <span class=\"token string\">\"VERB\"</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># root verb (event) extraction</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">.</span>dep_<span class=\"token punctuation\">,</span> token<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> token<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">for</span> argument <span class=\"token keyword\">in</span> token<span class=\"token punctuation\">.</span>children<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> argument<span class=\"token punctuation\">.</span>dep_ <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"csubj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"nsubj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"nsubjpass\"</span><span class=\"token punctuation\">}</span> <span class=\"token keyword\">and</span> argument<span class=\"token punctuation\">.</span>pos_ <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"PRON\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"NOUN\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"PROPN\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">:</span>\n                <span class=\"token comment\"># named entity extraction</span>\n                <span class=\"token keyword\">if</span> argument<span class=\"token punctuation\">.</span>pos_ <span class=\"token operator\">==</span> <span class=\"token string\">\"PRON\"</span> <span class=\"token keyword\">and</span> argument<span class=\"token punctuation\">.</span>_<span class=\"token punctuation\">.</span>in_coref<span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>argument<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> <span class=\"token string\">\"=\"</span><span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>_<span class=\"token punctuation\">.</span>coref_clusters<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>main<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">)</span>\n\n                <span class=\"token comment\"># subject extraction</span>\n                <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>argument<span class=\"token punctuation\">.</span>dep_<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">)</span>\n\n            <span class=\"token keyword\">if</span> argument<span class=\"token punctuation\">.</span>dep_ <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"dobj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"obj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"iobj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"pobj\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">:</span>\n                <span class=\"token comment\"># object extraction</span>\n                <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>argument<span class=\"token punctuation\">.</span>dep_<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">)</span></code></pre></div>\n<p>This generates a graph <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span> which we are able to export into a useable format for downstream tasks. We also develop auxillary functions for querying and interacting with the knowledge graph.</p>\n<h4>Representation</h4>\n<p>In order to provide the knowledge graph as an input to standard language models, we must first generate a string representation for the graph. The simplest way to do this is to perform a string concatenation between the various nodes and edge annotations, ensuring that nodes and edges appear close-by in the representation.</p>\n<p>This notes a more ambitious, but less robust departure from GraphWriter, which relies on a graph neural network to process the given knowledge graph. </p>\n<h4>Usage</h4>\n<p>After exporting the knowledge graph, we are able to load it into our open source knowledge graph browser, which is built in JavaScript and is self-hostable for researchers.</p>\n<h2>Transformer-based Knowledge Graph Construction/Bootstrapping</h2>\n<h3>Experimental Design</h3>\n<h4>Generative Construction Probe</h4>\n<p>Dataset: <a href=\"http://www.cs.cmu.edu/~dbamman/booksummaries.html\">CMU Book Summaries</a></p>\n<ol>\n<li>Extract knowledge graph using OpenIE + generative approach (OpenIE for subject and relation, generate object).</li>\n<li>Compare accuracy/embedding similarity.</li>\n</ol>\n<h4>Knowledge Graph Reasoning Task</h4>\n<p>Dataset: <a href=\"https://homes.cs.washington.edu/~msap/atomic/\">ATOMIC Dataset</a></p>\n<ol>\n<li>Generate knowledge graph with NER + dependency parsing approach.</li>\n<li>Feed knowledge graph representation to language model and evaluate on ATOMIC dataset.</li>\n</ol>\n<p>TODO</p>\n<h3>Results</h3>\n<p>TODO</p>\n<h3>Comparison</h3>\n<p>TODO</p>\n<h2>Future Work</h2>\n<p>TODO</p>\n<h2>Conclusion</h2>\n<p>TODO</p>\n<h2>Bibliography</h2>\n<ol>\n<li><a href=\"https://blog.google/products/search/introducing-knowledge-graph-things-not/\">https://blog.google/products/search/introducing-knowledge-graph-things-not/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1906.05317.pdf\">https://arxiv.org/pdf/1906.05317.pdf</a></li>\n<li><a href=\"https://arxiv.org/pdf/1610.08763.pdf\">https://arxiv.org/pdf/1610.08763.pdf</a></li>\n<li><a href=\"https://arxiv.org/pdf/1904.02342.pdf\">https://arxiv.org/pdf/1904.02342.pdf</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~mg1/thesis.pdf\">https://www.cs.cmu.edu/~mg1/thesis.pdf</a></li>\n<li><a href=\"https://www.aclweb.org/anthology/D19-1250.pdf\">https://www.aclweb.org/anthology/D19-1250.pdf</a></li>\n</ol>","frontmatter":{"title":"My Undergraduate Thesis (In Progress)","date":"2021-03-16","tags":["machine learning","research"]}}},"pageContext":{"pathSlug":"/blog/thesis"}}}