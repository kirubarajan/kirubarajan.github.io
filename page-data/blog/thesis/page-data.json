{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/thesis","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n<div class=\"notification is-link\">\n  This post will be under construction until I graduate, and is subject to (probably a lot of) change. For any errata, feel free to reach out to me!\n</div>\n\n# My Undergraduate Thesis\n> Generating Declarative Knowledge From Transformer Language Models\n\nAfter (almost) 4 long years at the University of Pennsylvania, I'm finally a senior approaching my final few weeks as a student. Sentiments aside, this means that I'm also wrapping up a lot of the research on machine learning and natural language processing I've done during my time at Penn. For my program, this culminates in a senior thesis (on a topic of my choosing) during my final semester.\n\nA single semester isn't typically enough time to make a significant contribution to the field, so I'm hoping to use this thesis as an opportunity to explore new areas of machine intelligence. I'm really excited to shift some of my focus from my previous research from generative models (i.e. language models) towards knowledge representation.\n\nI'll be using this part of my website as a public workspace for my foray into **information extraction, knowledge graph construction, and machine reasoning**. This will probably include (but isn't limited to) code snippets, equations/derivations, background reading, literature review, and personal hot takes.\n\n---\n\n## Introduction\n\nIn recent years, there has been an explosion of data of various formats (e.g. video, text, sensor), which are not always immediately useful for completing tasks. An overarching goal in the field of machine intelligence is the problem of **knowledge representation**, a field dedicated to designing *efficient* representations of data that capture their respective data.\n\nThis problem is not just a question of information theory, but also relies on biological inspiration. When collecting new information, humans are able to make connections between related data, which enables not only efficient recall but also provides a framework for synthesizing new thoughts or beliefs. Providing such a framework for machines to perform such types of reasoning over previously given data is a goal for the field and, in turn, this paper.\n\nThe approach that this thesis is primarily interested in is the **knowledge graph** model, which models real life (or abstract) entities and their relationships as a graph $G = (V, E)$. Google aptly describes this approach as \"things, not strings\" [1], referring to the notion that these graphs are often mined from pure language but contain a far more powerful semantic meaning.\n\nIn this paper, we explore various knowledge graph construction methods, both from their theoretical merits, as well as their empirical performance provided real natural language data as source material. We also aim to provide an unopinionated framework for expressing declarative knowledge over mined knowledge graphs to aid in logical reasoning. In addition, an engineering goal for this thesis is to make such knowledge graphs accessible to the general public, by offering an open source front-end application that is self-hostable for researchers.\n\n## Background\n\n### Machine Learning\n\nIn recent years, researchers have made incredible progress towards the goal of artificial intelligence through the field of machine learning. For tasks that cannot be completed with imperative rules (e.g. image classification, text generation), traditional algorithms and software fail to produce adequete results. Machine learning serves to remedy this by automatically extracting such patterns and rules from data, and applying them at algorithm runtime to make decisions.\n\n<br />\n\n![](https://drek4537l1klr.cloudfront.net/chollet2/v-3/Figures/ch01-a-new-programming-paradigm.png)\n\n<small> Source: François Chollet </small>\n\n<br />\n\nAlthough algorithms to make predictions from data have existed for many decades, the recent explosion in machine learning progress can be attributed to two key developments: more computing resources and more data. The first development is a byproduct of both Moore's Law, as well as readily available compute being made more prevelant by cloud providers such as Amazon's AWS or Microsoft's Azure. The latter development in machine learning progress is partly brought about by the former (i.e. more storage availability for big data), but is also made possible by an increased number of devices connected to the internet, providing troves of data to analyze.\n\nRegarding this paper on knowledge graphs, machine learning has made it possible to automatically mine such graphical representations, instead of hand-annotating potentially billions of entities and their relations (as is the case with Google's Knowledge Graph, presumably the largest in existence).\n\n#### Neural Networks\n\nThe most prevalent model in the machine learning renaissance is the **neural network**, which also goes by the name of **deep learning**. As the name implies, neural networks are machine learning models with a biological inspiration of neurons in animal brains. In particular, neurons can fire in accordance with other \"similarly wired\" neurons, where the strength between neurons (or synapses) can be modelled using continuous weights. \n\nIn **feed-forward** neural networks (pictured below), we can often stack neurons in order to create a longer synapse path between the input layer and the output layer, thus creating the need for additional learnable model parameters. This increase in model capacity provides representational power to the neural network, thus giving the name **deep learning** (due to networks with hidden layers being \"deep\").\n\n![](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png)\n<small> Source: IBM </small>\n\nNeural networks offer a primary advantage of not requiring hand-picked or discrete features during inference. This is especially important when working when modelling data types that do not have explicitly identifiable features, such as language or image data. The downside to this power is that neural networks require orders of magnitude more training data than classical machine learning models (such as random forests or linear regression).\n\nNeural Network parameters are generally stochastically optimized using **gradient descent**. The process for doing this is to first define an auxillary **loss function** that measures the \"correctness\" of the model's output with respect to some training data, known as a *forward pass*. Next, the gradients of the model parameters with respect to the loss function are computed using the chain rule of calculus in a dynamic programming algorithm named **backpropagation**, also known as the *backward pass*. Finally, the model parameters are updated in accordance with their gradients in order to minimize the loss function. The stochasticity in the optimization process arrives from sampling a **batch** of training examples at a time in order to leverage hardware parallelization (particularly in GPUs). \n\nThe gradient descent update rule is given by:\n\n$$\nw_{t + 1} = w_t - \\alpha \\nabla w_t\n$$\n\nwhere $\\alpha$ is the **learning rate**, often an integer in the range [0.01, 0.05].\n\n### Transformers\n\nThe current state-of-the-art in various natural language processing tasks (particularly related to information extraction and text generation) are **transformer** models, which have been popularized by Vatswani et. al in 2017. These models are largely extensions of feed-forward neural networks, and mark a departure from the previous state-of-the-art models that were variations of **recurrent neural networks**. Due to their feed-forward nature, transformer networks are highly parallelizable, and thereby making it feasible to train of vast quantities of language data.\n\n![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n\n#### Connection to Graph Neural Networks\n\nSince the attention mechanism can be interpreted as a connected graph, with edged annotated with their various weights (or attention strenghths), transformer models are closely related to neural networks that operate over graphs, known as **graph neural networks**. In particular, a challenge in graph neural networks is the requirement that nodes are encoded in a representation that captures information about the local structure, namely to derive semantic value from a given node's neighbourhood. In transformer models, this same requirement is prevalent as well, since the feed-forward nature of the neural network requires that .\n\n### Language Models\n\nWe can use the chain rule in probability to break down a sequence (or sentence) of words into step-by-step calculations. For example, if we are considering the probability of the phrase \"cat in the hat\":\n\n$$\nP(\\text{cat in the hat})\n$$\n\nWe can break this value down into the product of the following terms (where $\\text{<s>}$ denotes the starting token):\n\n$$\nP(\\text{<s>})\n\\newline\nP(\\text{cat} ~|~ \\text{<s>})\n\\newline \nP(\\text{in} ~|~ \\text{<s> cat}) \n\\newline \nP(\\text{the} ~|~ \\text{<s> cat in}) \n\\newline \nP(\\text{hat} ~|~ \\text{<s> cat in the}) \n$$\n\nThese probabilities are provided by a **language model**. In essence, a language model’s purpose is to provide $P(w ~|~ c)$, where $w$ is a particular target word (i.e. the next word) and $c$ is the context that precedes the target word. Using a trained model, we can use $P(w ~|~ c)$ to create a distribution of the likelihood for the next word.\n\n#### Text Generation\n\nNow, we turn our focus to using these probabilities to **create** text. Let's determine what the first word of our generation would be. Similar to the previous example, where we have a $\\text{<s>}$ token to signify the beginning of a sequence, we can ask the model what the value of $P(w ~|~ \\text{<s>})$ for a variety of different values of $w$.\n\nMore broadly, our goal is to select the words that maximize:\n\n$$\n\\prod _{i = 0} ^{n} P(w_i ~ | ~ c_0 ~ ... ~ c_{i - 1})\n$$\n\n### Machine Reasoning\n\n#### Reasoning Tasks\n\nTODO\n\n#### Knowledge Graphs\n\nFor this paper, the knowledge representation scheme that we are concerned with are **knowledge graphs**. As the name implies, these are graphical data models that collect a set of *entities*, as well as various relationships between them. For example, Google Search incorporates a knowledge graph of various entities from the results of web searches (e.g. President Barack Obama), which allows it to provide information about the entity without performing additional web crawls.\n\n![](https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/5ff89cbc-ea1e-4ab0-b646-877369cad553/File/99553b788b5a24eb03c3e35e6917c008/disease_symptom_healthcare_knowledge_graph.png)\n\n<small> Source: Oracle </small>\n\nCollaborative human approaches to knowledge graph constructions have existed as volunteer efforts, namely through [Wikidata]() and [Freebase](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&rep=rep1&type=pdf). However, the limitations to human approaches are (by nature) their inability to automatically adapt to new raw text, as well as their reliance on humans, which are either costly or time intensive (or both).\n\n### Information Extraction\n\n#### Named Entity Recognition\n\nTODO\n\n#### Dependency Parsing\n\nTODO\n\n## Related Work\n\n### Knowledge Extraction\n\n#### [OpenIE](https://nlp.stanford.edu/pubs/2015angeli-openie.pdf)\n##### Summary\nTODO\n##### Analysis\nTODO\n\n#### [Language Models as Knowledge Bases?](https://www.aclweb.org/anthology/D19-1250.pdf)\n##### Summary\nTODO\n##### Analysis\nTODO\n\n### Knowledge Graph Tasks\n\n#### [COMET](https://arxiv.org/pdf/1906.05317.pdf)\n##### Summary\nTODO\n##### Analysis\nTODO\n\n#### [GraphWriter](https://arxiv.org/pdf/1904.02342.pdf)\n##### Summary\nTODO\n##### Analysis\nTODO\n\n## Dependency-Based Construction Task\n\n### Experimental Design\nDataset: [CMU Book Summaries](http://www.cs.cmu.edu/~dbamman/booksummaries.html)\n\n1. Extract knowledge graph using OpenIE.\n2. Extract knowledge graph using NER + dependency parsing.\n3. Compare extraction similarities.\n\n### Pipeline Architecture\n\n#### SpaCy\n\nIt can often be a considerable engineering effort to develop an NLP workflow from scratch that is performant enough to work with large datasets. The open source Python package spaCy solves this issue by providing industry-strength and reproducible NLP workflows over a variety of NLP tasks. In this thesis, we use spaCy's dependency parsing and named entity extraction pipelines to leverage reproducible pipelines, as well as clear and helpful abstractions over such NLP tasks.\n\n#### HuggingFace Transformers\n\nUtilizing pre-trained models can often be a complicated task due to the large number of different language models, as well as their differing input formats. In particular, large-scale language models with billions of parameters can be unwieldly due to their sheer size, making it difficult to implement in Python without GPU acceleration. The HuggingFace Transformers open source Python package solves this by providing a general purpose Python architecture for working with Transformer models, as well as a model registry for fine-tuning existing language models. We leverage the HuggingFace Transformers package for generating text from BERT-based language models.\n\n#### Implementation\n\nIn this prototype, we leverage Transformer neural network models (i.e. BERT-based fine-tuning) for named entity extraction and dependency parsing, which we use to generate nodes and edges, respectively, for the knowledge graph.\n\nThe core procedure (implemented in Python) is given as follows:\n\n```python\nfor token in doc:\n    if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n        # root verb (event) extraction\n        print(token.dep_, token.text, token.pos_)\n\n        for argument in token.children:\n            if argument.dep_ in {\"csubj\", \"nsubj\", \"nsubjpass\"} and argument.pos_ in {\"PRON\", \"NOUN\", \"PROPN\"}:\n                # named entity extraction\n                if argument.pos_ == \"PRON\" and argument._.in_coref:\n                    print(argument.text, \"=\", argument._.coref_clusters[0].main.text)\n\n                # subject extraction\n                print(argument.dep_, argument.text, argument.pos_)\n\n            if argument.dep_ in {\"dobj\", \"obj\", \"iobj\", \"pobj\"}:\n                # object extraction\n                print(argument.dep_, argument.text, argument.pos_)\n```\n\nThis generates a graph $G = (V, E)$ which we are able to export into a useable format for downstream tasks. We also develop auxillary functions for querying and interacting with the knowledge graph.\n\n#### Representation\n\nIn order to provide the knowledge graph as an input to standard language models, we must first generate a string representation for the graph. The simplest way to do this is to perform a string concatenation between the various nodes and edge annotations, ensuring that nodes and edges appear close-by in the representation.\n\nThis notes a more ambitious, but less robust departure from GraphWriter, which relies on a graph neural network to process the given knowledge graph. \n\n#### Usage\n\nAfter exporting the knowledge graph, we are able to load it into our open source knowledge graph browser, which is built in JavaScript and is self-hostable for researchers.\n\n## Transformer-based Knowledge Graph Construction/Bootstrapping\n\n### Experimental Design\n\n#### Generative Construction Probe\nDataset: [CMU Book Summaries](http://www.cs.cmu.edu/~dbamman/booksummaries.html)\n\n1. Extract knowledge graph using OpenIE + generative approach (OpenIE for subject and relation, generate object).\n2. Compare accuracy/embedding similarity.\n\n#### Knowledge Graph Reasoning Task\nDataset: [ATOMIC Dataset](https://homes.cs.washington.edu/~msap/atomic/)\n\n1. Generate knowledge graph with NER + dependency parsing approach.\n2. Feed knowledge graph representation to language model and evaluate on ATOMIC dataset.\n\nTODO\n\n### Results\n\nTODO\n\n### Comparison\n\nTODO\n\n## Future Work\n\nTODO\n\n## Conclusion\n\nTODO\n\n## Bibliography\n1. https://blog.google/products/search/introducing-knowledge-graph-things-not/\n2. https://arxiv.org/pdf/1906.05317.pdf\n3. https://arxiv.org/pdf/1610.08763.pdf\n4. https://arxiv.org/pdf/1904.02342.pdf\n5. https://www.cs.cmu.edu/~mg1/thesis.pdf\n6. https://www.aclweb.org/anthology/D19-1250.pdf","html":"<div class=\"notification is-link\">\n  This post will be under construction until I graduate, and is subject to (probably a lot of) change. For any errata, feel free to reach out to me!\n</div>\n<h1>My Undergraduate Thesis</h1>\n<blockquote>\n<p>Generating Declarative Knowledge From Transformer Language Models</p>\n</blockquote>\n<p>After (almost) 4 long years at the University of Pennsylvania, I'm finally a senior approaching my final few weeks as a student. Sentiments aside, this means that I'm also wrapping up a lot of the research on machine learning and natural language processing I've done during my time at Penn. For my program, this culminates in a senior thesis (on a topic of my choosing) during my final semester.</p>\n<p>A single semester isn't typically enough time to make a significant contribution to the field, so I'm hoping to use this thesis as an opportunity to explore new areas of machine intelligence. I'm really excited to shift some of my focus from my previous research from generative models (i.e. language models) towards knowledge representation.</p>\n<p>I'll be using this part of my website as a public workspace for my foray into <strong>information extraction, knowledge graph construction, and machine reasoning</strong>. This will probably include (but isn't limited to) code snippets, equations/derivations, background reading, literature review, and personal hot takes.</p>\n<hr>\n<h2>Introduction</h2>\n<p>In recent years, there has been an explosion of data of various formats (e.g. video, text, sensor), which are not always immediately useful for completing tasks. An overarching goal in the field of machine intelligence is the problem of <strong>knowledge representation</strong>, a field dedicated to designing <em>efficient</em> representations of data that capture their respective data.</p>\n<p>This problem is not just a question of information theory, but also relies on biological inspiration. When collecting new information, humans are able to make connections between related data, which enables not only efficient recall but also provides a framework for synthesizing new thoughts or beliefs. Providing such a framework for machines to perform such types of reasoning over previously given data is a goal for the field and, in turn, this paper.</p>\n<p>The approach that this thesis is primarily interested in is the <strong>knowledge graph</strong> model, which models real life (or abstract) entities and their relationships as a graph <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span>. Google aptly describes this approach as \"things, not strings\" [1], referring to the notion that these graphs are often mined from pure language but contain a far more powerful semantic meaning.</p>\n<p>In this paper, we explore various knowledge graph construction methods, both from their theoretical merits, as well as their empirical performance provided real natural language data as source material. We also aim to provide an unopinionated framework for expressing declarative knowledge over mined knowledge graphs to aid in logical reasoning. In addition, an engineering goal for this thesis is to make such knowledge graphs accessible to the general public, by offering an open source front-end application that is self-hostable for researchers.</p>\n<h2>Background</h2>\n<h3>Machine Learning</h3>\n<p>In recent years, researchers have made incredible progress towards the goal of artificial intelligence through the field of machine learning. For tasks that cannot be completed with imperative rules (e.g. image classification, text generation), traditional algorithms and software fail to produce adequete results. Machine learning serves to remedy this by automatically extracting such patterns and rules from data, and applying them at algorithm runtime to make decisions.</p>\n<br />\n<p><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-3/Figures/ch01-a-new-programming-paradigm.png\"></p>\n<p><small> Source: François Chollet </small></p>\n<br />\n<p>Although algorithms to make predictions from data have existed for many decades, the recent explosion in machine learning progress can be attributed to two key developments: more computing resources and more data. The first development is a byproduct of both Moore's Law, as well as readily available compute being made more prevelant by cloud providers such as Amazon's AWS or Microsoft's Azure. The latter development in machine learning progress is partly brought about by the former (i.e. more storage availability for big data), but is also made possible by an increased number of devices connected to the internet, providing troves of data to analyze.</p>\n<p>Regarding this paper on knowledge graphs, machine learning has made it possible to automatically mine such graphical representations, instead of hand-annotating potentially billions of entities and their relations (as is the case with Google's Knowledge Graph, presumably the largest in existence).</p>\n<h4>Neural Networks</h4>\n<p>The most prevalent model in the machine learning renaissance is the <strong>neural network</strong>, which also goes by the name of <strong>deep learning</strong>. As the name implies, neural networks are machine learning models with a biological inspiration of neurons in animal brains. In particular, neurons can fire in accordance with other \"similarly wired\" neurons, where the strength between neurons (or synapses) can be modelled using continuous weights. </p>\n<p>In <strong>feed-forward</strong> neural networks (pictured below), we can often stack neurons in order to create a longer synapse path between the input layer and the output layer, thus creating the need for additional learnable model parameters. This increase in model capacity provides representational power to the neural network, thus giving the name <strong>deep learning</strong> (due to networks with hidden layers being \"deep\").</p>\n<p><img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\">\n<small> Source: IBM </small></p>\n<p>Neural networks offer a primary advantage of not requiring hand-picked or discrete features during inference. This is especially important when working when modelling data types that do not have explicitly identifiable features, such as language or image data. The downside to this power is that neural networks require orders of magnitude more training data than classical machine learning models (such as random forests or linear regression).</p>\n<p>Neural Network parameters are generally stochastically optimized using <strong>gradient descent</strong>. The process for doing this is to first define an auxillary <strong>loss function</strong> that measures the \"correctness\" of the model's output with respect to some training data, known as a <em>forward pass</em>. Next, the gradients of the model parameters with respect to the loss function are computed using the chain rule of calculus in a dynamic programming algorithm named <strong>backpropagation</strong>, also known as the <em>backward pass</em>. Finally, the model parameters are updated in accordance with their gradients in order to minimize the loss function. The stochasticity in the optimization process arrives from sampling a <strong>batch</strong> of training examples at a time in order to leverage hardware parallelization (particularly in GPUs). </p>\n<p>The gradient descent update rule is given by:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mi>t</mi></msub><mo>−</mo><mi>α</mi><mi mathvariant=\"normal\">∇</mi><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{t + 1} = w_t - \\alpha \\nabla w_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord\">∇</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span> is the <strong>learning rate</strong>, often an integer in the range [0.01, 0.05].</p>\n<h3>Transformers</h3>\n<p>The current state-of-the-art in various natural language processing tasks (particularly related to information extraction and text generation) are <strong>transformer</strong> models, which have been popularized by Vatswani et. al in 2017. These models are largely extensions of feed-forward neural networks, and mark a departure from the previous state-of-the-art models that were variations of <strong>recurrent neural networks</strong>. Due to their feed-forward nature, transformer networks are highly parallelizable, and thereby making it feasible to train of vast quantities of language data.</p>\n<p><img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\"></p>\n<h4>Connection to Graph Neural Networks</h4>\n<p>Since the attention mechanism can be interpreted as a connected graph, with edged annotated with their various weights (or attention strenghths), transformer models are closely related to neural networks that operate over graphs, known as <strong>graph neural networks</strong>. In particular, a challenge in graph neural networks is the requirement that nodes are encoded in a representation that captures information about the local structure, namely to derive semantic value from a given node's neighbourhood. In transformer models, this same requirement is prevalent as well, since the feed-forward nature of the neural network requires that .</p>\n<h3>Language Models</h3>\n<p>We can use the chain rule in probability to break down a sequence (or sentence) of words into step-by-step calculations. For example, if we are considering the probability of the phrase \"cat in the hat\":</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>cat in the hat</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(\\text{cat in the hat})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">cat in the hat</span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>We can break this value down into the product of the following terms (where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>&lt;s&gt;</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{&lt;s&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span></span></span></span> denotes the starting token):</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>&lt;s&gt;</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>cat </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt;</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>in </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt; cat</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>the </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt; cat in</mtext><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>hat </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt; cat in the</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(\\text{&lt;s&gt;})\n\\newline\nP(\\text{cat} ~|~ \\text{&lt;s&gt;})\n\\newline \nP(\\text{in} ~|~ \\text{&lt;s&gt; cat}) \n\\newline \nP(\\text{the} ~|~ \\text{&lt;s&gt; cat in}) \n\\newline \nP(\\text{hat} ~|~ \\text{&lt;s&gt; cat in the}) </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">cat</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">in</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt; cat</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">the</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt; cat in</span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">hat</span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt; cat in the</span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>These probabilities are provided by a <strong>language model</strong>. In essence, a language model’s purpose is to provide <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span></span></span></span>, where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> is a particular target word (i.e. the next word) and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> is the context that precedes the target word. Using a trained model, we can use <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span></span></span></span> to create a distribution of the likelihood for the next word.</p>\n<h4>Text Generation</h4>\n<p>Now, we turn our focus to using these probabilities to <strong>create</strong> text. Let's determine what the first word of our generation would be. Similar to the previous example, where we have a <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>&lt;s&gt;</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{&lt;s&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span></span></span></span> token to signify the beginning of a sequence, we can ask the model what the value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> &lt;s&gt;</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w ~|~ \\text{&lt;s&gt;})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord text\"><span class=\"mord\">&lt;s&gt;</span></span><span class=\"mclose\">)</span></span></span></span> for a variety of different values of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span>.</p>\n<p>More broadly, our goal is to select the words that maximize:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><msub><mi>c</mi><mn>0</mn></msub><mtext> </mtext><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mtext> </mtext><msub><mi>c</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\prod _{i = 0} ^{n} P(w_i ~ | ~ c_0 ~ ... ~ c_{i - 1})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">0</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∏</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<h3>Machine Reasoning</h3>\n<h4>Reasoning Tasks</h4>\n<p>TODO</p>\n<h4>Knowledge Graphs</h4>\n<p>For this paper, the knowledge representation scheme that we are concerned with are <strong>knowledge graphs</strong>. As the name implies, these are graphical data models that collect a set of <em>entities</em>, as well as various relationships between them. For example, Google Search incorporates a knowledge graph of various entities from the results of web searches (e.g. President Barack Obama), which allows it to provide information about the entity without performing additional web crawls.</p>\n<p><img src=\"https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/5ff89cbc-ea1e-4ab0-b646-877369cad553/File/99553b788b5a24eb03c3e35e6917c008/disease_symptom_healthcare_knowledge_graph.png\"></p>\n<p><small> Source: Oracle </small></p>\n<p>Collaborative human approaches to knowledge graph constructions have existed as volunteer efforts, namely through <a href=\"\">Wikidata</a> and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&#x26;rep=rep1&#x26;type=pdf\">Freebase</a>. However, the limitations to human approaches are (by nature) their inability to automatically adapt to new raw text, as well as their reliance on humans, which are either costly or time intensive (or both).</p>\n<h3>Information Extraction</h3>\n<h4>Named Entity Recognition</h4>\n<p>TODO</p>\n<h4>Dependency Parsing</h4>\n<p>TODO</p>\n<h2>Related Work</h2>\n<h3>Knowledge Extraction</h3>\n<h4><a href=\"https://nlp.stanford.edu/pubs/2015angeli-openie.pdf\">OpenIE</a></h4>\n<h5>Summary</h5>\n<p>TODO</p>\n<h5>Analysis</h5>\n<p>TODO</p>\n<h4><a href=\"https://www.aclweb.org/anthology/D19-1250.pdf\">Language Models as Knowledge Bases?</a></h4>\n<h5>Summary</h5>\n<p>TODO</p>\n<h5>Analysis</h5>\n<p>TODO</p>\n<h3>Knowledge Graph Tasks</h3>\n<h4><a href=\"https://arxiv.org/pdf/1906.05317.pdf\">COMET</a></h4>\n<h5>Summary</h5>\n<p>TODO</p>\n<h5>Analysis</h5>\n<p>TODO</p>\n<h4><a href=\"https://arxiv.org/pdf/1904.02342.pdf\">GraphWriter</a></h4>\n<h5>Summary</h5>\n<p>TODO</p>\n<h5>Analysis</h5>\n<p>TODO</p>\n<h2>Dependency-Based Construction Task</h2>\n<h3>Experimental Design</h3>\n<p>Dataset: <a href=\"http://www.cs.cmu.edu/~dbamman/booksummaries.html\">CMU Book Summaries</a></p>\n<ol>\n<li>Extract knowledge graph using OpenIE.</li>\n<li>Extract knowledge graph using NER + dependency parsing.</li>\n<li>Compare extraction similarities.</li>\n</ol>\n<h3>Pipeline Architecture</h3>\n<h4>SpaCy</h4>\n<p>It can often be a considerable engineering effort to develop an NLP workflow from scratch that is performant enough to work with large datasets. The open source Python package spaCy solves this issue by providing industry-strength and reproducible NLP workflows over a variety of NLP tasks. In this thesis, we use spaCy's dependency parsing and named entity extraction pipelines to leverage reproducible pipelines, as well as clear and helpful abstractions over such NLP tasks.</p>\n<h4>HuggingFace Transformers</h4>\n<p>Utilizing pre-trained models can often be a complicated task due to the large number of different language models, as well as their differing input formats. In particular, large-scale language models with billions of parameters can be unwieldly due to their sheer size, making it difficult to implement in Python without GPU acceleration. The HuggingFace Transformers open source Python package solves this by providing a general purpose Python architecture for working with Transformer models, as well as a model registry for fine-tuning existing language models. We leverage the HuggingFace Transformers package for generating text from BERT-based language models.</p>\n<h4>Implementation</h4>\n<p>In this prototype, we leverage Transformer neural network models (i.e. BERT-based fine-tuning) for named entity extraction and dependency parsing, which we use to generate nodes and edges, respectively, for the knowledge graph.</p>\n<p>The core procedure (implemented in Python) is given as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> token<span class=\"token punctuation\">.</span>dep_ <span class=\"token operator\">==</span> <span class=\"token string\">\"ROOT\"</span> <span class=\"token keyword\">and</span> token<span class=\"token punctuation\">.</span>pos_ <span class=\"token operator\">==</span> <span class=\"token string\">\"VERB\"</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># root verb (event) extraction</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">.</span>dep_<span class=\"token punctuation\">,</span> token<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> token<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">for</span> argument <span class=\"token keyword\">in</span> token<span class=\"token punctuation\">.</span>children<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> argument<span class=\"token punctuation\">.</span>dep_ <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"csubj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"nsubj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"nsubjpass\"</span><span class=\"token punctuation\">}</span> <span class=\"token keyword\">and</span> argument<span class=\"token punctuation\">.</span>pos_ <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"PRON\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"NOUN\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"PROPN\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">:</span>\n                <span class=\"token comment\"># named entity extraction</span>\n                <span class=\"token keyword\">if</span> argument<span class=\"token punctuation\">.</span>pos_ <span class=\"token operator\">==</span> <span class=\"token string\">\"PRON\"</span> <span class=\"token keyword\">and</span> argument<span class=\"token punctuation\">.</span>_<span class=\"token punctuation\">.</span>in_coref<span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>argument<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> <span class=\"token string\">\"=\"</span><span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>_<span class=\"token punctuation\">.</span>coref_clusters<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>main<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">)</span>\n\n                <span class=\"token comment\"># subject extraction</span>\n                <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>argument<span class=\"token punctuation\">.</span>dep_<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">)</span>\n\n            <span class=\"token keyword\">if</span> argument<span class=\"token punctuation\">.</span>dep_ <span class=\"token keyword\">in</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"dobj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"obj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"iobj\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"pobj\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">:</span>\n                <span class=\"token comment\"># object extraction</span>\n                <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>argument<span class=\"token punctuation\">.</span>dep_<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">,</span> argument<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">)</span></code></pre></div>\n<p>This generates a graph <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span> which we are able to export into a useable format for downstream tasks. We also develop auxillary functions for querying and interacting with the knowledge graph.</p>\n<h4>Representation</h4>\n<p>In order to provide the knowledge graph as an input to standard language models, we must first generate a string representation for the graph. The simplest way to do this is to perform a string concatenation between the various nodes and edge annotations, ensuring that nodes and edges appear close-by in the representation.</p>\n<p>This notes a more ambitious, but less robust departure from GraphWriter, which relies on a graph neural network to process the given knowledge graph. </p>\n<h4>Usage</h4>\n<p>After exporting the knowledge graph, we are able to load it into our open source knowledge graph browser, which is built in JavaScript and is self-hostable for researchers.</p>\n<h2>Transformer-based Knowledge Graph Construction/Bootstrapping</h2>\n<h3>Experimental Design</h3>\n<h4>Generative Construction Probe</h4>\n<p>Dataset: <a href=\"http://www.cs.cmu.edu/~dbamman/booksummaries.html\">CMU Book Summaries</a></p>\n<ol>\n<li>Extract knowledge graph using OpenIE + generative approach (OpenIE for subject and relation, generate object).</li>\n<li>Compare accuracy/embedding similarity.</li>\n</ol>\n<h4>Knowledge Graph Reasoning Task</h4>\n<p>Dataset: <a href=\"https://homes.cs.washington.edu/~msap/atomic/\">ATOMIC Dataset</a></p>\n<ol>\n<li>Generate knowledge graph with NER + dependency parsing approach.</li>\n<li>Feed knowledge graph representation to language model and evaluate on ATOMIC dataset.</li>\n</ol>\n<p>TODO</p>\n<h3>Results</h3>\n<p>TODO</p>\n<h3>Comparison</h3>\n<p>TODO</p>\n<h2>Future Work</h2>\n<p>TODO</p>\n<h2>Conclusion</h2>\n<p>TODO</p>\n<h2>Bibliography</h2>\n<ol>\n<li><a href=\"https://blog.google/products/search/introducing-knowledge-graph-things-not/\">https://blog.google/products/search/introducing-knowledge-graph-things-not/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1906.05317.pdf\">https://arxiv.org/pdf/1906.05317.pdf</a></li>\n<li><a href=\"https://arxiv.org/pdf/1610.08763.pdf\">https://arxiv.org/pdf/1610.08763.pdf</a></li>\n<li><a href=\"https://arxiv.org/pdf/1904.02342.pdf\">https://arxiv.org/pdf/1904.02342.pdf</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~mg1/thesis.pdf\">https://www.cs.cmu.edu/~mg1/thesis.pdf</a></li>\n<li><a href=\"https://www.aclweb.org/anthology/D19-1250.pdf\">https://www.aclweb.org/anthology/D19-1250.pdf</a></li>\n</ol>","frontmatter":{"title":"My Undergraduate Thesis (In Progress)","date":"2021-03-16","tags":["machine learning","research"]}}},"pageContext":{"pathSlug":"/blog/thesis"}}}