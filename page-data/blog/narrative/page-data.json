{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/narrative","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# Unsupervised Learning of Narrative Event Chains\n*Nathanael Chambers and Dan Jurafsky (2008)*\n\nIn Fall 2019, I worked on an updated implementation of *Unsupervised Learning of Narrative Event Chains* by Chambers and Jurafsky (2008) as part of an independent study project at the University of Pennsylvania, advised by Chris Callison-Burch. The overall goal of the project is to learn discrete representations of narrative knowledge through **Narrative Events** and orderings known as **Narrative Chains**. \n\nHand-written scripts were used in NLP in the 1980s as a structured representation of a body of text. In this paper, such scripts are learned for narrative text, referred to as **narrative chains**. These chains not only provide a representation of the source text, but also encode subject/verb semantics and temporal orderings of events as well. From the paper: \"Since we are focusing on a single actor in this study, a narrative event is thus a tuple of the event and the typed dependency of the protagonist\". Let's formalize:\n\n## The Paper\n\nThe contributions of this paper are three-fold: 1) learning unsupervised relations between entities, 2) temporal ordering of narrative events, 3) pruning of the narrative chains into discrete sets.\n\n### The Narrative Chain Model\nThe authors define two key terminology: **narrative chains** and **narrative events**. Narrative Events are defined as tuples of an event and its participants, represented as typed dependencies. This paper only considers single actors as **protagonists** and as such narrative events are a tuple of the event and the typed dependency of the protagonist: *(event, dependency)*. Narrative Chains are therefore defined as a partially ordered set of narrative events that share a common protagonist/actor. Formally this is defined as $\\{e_1, e_2, ..., e_n \\}$ where $n$ is the length of the chain and relationship $B(e_i, e_j)$ is true if and only if event $i$ occurs strictly before event $j$.\n\n### Learning Narrative Relations\nGiven a list of observed verb/dependency frequencies, we can compute the pointwise mutual information between these occurances as:\n\n$$\nPMI[e(w, d), e(v, g)] = \\operatorname{log} \\frac{P[e(w, d), e(v, g)]}{P[e(w, d)] \\cdot P[e(v, g)]}\n$$\n\nwhere $e(w, d)$ is the verb/dependency pair between $w$ and $d$.\n\n### Evaluation\nEvaluation is performed using the *Narrative Cloze* Evaluation Task for narrative coherence. A narrative chain is provided to the task and an event is removed in order for the model to perform a prediction to be evaluated on. The aim of the task is to perform a fill-in-the-blanks task, which upon successful completion indicates the presence of coherent narrative knowledge by the model. Given of tuple list of `(chain, event)` where `chain` is missing the true prediction `event`, the evaluation module returns the average model position. The model position is defined as the true event's position in the model's ranked candidate outputs (lower is better).\n\n## My Implementation\nMy implementation of (Chambers and Jurafsky, 2008) uses updated libaries, classes and functions. Written in Python, using the Stanford CoreNLP library (updated dependency parsing from transition model to neural-based Universal Dependencies) as well as the SpaCy pipeline for neural network models (with extensions from HuggingFace). I also extended the project to use NLP's secret sauce: word embeddings. An interpolated model between pointwise mutual information and cosine similarity shows strong results with low amounts of training data.\n\nThe following libraries are used throughout the study:\n1. Stanford CoreNLP Python Implementation (`stanfordnlp`)\n2. SpaCy Dependency Parser (`spacy`)\n3. HuggingFace Neural Coreference Resolution (`neuralcoref`)\n\nExtensions include:\n1. Magnitude Embedding Library (`pymagnitude`)\n2. Word2Vec Google News Skip-Gram Model\n\n### Examples\nExamples of identified narrative events in the format `(subject, verb, dependency, dependency_type, probability)`:\n\n```\nyou kiss girl dobj 0.00023724792408066428\nthat enables users dobj 0.00023724792408066428\nGod bestows benefaction dobj 0.00023724792408066428\nAstronomers observed planets dobj 0.00023724792408066428\n```\n\nExamples of generated narrative chains (using a Greedy Decoding strategy):\n\n*(Embedding-Similary Based)*\n```\nseed event: play I dsubj -> I play\nscore nsubj -> I score\nwin nsubj -> I win\nbeat nubj -> I beat\n```\n\n*(Pointwise Mutual Information Approximation Based)*\n```\nseed event:  go I nsubj -> I go\nget nsubj -> I get\ndo nsubj -> I do\nwant nsubj -> I want\n```\n\n### Implementation Notes \n1. verb space too large -> lemmatizing verbs before parsing\n2. events are similar to themselves -> removing seen verbs in chain from prediction candidates\n3. coreference resolution fails occasionally -> increase chunk size\n4. parsing is slow -> single grammatical pass and resolve entities ad-hoc\n5. coreference count computation is slow -> refactor to matrix implementation\n\n## Conclusion\nThis was a really interesting approach to modelling narrative semantics. I'm currently taking an [advanced seminar course](interactive-fiction-class.org/) in text generation and interactive fiction, and I hope to draw inspiration from this project to state of the art models/games such as GPT-2 and AI Dungeon 2!","html":"<h1>Unsupervised Learning of Narrative Event Chains</h1>\n<p><em>Nathanael Chambers and Dan Jurafsky (2008)</em></p>\n<p>In Fall 2019, I worked on an updated implementation of <em>Unsupervised Learning of Narrative Event Chains</em> by Chambers and Jurafsky (2008) as part of an independent study project at the University of Pennsylvania, advised by Chris Callison-Burch. The overall goal of the project is to learn discrete representations of narrative knowledge through <strong>Narrative Events</strong> and orderings known as <strong>Narrative Chains</strong>. </p>\n<p>Hand-written scripts were used in NLP in the 1980s as a structured representation of a body of text. In this paper, such scripts are learned for narrative text, referred to as <strong>narrative chains</strong>. These chains not only provide a representation of the source text, but also encode subject/verb semantics and temporal orderings of events as well. From the paper: \"Since we are focusing on a single actor in this study, a narrative event is thus a tuple of the event and the typed dependency of the protagonist\". Let's formalize:</p>\n<h2>The Paper</h2>\n<p>The contributions of this paper are three-fold: 1) learning unsupervised relations between entities, 2) temporal ordering of narrative events, 3) pruning of the narrative chains into discrete sets.</p>\n<h3>The Narrative Chain Model</h3>\n<p>The authors define two key terminology: <strong>narrative chains</strong> and <strong>narrative events</strong>. Narrative Events are defined as tuples of an event and its participants, represented as typed dependencies. This paper only considers single actors as <strong>protagonists</strong> and as such narrative events are a tuple of the event and the typed dependency of the protagonist: <em>(event, dependency)</em>. Narrative Chains are therefore defined as a partially ordered set of narrative events that share a common protagonist/actor. Formally this is defined as <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>e</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{e_1, e_2, ..., e_n \\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">}</span></span></span></span> where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> is the length of the chain and relationship <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">B(e_i, e_j)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> is true if and only if event <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> occurs strictly before event <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>.</p>\n<h3>Learning Narrative Relations</h3>\n<p>Given a list of observed verb/dependency frequencies, we can compute the pointwise mutual information between these occurances as:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mi>M</mi><mi>I</mi><mo stretchy=\"false\">[</mo><mi>e</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo separator=\"true\">,</mo><mi>d</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>e</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo separator=\"true\">,</mo><mi>g</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>=</mo><mi mathvariant=\"normal\">log</mi><mo>⁡</mo><mfrac><mrow><mi>P</mi><mo stretchy=\"false\">[</mo><mi>e</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo separator=\"true\">,</mo><mi>d</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>e</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo separator=\"true\">,</mo><mi>g</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><mrow><mi>P</mi><mo stretchy=\"false\">[</mo><mi>e</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo separator=\"true\">,</mo><mi>d</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>⋅</mo><mi>P</mi><mo stretchy=\"false\">[</mo><mi>e</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo separator=\"true\">,</mo><mi>g</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">PMI[e(w, d), e(v, g)] = \\operatorname{log} \\frac{P[e(w, d), e(v, g)]}{P[e(w, d)] \\cdot P[e(v, g)]}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">[</span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">l</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\" style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">[</span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">[</span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">[</span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>e</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo separator=\"true\">,</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">e(w, d)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">e</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mclose\">)</span></span></span></span> is the verb/dependency pair between <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">d</span></span></span></span>.</p>\n<h3>Evaluation</h3>\n<p>Evaluation is performed using the <em>Narrative Cloze</em> Evaluation Task for narrative coherence. A narrative chain is provided to the task and an event is removed in order for the model to perform a prediction to be evaluated on. The aim of the task is to perform a fill-in-the-blanks task, which upon successful completion indicates the presence of coherent narrative knowledge by the model. Given of tuple list of <code class=\"language-text\">(chain, event)</code> where <code class=\"language-text\">chain</code> is missing the true prediction <code class=\"language-text\">event</code>, the evaluation module returns the average model position. The model position is defined as the true event's position in the model's ranked candidate outputs (lower is better).</p>\n<h2>My Implementation</h2>\n<p>My implementation of (Chambers and Jurafsky, 2008) uses updated libaries, classes and functions. Written in Python, using the Stanford CoreNLP library (updated dependency parsing from transition model to neural-based Universal Dependencies) as well as the SpaCy pipeline for neural network models (with extensions from HuggingFace). I also extended the project to use NLP's secret sauce: word embeddings. An interpolated model between pointwise mutual information and cosine similarity shows strong results with low amounts of training data.</p>\n<p>The following libraries are used throughout the study:</p>\n<ol>\n<li>Stanford CoreNLP Python Implementation (<code class=\"language-text\">stanfordnlp</code>)</li>\n<li>SpaCy Dependency Parser (<code class=\"language-text\">spacy</code>)</li>\n<li>HuggingFace Neural Coreference Resolution (<code class=\"language-text\">neuralcoref</code>)</li>\n</ol>\n<p>Extensions include:</p>\n<ol>\n<li>Magnitude Embedding Library (<code class=\"language-text\">pymagnitude</code>)</li>\n<li>Word2Vec Google News Skip-Gram Model</li>\n</ol>\n<h3>Examples</h3>\n<p>Examples of identified narrative events in the format <code class=\"language-text\">(subject, verb, dependency, dependency_type, probability)</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">you kiss girl dobj 0.00023724792408066428\nthat enables users dobj 0.00023724792408066428\nGod bestows benefaction dobj 0.00023724792408066428\nAstronomers observed planets dobj 0.00023724792408066428</code></pre></div>\n<p>Examples of generated narrative chains (using a Greedy Decoding strategy):</p>\n<p><em>(Embedding-Similary Based)</em></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">seed event: play I dsubj -&gt; I play\nscore nsubj -&gt; I score\nwin nsubj -&gt; I win\nbeat nubj -&gt; I beat</code></pre></div>\n<p><em>(Pointwise Mutual Information Approximation Based)</em></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">seed event:  go I nsubj -&gt; I go\nget nsubj -&gt; I get\ndo nsubj -&gt; I do\nwant nsubj -&gt; I want</code></pre></div>\n<h3>Implementation Notes</h3>\n<ol>\n<li>verb space too large -> lemmatizing verbs before parsing</li>\n<li>events are similar to themselves -> removing seen verbs in chain from prediction candidates</li>\n<li>coreference resolution fails occasionally -> increase chunk size</li>\n<li>parsing is slow -> single grammatical pass and resolve entities ad-hoc</li>\n<li>coreference count computation is slow -> refactor to matrix implementation</li>\n</ol>\n<h2>Conclusion</h2>\n<p>This was a really interesting approach to modelling narrative semantics. I'm currently taking an <a href=\"interactive-fiction-class.org/\">advanced seminar course</a> in text generation and interactive fiction, and I hope to draw inspiration from this project to state of the art models/games such as GPT-2 and AI Dungeon 2!</p>","frontmatter":{"title":"Independent Study on Narrative Event Chains","date":"2020-01-10","tags":["machine learning","research"]}}},"pageContext":{"pathSlug":"/blog/narrative"}}}