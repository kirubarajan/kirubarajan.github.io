{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/embeddings","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# A Gentle Introduction to Word Embeddings\n> Fun with Natural  Language Processing’s “Secret Sauce“\n\nComputers don’t understand the nuances of language. It's because they only understand numbers and, as you can imagine, it’s impossible for us to enumerate every single nuance in understanding human language (let alone as numbers). But, we’ve seen a lot of progress in recent years of computers understanding language. So how do these work?  More specifically, how are these system representing words as *numbers*? \n\nIn this post, we’re going to talk about one of the coolest advances in machine learning and natural language processing: *word embeddings*. I hope that \nthis post serves as a launch pad to get started with embeddings, including code from how the models are trained to using pre-trained embeddings. In addition, we'll be looking at interesting use cases of word embeddings and recent research in the area of representation.\n\n## Representation\nSo how do we represent one of the most basic units of natural language, **words**? If this is the first time you’ve thought about this, you might be tempted to say the appropriate data structure is obviously a string! However, this comes with some design considerations:\n\n1. Your compiler doesn’t understand the contents of strings - it just recognizes the ASCII values that correspond to each symbol. This representation has room for improvement. Recognizing the word `cat` as being `[3, 1, 20]` is not very useful for capturing the *meaning of words*.\n2. Strings can have a variable length. If we wanted to give a machine learning model a string, it might truncate (or artificially elongate) values for consistency’s sake and we would lose (or create useless) information.\n\nFor a long time, one of best approaches we could do was representing a word as an vector (or array) with all 0s that had 1 in the index corresponding to a specific word. This is called **one-hot encoding**. For example, in the vocabulary `{\"I\", \"am\", \"a\", \"cat\"}`, the word `I` would correspond to `[1, 0, 0, 0]`, the word `am` as `[0, 1, 0, 0]`, and so forth. We’ll see that word representation has shifted recent from one-hot encoding to something *better*.\n\n### Better Representation\nSo what can we do about our representation problem? Surprise, surprise: we can turn to linguistics for the answer. Distributional semantics, in particular, holds the key:\n\n> \"You shall know a word by the company it keeps.\" - Firth (1957)\n\nIn essence, it makes sense to be defining words in relation to other words. Intuitively, this is easy to see; if I asked you to describe the word “avocado”, you would probably define it in terms of other words like “fruit” and “green” because the meanings of the word are *close*. This idea of word “closeness” gives rise to the notion of placing our words in some kind of space where we can measure their distance.\n\nTo be specific, this means these words exist in some *vector spaces* such as $\\mathbb{R}^2$. This means every word can be thought as a vector, like with one-hot encoding! This may seem scary, but this is just because we want to mathematically determine the “closeness” between two words. We could define a one dimensional scalar value for each word (e.g. `{\"cat\": 1}`), but the nuances of language extend far beyond a single dimension. That’s why we place them in higher order dimensions - typically in the tens or hundreds. Our brains can’t really imagine dimensions higher than three, so for the sake of visualization we’ll work in two dimensions for now: the classic $x-y$ plane.\n\nWe’ll discuss how to obtain word vectors (hereby referred to as **word embeddings**) in just a little bit. For now, let’s imagine that in our vocabulary, we have the word `“cat”`. Furthermore, let’s imagine that we also have access to a word embedding fairy that gives us the *perfect* vector for each word. \n\nAs such, she gives us the vector `[1, 4]` to represent the word **cat**.\n\n## Word Embeddings\nWhy do we want to represent a three-letter word as a vector with potentially a vector of hundreds of values? In short, we want to create the embeddings such that the vectors **capture the meaning of a given word**. This can intuitively be visualized as the vectors for similar words being group together. For example, if the vector for `\"cat\"` is `[1, 4]`, the vector for `\"kitten\"` would be something like `[2, 4]` whereas the vector for `\"dog\"` would be close by, for example `[1, 5]`. \n\nSince words are now vectors, we are also able to perform linear algebra operations on the given language. Although it may feel weird to subtract `dog` from `cat`, it turns out performing such operations tends to be useful for a variety of tasks. For example, the cosine distance (which encodes similarity) between two vectors is a powerful function that is easily applied to tasks involving natural language. For word vectors $u$ and $v$, we can define cosine similarity as:\n\n$$ \n\\operatorname{cos}(u, v) = \\frac{u \\cdot v}{|u| \\cdot |v|} = \\frac{\\sum_{i = 1}^{n} u_i v_i}{\\sqrt{\\sum_{i = 1}^{n} u^2_i} \\cdot \\sqrt{\\sum_{i = 1}^{n} v^2_i}} \n$$\n\nAs a result, something interesting we can do is train our word embeddings to create **analogies**. For example, a classic example in the field is using word embeddings to see that  `\"king\" - \"man\" = \"queen\" - \"woman\"`.  We can even generalize this to fill-in-the-blanks for sentences like “Bill Gates is to Microsoft as `____` is to Apple” by predicting `\"Steve Jobs\"`. This prediction is relatively straightforward when you have good embeddings and can be computed as:\n\n$$\nd = \\operatorname*{arg\\, max}_{v \\in V} ~ \\operatorname{cos} (v, ~ a - b + c)\n$$\n\nwhere $s$ is our cosine similarity function from earlier. In the above example, we have that `a = \"Bill Gates\"`, `b = \"Microsoft\"`, `c = \"Apple\"`. Finally, this gives us `d = \"Steve Jobs\"`. \n\n\n### Why Are Word Embeddings \"Secret Sauce\"?\nAnalogy generation was a fun result that was discovered by researchers, but this isn't where the true potential of word emebddings lie. Although these ad-hoc analyses are interesting to think about, the real use of word embeddings is to serve as a semantically-aware representation of words for **other downstream tasks**. For example, providing word embeddings to a neural network that powers a chatbot will let it generate sentences that make more sense than if we represented words using a string-to-index mapping. \n\nThe use of pre-trained word embeddings galvanized progress in natural language processing research since representation is often at the root of most machine learning problems. It's hard to think of mathematical grounding for this kind of phenomenon, but intuitively it's clear that better representation of language means that our neural networks can better understand semantics and therefore model language.\n\nIt seems like perfect word embeddings will end up being too specific  and are therefore good to be exist. These concerns are true thanks to the curse of dimensionality, but we can actually still approximate very powerful word embeddings that capture semantic meaning by training neural networks using stochastic gradient descent.\n\nBut, why neural networks? One previous method of word embedding generation was to perform dimensionality reduction on word co-occurrence matrices (which doesn’t involve deep learning). This procedure captures the intuition behind distributional semantics, but doesn’t have the powerful non-linearity capabilities of neural networks. However, it’s still useful to think about since certain methods of generation word embeddings draw upon this as reference.\n\nBut to be clear, I would actually like you to forget about pre-neural network methods, for now. It turns out that NLP can be implemented “from scratch“, i.e. purely through statistical and neural means. (You can read more about this from [Collobert et al.](https://arxiv.org/pdf/1103.0398v1.pdf)).\n\n### How Do We Create Word Embeddings?\n\nSo you now know we can generate word embeddings using a neural network. But exactly **how** do we do that? A commonly used implementation to generate word embeddings is called `word2vec`, which is what we will use as reference in this guide. This model was conceptualized at Google around 5 years ago and has gone on to push the state of the art in natural language processing.\n\nThe `word2vec` model generates word embeddings through one of two related models. Both models are be trained using different objectives and as such, we can build two simple neural networks that performs the following tasks:\n\n1. **Continuous Bag Of Word**: predicts a given missing word in a sentence/phrase based on context (faster but less specific)\n2. **Skip Gram**: given a word, predicts the words that will co-appear near it (slower but works better for infrequent words)\n\n\nIf you notice, they are in essence the inverse of the other. This is good for our intuition of how `word2vec` works to generate word embeddings as both are really good examples of the *distributional hypothesis* from earlier!\n\nA simple implementation of the above objectives would be logistic regression, which is nothing more than a fancy perceptron. You might be wondering: how do we get the word vectors from this process? Turns out what we're actually doing is making the network perform a **fake** that we training the network off of - we actually won’t use the model that’s trained. Instead, the goodies are encoded in the parameters of the neural network layers: the weights and biases of each neuron. The network’s internal representation of different words encodes the embeddings that we are looking for.\n\n## Recent Applications\n\nWord embeddings are an idea from as early as 2003. In the nearly two decades, there has been a huge surge in its use across the machine learning landscape. From powering virtually every NLP system at Google to inspiring a slew of new models, here are some recent developments regarding word embeddings:\n\n### Recommendation \nSpotify, Airbnb, etc.\n\n### Contextualization\nBERT, etc.\n\n## Implementation in Python\n\n### Pre-Trained Word Embeddings\nLet’s use **pre-trained word embeddings** from Google (trained by reading through Google News). Using trusted pre-trained models will allow us to quickly play with word vectors as well as prototype with deep learning faster since such models already been worked well in practice.\n\nWe first want to run `pip install pymagnitude` to install the embedding format. Then we can download the pre-trained word2vec embeddings using some `wget` magic:\n\n```bash\nwget http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude\n``` \n\nFinally, we can import the package and start writing queries:\n\n```python\nfrom pymagnitude import Magnitude\nvectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n\nprint(vectors.distance(\"cat\", \"dog\"))\n```\n\nThere's a lot of great documentation for how you can query the vectories and gain interesting insights available at the GitHub repository for Magnitude [here](https://github.com/plasticityai/magnitude).\n\n\n### Training Word Embeddings\nTraining word embeddings with a given dataset is easy using `gensim`, a Python package that abstracts the implementation of the `word2vec` neural network. This is one of the most commonly used Python package for generating word embeddings. \n\n```python\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(sentences)\n\nprint(model)\n```\n\n### Extra: Visualizing Word Embeddings with t-SNE\nIt would be cool to visualize the word vectors. Sadly, we humans are mostly incapable of visualizing in the 300th dimension.\n\nInstead, we can use a process called **dimensionality reduction** which will allow us to turn our 300 dimensions into regular 2D vectors that we can visualize. We will be using an algorithm called t-SNE (t-Distributed Stochastic Neighbouring Entities) to perform our dimensionality reduction from 300 dimensions to 2 dimensions.\n\nYou might be wondering how we can find a correspondance of vectors in $\\mathbb{R}^{300}$ to vectors in $\\mathbb{R}^2$. Why don't we just work with these 2D vectors in the first place? The truth is, these new embeddings (in 2D) actually do lose information. This seems obvious since we are going from a high dimensional space to a lower dimensional space. But more concretely, if we imagine an embedding space as being a distribution of points, we can measure the Kullback-Leiber Divergence of the original vectors and the transformed vectors:\n\n$$\nKL(p || q) = \\sum_x {~ p(x) \\cdot \\operatorname{log} ~ \\frac{p(x)}{q(x)}}\n$$\n\nThis is a measure of how \"different\" two probability distributions are. As a result, minimizing the KL Divergence between the two distributions using gradient descent \"learns\" us new a representation of the original embeddings such that they preserve information. This is a very powerful result! It helps us build intuition on high-dimensional embeddings for otherwise blackbox systems.\n\nThis is a multi-core implementation of t-SNE available [here](https://github.com/DmitryUlyanov/Multicore-TSNE). In my usage, Sci-Kit Learn's implementation of the algorithm is much slower, especially when running on more powerful machines (e.g. Google Colab).\n\n```python\nfrom MulticoreTSNE import MulticoreTSNE as TSNE\nfrom matplotlib import pyplot as plt\n\nembeddings = TSNE(n_jobs=4).fit_transform(vectors)\n\nvis_x = embeddings[:, 0]\nvis_y = embeddings[:, 1]\n\nplt.scatter(vis_x, vis_y, c=digits.target, cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.clim(-0.5, 9.5)\n\nplt.show()\n```\n\n### Extra: Gensim Compatibility with Gensim\nA lot of natural language processing might use the `gensim` package, which has a different API as the faster `pymagnitude` package we’ve been using. In order to interface with the `pymagnitude` model, we can write a wrapper class to use the same API as the `gensim` model:\n\n```python\nclass Word2Vec:\n\tdef __init__(self, vectors):\n\t\tself.vectors = vectors\n\t\tself.layer1_size = self.vectors.dim\n\n\tdef __getitem__(self, word):\n\t\treturn self.vectors.query(word)\n\t\n\tdef __contains__(self, word):\n\t\treturn word in self.vectors\n\t\n\tdef dim(self):\n\t\treturn self.vectors.dim \n```\n\nUsing this, we can wrap our magnitude model as follows:\n\n```python\nvectors = Magnitude('vectors.magnitude')\nw2v = Word2Vec(vectors)\n```\n\nAnd we can access the vector exactly the same as we would with `gensim` as follows:\n\n```python\ncat_vector = w2v['cat']\n```\n\nThis should resolve a lot of compatibility issues if you choose to leverage faster Magnitude embeddings with an existing Gensim codebase. \n\n## Epilogue: why am I writing about this?\nI read about word embeddings sometime during my freshman year of university. I’m not really sure where I learned about it, but I found the idea really enchanting.\n\nWord embeddings are a good introduction to neural networks as well as computational linguistics, and it’s what eventually introduced me to academia - which has made me a better developer and computer scientist. I decided to pay homage by writing this tutorial. I know lots of good resources about word embeddings exist already, but I wanted to help excite others to the wonders of NLP!","html":"<h1>A Gentle Introduction to Word Embeddings</h1>\n<blockquote>\n<p>Fun with Natural  Language Processing’s “Secret Sauce“</p>\n</blockquote>\n<p>Computers don’t understand the nuances of language. It's because they only understand numbers and, as you can imagine, it’s impossible for us to enumerate every single nuance in understanding human language (let alone as numbers). But, we’ve seen a lot of progress in recent years of computers understanding language. So how do these work?  More specifically, how are these system representing words as <em>numbers</em>? </p>\n<p>In this post, we’re going to talk about one of the coolest advances in machine learning and natural language processing: <em>word embeddings</em>. I hope that\nthis post serves as a launch pad to get started with embeddings, including code from how the models are trained to using pre-trained embeddings. In addition, we'll be looking at interesting use cases of word embeddings and recent research in the area of representation.</p>\n<h2>Representation</h2>\n<p>So how do we represent one of the most basic units of natural language, <strong>words</strong>? If this is the first time you’ve thought about this, you might be tempted to say the appropriate data structure is obviously a string! However, this comes with some design considerations:</p>\n<ol>\n<li>Your compiler doesn’t understand the contents of strings - it just recognizes the ASCII values that correspond to each symbol. This representation has room for improvement. Recognizing the word <code class=\"language-text\">cat</code> as being <code class=\"language-text\">[3, 1, 20]</code> is not very useful for capturing the <em>meaning of words</em>.</li>\n<li>Strings can have a variable length. If we wanted to give a machine learning model a string, it might truncate (or artificially elongate) values for consistency’s sake and we would lose (or create useless) information.</li>\n</ol>\n<p>For a long time, one of best approaches we could do was representing a word as an vector (or array) with all 0s that had 1 in the index corresponding to a specific word. This is called <strong>one-hot encoding</strong>. For example, in the vocabulary <code class=\"language-text\">{&quot;I&quot;, &quot;am&quot;, &quot;a&quot;, &quot;cat&quot;}</code>, the word <code class=\"language-text\">I</code> would correspond to <code class=\"language-text\">[1, 0, 0, 0]</code>, the word <code class=\"language-text\">am</code> as <code class=\"language-text\">[0, 1, 0, 0]</code>, and so forth. We’ll see that word representation has shifted recent from one-hot encoding to something <em>better</em>.</p>\n<h3>Better Representation</h3>\n<p>So what can we do about our representation problem? Surprise, surprise: we can turn to linguistics for the answer. Distributional semantics, in particular, holds the key:</p>\n<blockquote>\n<p>\"You shall know a word by the company it keeps.\" - Firth (1957)</p>\n</blockquote>\n<p>In essence, it makes sense to be defining words in relation to other words. Intuitively, this is easy to see; if I asked you to describe the word “avocado”, you would probably define it in terms of other words like “fruit” and “green” because the meanings of the word are <em>close</em>. This idea of word “closeness” gives rise to the notion of placing our words in some kind of space where we can measure their distance.</p>\n<p>To be specific, this means these words exist in some <em>vector spaces</em> such as <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"double-struck\">R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbb{R}^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>. This means every word can be thought as a vector, like with one-hot encoding! This may seem scary, but this is just because we want to mathematically determine the “closeness” between two words. We could define a one dimensional scalar value for each word (e.g. <code class=\"language-text\">{&quot;cat&quot;: 1}</code>), but the nuances of language extend far beyond a single dimension. That’s why we place them in higher order dimensions - typically in the tens or hundreds. Our brains can’t really imagine dimensions higher than three, so for the sake of visualization we’ll work in two dimensions for now: the classic <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo>−</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">x-y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> plane.</p>\n<p>We’ll discuss how to obtain word vectors (hereby referred to as <strong>word embeddings</strong>) in just a little bit. For now, let’s imagine that in our vocabulary, we have the word <code class=\"language-text\">“cat”</code>. Furthermore, let’s imagine that we also have access to a word embedding fairy that gives us the <em>perfect</em> vector for each word. </p>\n<p>As such, she gives us the vector <code class=\"language-text\">[1, 4]</code> to represent the word <strong>cat</strong>.</p>\n<h2>Word Embeddings</h2>\n<p>Why do we want to represent a three-letter word as a vector with potentially a vector of hundreds of values? In short, we want to create the embeddings such that the vectors <strong>capture the meaning of a given word</strong>. This can intuitively be visualized as the vectors for similar words being group together. For example, if the vector for <code class=\"language-text\">&quot;cat&quot;</code> is <code class=\"language-text\">[1, 4]</code>, the vector for <code class=\"language-text\">&quot;kitten&quot;</code> would be something like <code class=\"language-text\">[2, 4]</code> whereas the vector for <code class=\"language-text\">&quot;dog&quot;</code> would be close by, for example <code class=\"language-text\">[1, 5]</code>. </p>\n<p>Since words are now vectors, we are also able to perform linear algebra operations on the given language. Although it may feel weird to subtract <code class=\"language-text\">dog</code> from <code class=\"language-text\">cat</code>, it turns out performing such operations tends to be useful for a variety of tasks. For example, the cosine distance (which encodes similarity) between two vectors is a powerful function that is easily applied to tasks involving natural language. For word vectors <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">u</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span></span></span></span>, we can define cosine similarity as:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">cos</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>u</mi><mo separator=\"true\">,</mo><mi>v</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mi mathvariant=\"normal\">∣</mi><mi>u</mi><mi mathvariant=\"normal\">∣</mi><mo>⋅</mo><mi mathvariant=\"normal\">∣</mi><mi>v</mi><mi mathvariant=\"normal\">∣</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><mrow><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><mo>⋅</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>v</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\operatorname{cos}(u, v) = \\frac{u \\cdot v}{|u| \\cdot |v|} = \\frac{\\sum_{i = 1}^{n} u_i v_i}{\\sqrt{\\sum_{i = 1}^{n} u^2_i} \\cdot \\sqrt{\\sum_{i = 1}^{n} v^2_i}} </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">c</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">s</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">u</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0574500000000002em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.12145em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"mord mathdefault\">u</span><span class=\"mord\">∣</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord\">∣</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.624002em;vertical-align:-1.13em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.494002em;\"><span style=\"top:-2.1727090000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.937291em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7959080000000001em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.0448000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.897291em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.302709em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.937291em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7959080000000001em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.0448000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.897291em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.302709em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.6897100000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.13em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>As a result, something interesting we can do is train our word embeddings to create <strong>analogies</strong>. For example, a classic example in the field is using word embeddings to see that  <code class=\"language-text\">&quot;king&quot; - &quot;man&quot; = &quot;queen&quot; - &quot;woman&quot;</code>.  We can even generalize this to fill-in-the-blanks for sentences like “Bill Gates is to Microsoft as <code class=\"language-text\">____</code> is to Apple” by predicting <code class=\"language-text\">&quot;Steve Jobs&quot;</code>. This prediction is relatively straightforward when you have good embeddings and can be computed as:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mo>=</mo><munder><mo><mi mathvariant=\"normal\">arg max</mi><mo>⁡</mo></mo><mrow><mi>v</mi><mo>∈</mo><mi>V</mi></mrow></munder><mtext> </mtext><mi mathvariant=\"normal\">cos</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>v</mi><mo separator=\"true\">,</mo><mtext> </mtext><mi>a</mi><mo>−</mo><mi>b</mi><mo>+</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">d = \\operatorname*{arg\\, max}_{v \\in V} ~ \\operatorname{cos} (v, ~ a - b + c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.716141em;vertical-align:-0.966141em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.43055999999999994em;\"><span style=\"top:-2.161229em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\" style=\"margin-right:0.01389em;\">g</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">x</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.966141em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace nobreak\"> </span><span class=\"mop\"><span class=\"mord mathrm\">c</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">s</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">a</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span> is our cosine similarity function from earlier. In the above example, we have that <code class=\"language-text\">a = &quot;Bill Gates&quot;</code>, <code class=\"language-text\">b = &quot;Microsoft&quot;</code>, <code class=\"language-text\">c = &quot;Apple&quot;</code>. Finally, this gives us <code class=\"language-text\">d = &quot;Steve Jobs&quot;</code>. </p>\n<h3>Why Are Word Embeddings \"Secret Sauce\"?</h3>\n<p>Analogy generation was a fun result that was discovered by researchers, but this isn't where the true potential of word emebddings lie. Although these ad-hoc analyses are interesting to think about, the real use of word embeddings is to serve as a semantically-aware representation of words for <strong>other downstream tasks</strong>. For example, providing word embeddings to a neural network that powers a chatbot will let it generate sentences that make more sense than if we represented words using a string-to-index mapping. </p>\n<p>The use of pre-trained word embeddings galvanized progress in natural language processing research since representation is often at the root of most machine learning problems. It's hard to think of mathematical grounding for this kind of phenomenon, but intuitively it's clear that better representation of language means that our neural networks can better understand semantics and therefore model language.</p>\n<p>It seems like perfect word embeddings will end up being too specific  and are therefore good to be exist. These concerns are true thanks to the curse of dimensionality, but we can actually still approximate very powerful word embeddings that capture semantic meaning by training neural networks using stochastic gradient descent.</p>\n<p>But, why neural networks? One previous method of word embedding generation was to perform dimensionality reduction on word co-occurrence matrices (which doesn’t involve deep learning). This procedure captures the intuition behind distributional semantics, but doesn’t have the powerful non-linearity capabilities of neural networks. However, it’s still useful to think about since certain methods of generation word embeddings draw upon this as reference.</p>\n<p>But to be clear, I would actually like you to forget about pre-neural network methods, for now. It turns out that NLP can be implemented “from scratch“, i.e. purely through statistical and neural means. (You can read more about this from <a href=\"https://arxiv.org/pdf/1103.0398v1.pdf\">Collobert et al.</a>).</p>\n<h3>How Do We Create Word Embeddings?</h3>\n<p>So you now know we can generate word embeddings using a neural network. But exactly <strong>how</strong> do we do that? A commonly used implementation to generate word embeddings is called <code class=\"language-text\">word2vec</code>, which is what we will use as reference in this guide. This model was conceptualized at Google around 5 years ago and has gone on to push the state of the art in natural language processing.</p>\n<p>The <code class=\"language-text\">word2vec</code> model generates word embeddings through one of two related models. Both models are be trained using different objectives and as such, we can build two simple neural networks that performs the following tasks:</p>\n<ol>\n<li><strong>Continuous Bag Of Word</strong>: predicts a given missing word in a sentence/phrase based on context (faster but less specific)</li>\n<li><strong>Skip Gram</strong>: given a word, predicts the words that will co-appear near it (slower but works better for infrequent words)</li>\n</ol>\n<p>If you notice, they are in essence the inverse of the other. This is good for our intuition of how <code class=\"language-text\">word2vec</code> works to generate word embeddings as both are really good examples of the <em>distributional hypothesis</em> from earlier!</p>\n<p>A simple implementation of the above objectives would be logistic regression, which is nothing more than a fancy perceptron. You might be wondering: how do we get the word vectors from this process? Turns out what we're actually doing is making the network perform a <strong>fake</strong> that we training the network off of - we actually won’t use the model that’s trained. Instead, the goodies are encoded in the parameters of the neural network layers: the weights and biases of each neuron. The network’s internal representation of different words encodes the embeddings that we are looking for.</p>\n<h2>Recent Applications</h2>\n<p>Word embeddings are an idea from as early as 2003. In the nearly two decades, there has been a huge surge in its use across the machine learning landscape. From powering virtually every NLP system at Google to inspiring a slew of new models, here are some recent developments regarding word embeddings:</p>\n<h3>Recommendation</h3>\n<p>Spotify, Airbnb, etc.</p>\n<h3>Contextualization</h3>\n<p>BERT, etc.</p>\n<h2>Implementation in Python</h2>\n<h3>Pre-Trained Word Embeddings</h3>\n<p>Let’s use <strong>pre-trained word embeddings</strong> from Google (trained by reading through Google News). Using trusted pre-trained models will allow us to quickly play with word vectors as well as prototype with deep learning faster since such models already been worked well in practice.</p>\n<p>We first want to run <code class=\"language-text\">pip install pymagnitude</code> to install the embedding format. Then we can download the pre-trained word2vec embeddings using some <code class=\"language-text\">wget</code> magic:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">wget</span> http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude</code></pre></div>\n<p>Finally, we can import the package and start writing queries:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> pymagnitude <span class=\"token keyword\">import</span> Magnitude\nvectors <span class=\"token operator\">=</span> Magnitude<span class=\"token punctuation\">(</span><span class=\"token string\">\"GoogleNews-vectors-negative300.magnitude\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>vectors<span class=\"token punctuation\">.</span>distance<span class=\"token punctuation\">(</span><span class=\"token string\">\"cat\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"dog\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>There's a lot of great documentation for how you can query the vectories and gain interesting insights available at the GitHub repository for Magnitude <a href=\"https://github.com/plasticityai/magnitude\">here</a>.</p>\n<h3>Training Word Embeddings</h3>\n<p>Training word embeddings with a given dataset is easy using <code class=\"language-text\">gensim</code>, a Python package that abstracts the implementation of the <code class=\"language-text\">word2vec</code> neural network. This is one of the most commonly used Python package for generating word embeddings. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Word2Vec\nmodel <span class=\"token operator\">=</span> Word2Vec<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Extra: Visualizing Word Embeddings with t-SNE</h3>\n<p>It would be cool to visualize the word vectors. Sadly, we humans are mostly incapable of visualizing in the 300th dimension.</p>\n<p>Instead, we can use a process called <strong>dimensionality reduction</strong> which will allow us to turn our 300 dimensions into regular 2D vectors that we can visualize. We will be using an algorithm called t-SNE (t-Distributed Stochastic Neighbouring Entities) to perform our dimensionality reduction from 300 dimensions to 2 dimensions.</p>\n<p>You might be wondering how we can find a correspondance of vectors in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"double-struck\">R</mi><mn>300</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbb{R}^{300}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">3</span><span class=\"mord mtight\">0</span><span class=\"mord mtight\">0</span></span></span></span></span></span></span></span></span></span></span></span> to vectors in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"double-struck\">R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbb{R}^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>. Why don't we just work with these 2D vectors in the first place? The truth is, these new embeddings (in 2D) actually do lose information. This seems obvious since we are going from a high dimensional space to a lower dimensional space. But more concretely, if we imagine an embedding space as being a distribution of points, we can measure the Kullback-Leiber Divergence of the original vectors and the transformed vectors:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>q</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mo>∑</mo><mi>x</mi></munder><mrow><mtext> </mtext><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi mathvariant=\"normal\">log</mi><mo>⁡</mo><mtext> </mtext><mfrac><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>q</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow></mrow><annotation encoding=\"application/x-tex\">KL(p || q) = \\sum_x {~ p(x) \\cdot \\operatorname{log} ~ \\frac{p(x)}{q(x)}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathdefault\">L</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">q</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.6770050000000003em;vertical-align:-1.250005em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.050005em;\"><span style=\"top:-1.8999949999999999em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">x</span></span></span><span style=\"top:-3.0500049999999996em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.250005em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">l</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\" style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">q</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></span>\n<p>This is a measure of how \"different\" two probability distributions are. As a result, minimizing the KL Divergence between the two distributions using gradient descent \"learns\" us new a representation of the original embeddings such that they preserve information. This is a very powerful result! It helps us build intuition on high-dimensional embeddings for otherwise blackbox systems.</p>\n<p>This is a multi-core implementation of t-SNE available <a href=\"https://github.com/DmitryUlyanov/Multicore-TSNE\">here</a>. In my usage, Sci-Kit Learn's implementation of the algorithm is much slower, especially when running on more powerful machines (e.g. Google Colab).</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> MulticoreTSNE <span class=\"token keyword\">import</span> MulticoreTSNE <span class=\"token keyword\">as</span> TSNE\n<span class=\"token keyword\">from</span> matplotlib <span class=\"token keyword\">import</span> pyplot <span class=\"token keyword\">as</span> plt\n\nembeddings <span class=\"token operator\">=</span> TSNE<span class=\"token punctuation\">(</span>n_jobs<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>vectors<span class=\"token punctuation\">)</span>\n\nvis_x <span class=\"token operator\">=</span> embeddings<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\nvis_y <span class=\"token operator\">=</span> embeddings<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>vis_x<span class=\"token punctuation\">,</span> vis_y<span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span>digits<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> cmap<span class=\"token operator\">=</span>plt<span class=\"token punctuation\">.</span>cm<span class=\"token punctuation\">.</span>get_cmap<span class=\"token punctuation\">(</span><span class=\"token string\">\"jet\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>clim<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9.5</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Extra: Gensim Compatibility with Gensim</h3>\n<p>A lot of natural language processing might use the <code class=\"language-text\">gensim</code> package, which has a different API as the faster <code class=\"language-text\">pymagnitude</code> package we’ve been using. In order to interface with the <code class=\"language-text\">pymagnitude</code> model, we can write a wrapper class to use the same API as the <code class=\"language-text\">gensim</code> model:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Word2Vec</span><span class=\"token punctuation\">:</span>\n\t<span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> vectors<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t\tself<span class=\"token punctuation\">.</span>vectors <span class=\"token operator\">=</span> vectors\n\t\tself<span class=\"token punctuation\">.</span>layer1_size <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>vectors<span class=\"token punctuation\">.</span>dim\n\n\t<span class=\"token keyword\">def</span> <span class=\"token function\">__getitem__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t\t<span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>vectors<span class=\"token punctuation\">.</span>query<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span>\n\t\n\t<span class=\"token keyword\">def</span> <span class=\"token function\">__contains__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t\t<span class=\"token keyword\">return</span> word <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>vectors\n\t\n\t<span class=\"token keyword\">def</span> <span class=\"token function\">dim</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t\t<span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>vectors<span class=\"token punctuation\">.</span>dim </code></pre></div>\n<p>Using this, we can wrap our magnitude model as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">vectors <span class=\"token operator\">=</span> Magnitude<span class=\"token punctuation\">(</span><span class=\"token string\">'vectors.magnitude'</span><span class=\"token punctuation\">)</span>\nw2v <span class=\"token operator\">=</span> Word2Vec<span class=\"token punctuation\">(</span>vectors<span class=\"token punctuation\">)</span></code></pre></div>\n<p>And we can access the vector exactly the same as we would with <code class=\"language-text\">gensim</code> as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">cat_vector <span class=\"token operator\">=</span> w2v<span class=\"token punctuation\">[</span><span class=\"token string\">'cat'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>This should resolve a lot of compatibility issues if you choose to leverage faster Magnitude embeddings with an existing Gensim codebase. </p>\n<h2>Epilogue: why am I writing about this?</h2>\n<p>I read about word embeddings sometime during my freshman year of university. I’m not really sure where I learned about it, but I found the idea really enchanting.</p>\n<p>Word embeddings are a good introduction to neural networks as well as computational linguistics, and it’s what eventually introduced me to academia - which has made me a better developer and computer scientist. I decided to pay homage by writing this tutorial. I know lots of good resources about word embeddings exist already, but I wanted to help excite others to the wonders of NLP!</p>","frontmatter":{"title":"A Gentle Introduction to Word Embeddings","date":"2019-11-18","tags":["machine learning","natural language processing"]}}},"pageContext":{"pathSlug":"/blog/embeddings"}}}