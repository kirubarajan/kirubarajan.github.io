{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/study","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# Independent Study on *Modern* Deep Learning\n> Misc. Machine Learning Methodologies\n\nThis page serves as both research notes and a workspace for my independent study at the University of Pennsylvania, advised by [Pratik Chaudhari](https://pratikac.github.io/). This independent study is \nmeant to serve as an extension to the course [ESE 546: Principles of Deep Learning](https://pratikac.github.io/pub/19_ese546.pdf). The goal of the study is to learn more advanced paradigms of training deep learning models, as well as inference using them. This work was split into two sections: 1) a literature review of papers on various topics and 2) experiments with different types of models and algorithms, implemented in Python. Although the COVID-19 pandemic limited certain topics I would have like to explore (namely reinforcement learning), I was able to learn many new concepts and promising research avenues.\n\nDisclaimer: the \"Modern Deep Learning* name is meant to be a joke, since the focus on this independent study is on cool/interesting trends in deep learning, which is already a trendy area of research.\n\n## Literature Review\nBelow are my notes on some recent interesting papers that have expanded my views on standard machine learning paradigms. \n\n### ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\n[*Rajbhandari et al. 2020*](https://arxiv.org/pdf/1910.02054.pdf)\n\nThis paper introduces a novel optimizer named the Zero Redundency Optimizer (ZeRO), which aims to make it feasible and efficient to train previously impossible to train model architectures whose training exhibits memory limitations. This is done by partitioning model states as opposed to standard model state replication across clusters. Memory analysis shows that the optimizer can train a **one trillion** parameter model on 1024 GPUs with data parallelism degree $N_d = 1024$.\n\nThanks, Microsoft!\n\n### Exploring Randomly Wired Neural Networks for Image Recognition\n*Xie et al. (2019)*\n\nThis paper explores different neural network architectures by generating random neural network wirings. This is done by defining a stochastic network generator to encapsulate Neural Architecture Search, and later using classical random graph algorithms for wiring the networks. The authors show that the generated networks have competitve performance on the ImageNet task.\n\n**Network Generator**\n\nNetwork Generators define a family of possible wiring patterns. Network architectures can thereby be sampled according to a probability distribution, which is differentiably learnable.\n\nFormally, the generator is a mapping $g: \\Theta \\rightarrow N$ where $\\Theta$ is the parameter space and $N$ is the space of neural network architectures. As such, $g$ determines how the computational graph representing the neural network is wired. The given parameters $\\theta \\in \\Theta$ specifies meta-information about the network such as the number of layers, activation types, etc. The output of $g$ is symbolic, so it doesn't return the weights of the networks (which can be learned from standard differentiable training processes) but instead a representation of the network (e.g. flow of data and types of operations).\n\nThe network generator $g$ can be extended to include an additional argument $s$, which acts as a stochastic seed. Then, the generator $g(s, \\theta)$ can be repeatedly called to generate a pseudo-random family of architectures.\n\n**Graphs to Neural Networks**\n\nThe neural network generator generates a *general graph*, which is a set of nodes followed by a set of edges that connect the nodes. This general representation does not specify how the graph corresponds to a neural network, which is a later post-processing step. The non-restrictivness of the general graph allows the use of classical graph generation techniques from graph theory. In particular, the authors experiment with Erdos-Renyi (ER), Barabasi-Albert (BA), and Watts-Strogatz (WS) models of graph generation.\n\nThe generated edges are defined to be data flow (i.e. sending a tensor of data from one node to another) and that nodes define operations of either:\n1. **Aggregation** (e.g. weighted sum)\n2. **Transformation** (e.g. non-linearity)\n3. **Distribution** (e.g. copying data)\n\n**Experiments**\n\nFor each generator, the authors sample 5 instances (generated by 5 random seeds), and train them from scratch. Networks are trained for roughly 100 epochs, using a half-period-cosine learning rate decay from an initial learning rate of 0.1 with a momentum of 0.9.\n\nThe authors note that every random generator yields decent accuracy. Furthermore, the variation among the random network instances is rather low with a standard deviation in the range of 0.2% to 0.4%.\n\n\n### Compositional Attention Networks for Machine Reasoning\n[Hudson et. al, 2018](https://arxiv.org/abs/1803.03067)\n\nThe authors design a novel fully differentiable neural network architecture that is capable of explicit and expressive reasoning. One primary goal of the paper is interpretability, without sacrificing the predictive performance of black box methods. Problems are decomposed into attention-based steps, and are solved using Memory, Attention, and Composition (MAC) sub-units. On the CLEVR dataset for visual reasoning, the model accomplishes a state-of-the-art 98.9% accuracy, using less data than competing models.\n \n## Experiments\nBelow are some experiments I've run for playing around with state-of-the-art models. Code will be available on GitHub, after the end of my independent study.\n\nA topic that originally prompted me to begin this independent study is Multitask Learning due to the similarities with how humans acquire knowledge in the real world. One example of this is Open AI's \n[GPT-2 model](https://openai.com/blog/better-language-models/), whose paper was named [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). As part of this study, I have implemented numerous experiments ranging from using libraries to fine-tune large pre-trained language models, to implementing various sampling algorithms for natural\nlanguage generation in PyTorch.\n\n### Fine-Tuning Transformer Models (GPT-2)\nEasy usage can be done through the GPT-2 Simple package by Max Woolf (https://github.com/minimaxir/gpt-2-simple).\n\nInstall using `pip3 install gpt-2-simple` and provide text for fine-tuning:\n\n```python\nimport gpt_2_simple as gpt2\nimport os\nimport requests\n\n# download pre-trained GPT-2 model with 124M parameters\nmodel_name = \"124M\"\ngpt2.download_gpt2(model_name=model_name) \n\n# provide file for fine-tuning\nfile_name = \"shakespeare.txt\"\n    \n# start fine-tuning tensorflow session\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess, file_name, model_name=model_name, steps=1000)\n\n# generate text\ngpt2.generate(sess)\n```\n\nI trained a fine-tuned GPT-2 model on a [corpus of Barack Obama tweets I put together](https://raw.githubusercontent.com/kirubarajan/phraseviz/master/corpus.txt).\n\n```\nWe have a clear goal: Ending the use of force in Afghanistan as quickly \nas possible. That means giving Congress more time to figure out how to \nmake that happen. And doing so is the single most effective way forward.\n\nThe Afghan people deserve better. They and I are foot soldiers for them. \nWe're going to use all our might to get that goal accomplished. \nBut America is not going to give ourselves up for expedience's sake.\n```\n\nWow! Thanks, Obama for the big policy change!\n\n### Transfer Learning via Control Sequences\nAs per the CTRL language model paper by [Keskar et al.](https://arxiv.org/pdf/1909.05858.pdf), I also experimented with prepending a \"control sequence\" $c$ that would allow the fine-tuning process to control downstream generation. In particular, I used the Hugging Face Transformers library to fine-tune the pre-trained 355M GPT-2 with Venture Captial tweets (in typical fashion). As a control sequence, I set $c$ to be the user name of generation (e.g. `@paulg`) and was able to reduce perplexity by over 20%. This creates an implicit Transfer Learning effect within the fine-tuning process.\n\nSome example generations turned out being very promising:\n\n```\n\\paulg- No such things as moral judgment to deal in a way that matters; I wish this could just happen for the good of the community rather than being a thing that's only achieved in theory. The world of the future...we could do better: \n\n\\paulg- You can see a bit of a pattern here; the more you work on a project the more things you learn about it. The same is true in life as in business, and the less we do, and the faster we work with… \n\n\\paulg- \"A great start in an exciting field? Make sure it's a good one. If not, you could be running a startup in the dark.\" - @joshk- \n```\n\nMy favourite is `\\paulg- It takes a village to raise an army.`.\n\n### Generative Adversarial Networks\n\nFollowing our discussion on Generative Adversarial Networks in ESE 546, I've also been playing with Style Transfer GANs by exploring abstract artwork from contemporary artists. These adversarial paradigms prove to be very promising in generation tasks, and can be a lot less noisy than Variational Autoencoders. An interesting area of research is applying these adversarial paradigms to discrete domains, such as natural language. \n \n![image](https://i.imgur.com/gFAOIRM.png) \n<small> (this is based off of pieces from Marc Chagall) </small>\n\n![image](https://i.imgur.com/Rv7KRFC.png) <br />\n<small> (this is based off pieces from Jerret Lee) </small>\n\n### Generation Algorithms for Language\n\nReading the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf)\nprompted me to learn more about different sampling algorithms. In particular, these algorithms\nattempt to efficiently determine the most likely sequence of text given a probability distribution $P(w_t ~|~ w_{t - 1}, w_{t - 2}, ...)$. This distribution is often the output of a softmax layer on \na neural network. Although the state-of-the-art language models are tremendously large models with hundreds of millions of parameters, I was able to experiment with different sampling algorithms \nusing a small LSTM language model.\n\nThe model definition for my LSTM was rather simple:\n\n```python\nclass Model(torch.nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n        super(Model, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.output = torch.nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, data, hidden):\n        embedded = self.embedding(data)\n        prediction, hidden = self.rnn(embedded, hidden)\n        return self.output(prediction), hidden\n\n    def init_hidden(self, BATCH_SIZE):\n        return torch.zeros(1, BATCH_SIZE, self.hidden_dim), torch.zeros(1, BATCH_SIZE, self.hidden_dim)\n```\n\nThe first and most intuitive generation strategy is known as **Greedy Decoding**, where we take the **most probable word** over a vocabulary $V$ for a context $c$ as the next word.\n\n$$\nw_i = \\operatorname*{arg\\, max}_{w \\in V} ~ P(w_i ~ | ~ c_0 ~ ... ~ c_{i - 1})\n$$\n\nHowever, this produced rather trite and non-sensical generations because language often has\na pertinent information horizon larger than a single time-step. An improvement is known as **Top-$k$ Sampling**, which truncates the probability distribution\nto the $k$ most likely tokens in the vocabulary.\n\n*\"I am proposing with an advantage over commerce budget. — (applause) the middle of commerce, way together more of each other people’s it. In the chance that the international issue, freedom we have never has allowed the other way, or share from footing or denied coverage for the work of Democrats and Republican administrations isn’t (Applause.) Now, none of this can happen unless we’re their own rules that progress on so tied long still blind you should make Wall good example. (Applause.) For unemployment to pull all we should leave just like us — (applause)\"*.\n\nIntroducing non-deterministic generation seems to be the key to creative and engaging generations.\n\n## Conclusion\n\nIn conclusion, this independent study has shown me that even though there is a lot of hype and effort in deep learning, there is still a lot of research left. Although companies and organizations are \ndeveloping larger and more expensive models, many core concepts are still researchable using smaller and more managable models. Chasing the state-of-the-art results is often a challenging and sometimes unrewarding endeavour, whereas devising new paradigms of machine learning training (like paralellizing training over cloud instances) and interesting inference strategies (e.g. Top-$k$ sampling) can make comparable results while still asking promising research questions about machine intelligence. Overall, \nI am excited to continue research in machine learning, as well as different topics in linguistics and probabilty.","html":"<h1>Independent Study on <em>Modern</em> Deep Learning</h1>\n<blockquote>\n<p>Misc. Machine Learning Methodologies</p>\n</blockquote>\n<p>This page serves as both research notes and a workspace for my independent study at the University of Pennsylvania, advised by <a href=\"https://pratikac.github.io/\">Pratik Chaudhari</a>. This independent study is\nmeant to serve as an extension to the course <a href=\"https://pratikac.github.io/pub/19_ese546.pdf\">ESE 546: Principles of Deep Learning</a>. The goal of the study is to learn more advanced paradigms of training deep learning models, as well as inference using them. This work was split into two sections: 1) a literature review of papers on various topics and 2) experiments with different types of models and algorithms, implemented in Python. Although the COVID-19 pandemic limited certain topics I would have like to explore (namely reinforcement learning), I was able to learn many new concepts and promising research avenues.</p>\n<p>Disclaimer: the \"Modern Deep Learning* name is meant to be a joke, since the focus on this independent study is on cool/interesting trends in deep learning, which is already a trendy area of research.</p>\n<h2>Literature Review</h2>\n<p>Below are my notes on some recent interesting papers that have expanded my views on standard machine learning paradigms. </p>\n<h3>ZeRO: Memory Optimization Towards Training A Trillion Parameter Models</h3>\n<p><a href=\"https://arxiv.org/pdf/1910.02054.pdf\"><em>Rajbhandari et al. 2020</em></a></p>\n<p>This paper introduces a novel optimizer named the Zero Redundency Optimizer (ZeRO), which aims to make it feasible and efficient to train previously impossible to train model architectures whose training exhibits memory limitations. This is done by partitioning model states as opposed to standard model state replication across clusters. Memory analysis shows that the optimizer can train a <strong>one trillion</strong> parameter model on 1024 GPUs with data parallelism degree <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>N</mi><mi>d</mi></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_d = 1024</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">d</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">2</span><span class=\"mord\">4</span></span></span></span>.</p>\n<p>Thanks, Microsoft!</p>\n<h3>Exploring Randomly Wired Neural Networks for Image Recognition</h3>\n<p><em>Xie et al. (2019)</em></p>\n<p>This paper explores different neural network architectures by generating random neural network wirings. This is done by defining a stochastic network generator to encapsulate Neural Architecture Search, and later using classical random graph algorithms for wiring the networks. The authors show that the generated networks have competitve performance on the ImageNet task.</p>\n<p><strong>Network Generator</strong></p>\n<p>Network Generators define a family of possible wiring patterns. Network architectures can thereby be sampled according to a probability distribution, which is differentiably learnable.</p>\n<p>Formally, the generator is a mapping <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo>:</mo><mi mathvariant=\"normal\">Θ</mi><mo>→</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">g: \\Theta \\rightarrow N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Θ</mi></mrow><annotation encoding=\"application/x-tex\">\\Theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Θ</span></span></span></span> is the parameter space and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> is the space of neural network architectures. As such, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> determines how the computational graph representing the neural network is wired. The given parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>∈</mo><mi mathvariant=\"normal\">Θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta \\in \\Theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Θ</span></span></span></span> specifies meta-information about the network such as the number of layers, activation types, etc. The output of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> is symbolic, so it doesn't return the weights of the networks (which can be learned from standard differentiable training processes) but instead a representation of the network (e.g. flow of data and types of operations).</p>\n<p>The network generator <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> can be extended to include an additional argument <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span>, which acts as a stochastic seed. Then, the generator <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">g(s, \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> can be repeatedly called to generate a pseudo-random family of architectures.</p>\n<p><strong>Graphs to Neural Networks</strong></p>\n<p>The neural network generator generates a <em>general graph</em>, which is a set of nodes followed by a set of edges that connect the nodes. This general representation does not specify how the graph corresponds to a neural network, which is a later post-processing step. The non-restrictivness of the general graph allows the use of classical graph generation techniques from graph theory. In particular, the authors experiment with Erdos-Renyi (ER), Barabasi-Albert (BA), and Watts-Strogatz (WS) models of graph generation.</p>\n<p>The generated edges are defined to be data flow (i.e. sending a tensor of data from one node to another) and that nodes define operations of either:</p>\n<ol>\n<li><strong>Aggregation</strong> (e.g. weighted sum)</li>\n<li><strong>Transformation</strong> (e.g. non-linearity)</li>\n<li><strong>Distribution</strong> (e.g. copying data)</li>\n</ol>\n<p><strong>Experiments</strong></p>\n<p>For each generator, the authors sample 5 instances (generated by 5 random seeds), and train them from scratch. Networks are trained for roughly 100 epochs, using a half-period-cosine learning rate decay from an initial learning rate of 0.1 with a momentum of 0.9.</p>\n<p>The authors note that every random generator yields decent accuracy. Furthermore, the variation among the random network instances is rather low with a standard deviation in the range of 0.2% to 0.4%.</p>\n<h3>Compositional Attention Networks for Machine Reasoning</h3>\n<p><a href=\"https://arxiv.org/abs/1803.03067\">Hudson et. al, 2018</a></p>\n<p>The authors design a novel fully differentiable neural network architecture that is capable of explicit and expressive reasoning. One primary goal of the paper is interpretability, without sacrificing the predictive performance of black box methods. Problems are decomposed into attention-based steps, and are solved using Memory, Attention, and Composition (MAC) sub-units. On the CLEVR dataset for visual reasoning, the model accomplishes a state-of-the-art 98.9% accuracy, using less data than competing models.</p>\n<h2>Experiments</h2>\n<p>Below are some experiments I've run for playing around with state-of-the-art models. Code will be available on GitHub, after the end of my independent study.</p>\n<p>A topic that originally prompted me to begin this independent study is Multitask Learning due to the similarities with how humans acquire knowledge in the real world. One example of this is Open AI's\n<a href=\"https://openai.com/blog/better-language-models/\">GPT-2 model</a>, whose paper was named <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language Models are Unsupervised Multitask Learners</a>. As part of this study, I have implemented numerous experiments ranging from using libraries to fine-tune large pre-trained language models, to implementing various sampling algorithms for natural\nlanguage generation in PyTorch.</p>\n<h3>Fine-Tuning Transformer Models (GPT-2)</h3>\n<p>Easy usage can be done through the GPT-2 Simple package by Max Woolf (<a href=\"https://github.com/minimaxir/gpt-2-simple\">https://github.com/minimaxir/gpt-2-simple</a>).</p>\n<p>Install using <code class=\"language-text\">pip3 install gpt-2-simple</code> and provide text for fine-tuning:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> gpt_2_simple <span class=\"token keyword\">as</span> gpt2\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> requests\n\n<span class=\"token comment\"># download pre-trained GPT-2 model with 124M parameters</span>\nmodel_name <span class=\"token operator\">=</span> <span class=\"token string\">\"124M\"</span>\ngpt2<span class=\"token punctuation\">.</span>download_gpt2<span class=\"token punctuation\">(</span>model_name<span class=\"token operator\">=</span>model_name<span class=\"token punctuation\">)</span> \n\n<span class=\"token comment\"># provide file for fine-tuning</span>\nfile_name <span class=\"token operator\">=</span> <span class=\"token string\">\"shakespeare.txt\"</span>\n    \n<span class=\"token comment\"># start fine-tuning tensorflow session</span>\nsess <span class=\"token operator\">=</span> gpt2<span class=\"token punctuation\">.</span>start_tf_sess<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ngpt2<span class=\"token punctuation\">.</span>finetune<span class=\"token punctuation\">(</span>sess<span class=\"token punctuation\">,</span> file_name<span class=\"token punctuation\">,</span> model_name<span class=\"token operator\">=</span>model_name<span class=\"token punctuation\">,</span> steps<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># generate text</span>\ngpt2<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>sess<span class=\"token punctuation\">)</span></code></pre></div>\n<p>I trained a fine-tuned GPT-2 model on a <a href=\"https://raw.githubusercontent.com/kirubarajan/phraseviz/master/corpus.txt\">corpus of Barack Obama tweets I put together</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">We have a clear goal: Ending the use of force in Afghanistan as quickly \nas possible. That means giving Congress more time to figure out how to \nmake that happen. And doing so is the single most effective way forward.\n\nThe Afghan people deserve better. They and I are foot soldiers for them. \nWe&#39;re going to use all our might to get that goal accomplished. \nBut America is not going to give ourselves up for expedience&#39;s sake.</code></pre></div>\n<p>Wow! Thanks, Obama for the big policy change!</p>\n<h3>Transfer Learning via Control Sequences</h3>\n<p>As per the CTRL language model paper by <a href=\"https://arxiv.org/pdf/1909.05858.pdf\">Keskar et al.</a>, I also experimented with prepending a \"control sequence\" <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> that would allow the fine-tuning process to control downstream generation. In particular, I used the Hugging Face Transformers library to fine-tune the pre-trained 355M GPT-2 with Venture Captial tweets (in typical fashion). As a control sequence, I set <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> to be the user name of generation (e.g. <code class=\"language-text\">@paulg</code>) and was able to reduce perplexity by over 20%. This creates an implicit Transfer Learning effect within the fine-tuning process.</p>\n<p>Some example generations turned out being very promising:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\\paulg- No such things as moral judgment to deal in a way that matters; I wish this could just happen for the good of the community rather than being a thing that&#39;s only achieved in theory. The world of the future...we could do better: \n\n\\paulg- You can see a bit of a pattern here; the more you work on a project the more things you learn about it. The same is true in life as in business, and the less we do, and the faster we work with… \n\n\\paulg- &quot;A great start in an exciting field? Make sure it&#39;s a good one. If not, you could be running a startup in the dark.&quot; - @joshk- </code></pre></div>\n<p>My favourite is <code class=\"language-text\">\\paulg- It takes a village to raise an army.</code>.</p>\n<h3>Generative Adversarial Networks</h3>\n<p>Following our discussion on Generative Adversarial Networks in ESE 546, I've also been playing with Style Transfer GANs by exploring abstract artwork from contemporary artists. These adversarial paradigms prove to be very promising in generation tasks, and can be a lot less noisy than Variational Autoencoders. An interesting area of research is applying these adversarial paradigms to discrete domains, such as natural language. </p>\n<p><img src=\"https://i.imgur.com/gFAOIRM.png\" alt=\"image\">\n<small> (this is based off of pieces from Marc Chagall) </small></p>\n<p><img src=\"https://i.imgur.com/Rv7KRFC.png\" alt=\"image\"> <br />\n<small> (this is based off pieces from Jerret Lee) </small></p>\n<h3>Generation Algorithms for Language</h3>\n<p>Reading the paper <a href=\"https://arxiv.org/pdf/1904.09751.pdf\">The Curious Case of Neural Text Degeneration</a>\nprompted me to learn more about different sampling algorithms. In particular, these algorithms\nattempt to efficiently determine the most likely sequence of text given a probability distribution <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w_t ~|~ w_{t - 1}, w_{t - 2}, ...)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span>. This distribution is often the output of a softmax layer on\na neural network. Although the state-of-the-art language models are tremendously large models with hundreds of millions of parameters, I was able to experiment with different sampling algorithms\nusing a small LSTM language model.</p>\n<p>The model definition for my LSTM was rather simple:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">,</span> hidden_dim<span class=\"token punctuation\">,</span> vocab_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>Model<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>hidden_dim <span class=\"token operator\">=</span> hidden_dim\n        self<span class=\"token punctuation\">.</span>embedding <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>rnn <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>embedding_dim<span class=\"token punctuation\">,</span> hidden_dim<span class=\"token punctuation\">,</span> batch_first<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>output <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_dim<span class=\"token punctuation\">,</span> vocab_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> data<span class=\"token punctuation\">,</span> hidden<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        embedded <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>embedding<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n        prediction<span class=\"token punctuation\">,</span> hidden <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>rnn<span class=\"token punctuation\">(</span>embedded<span class=\"token punctuation\">,</span> hidden<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">(</span>prediction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> hidden\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">init_hidden</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> BATCH_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> BATCH_SIZE<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>hidden_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> BATCH_SIZE<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>hidden_dim<span class=\"token punctuation\">)</span></code></pre></div>\n<p>The first and most intuitive generation strategy is known as <strong>Greedy Decoding</strong>, where we take the <strong>most probable word</strong> over a vocabulary <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> for a context <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> as the next word.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><munder><mo><mi mathvariant=\"normal\">arg max</mi><mo>⁡</mo></mo><mrow><mi>w</mi><mo>∈</mo><mi>V</mi></mrow></munder><mtext> </mtext><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><msub><mi>c</mi><mn>0</mn></msub><mtext> </mtext><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mtext> </mtext><msub><mi>c</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w_i = \\operatorname*{arg\\, max}_{w \\in V} ~ P(w_i ~ | ~ c_0 ~ ... ~ c_{i - 1})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.716141em;vertical-align:-0.966141em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.43055999999999994em;\"><span style=\"top:-2.161229em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\" style=\"margin-right:0.01389em;\">g</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">x</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.966141em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">∣</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mspace nobreak\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>However, this produced rather trite and non-sensical generations because language often has\na pertinent information horizon larger than a single time-step. An improvement is known as <strong>Top-<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> Sampling</strong>, which truncates the probability distribution\nto the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> most likely tokens in the vocabulary.</p>\n<p><em>\"I am proposing with an advantage over commerce budget. — (applause) the middle of commerce, way together more of each other people’s it. In the chance that the international issue, freedom we have never has allowed the other way, or share from footing or denied coverage for the work of Democrats and Republican administrations isn’t (Applause.) Now, none of this can happen unless we’re their own rules that progress on so tied long still blind you should make Wall good example. (Applause.) For unemployment to pull all we should leave just like us — (applause)\"</em>.</p>\n<p>Introducing non-deterministic generation seems to be the key to creative and engaging generations.</p>\n<h2>Conclusion</h2>\n<p>In conclusion, this independent study has shown me that even though there is a lot of hype and effort in deep learning, there is still a lot of research left. Although companies and organizations are\ndeveloping larger and more expensive models, many core concepts are still researchable using smaller and more managable models. Chasing the state-of-the-art results is often a challenging and sometimes unrewarding endeavour, whereas devising new paradigms of machine learning training (like paralellizing training over cloud instances) and interesting inference strategies (e.g. Top-<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> sampling) can make comparable results while still asking promising research questions about machine intelligence. Overall,\nI am excited to continue research in machine learning, as well as different topics in linguistics and probabilty.</p>","frontmatter":{"title":"Independent Study on *Modern* Deep Learning","date":"2020-05-16","tags":["machine learning","research"]}}},"pageContext":{"pathSlug":"/blog/study"}}}