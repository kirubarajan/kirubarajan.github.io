{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/study","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# Independent Study on *Modern* Deep Learning\n> Misc. Machine Learning Methodologies\n\nThis page serves as both research notes and a workspace for my independent study on Multitask Learning and other (meta) machine learning methodologies, advised by Pratik Chaudhari at the University of Pennsylvania. I'll be adding interesting code snippets/results here as well as a brief literature review of various research papers. \n\nDisclaimer: the \"Modern Deep Learning* name is meant to be a joke, since the focus on this independent study is on cool/interesting trends in deep learning research.\n\n\n## ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\n[*Rajbhandari et al. 2020*](https://arxiv.org/pdf/1910.02054.pdf)\n\nThis paper introduces a novel optimizer named the Zero Redundency Optimizer (ZeRO), which aims to make it feasible and efficient to train previously impossible to train model architectures whose training exhibits memory limitations. This is done by partitioning model states as opposed to standard model state replication across clusters. Memory analysis shows that the optimizer can train a **one trillion** parameter model on 1024 GPUs with data parallelism degree $N_d = 1024$.\n\nThanks, Microsoft!\n\n## Exploring Randomly Wired Neural Networks for Image Recognition\n*Xie et al. (2019)*\n\nThis paper explores different neural network architectures by generating random neural network wirings. This is done by defining a stochastic network generator to encapsulate Neural Architecture Search, and later using classical random graph algorithms for wiring the networks. The authors show that the generated networks have competitve performance on the ImageNet task.\n\n### Network Generator\nNetwork Generators define a family of possible wiring patterns. Network architectures can thereby be sampled according to a probability distribution, which is differentiably learnable.\n\nFormally, the generator is a mapping $g: \\Theta \\rightarrow N$ where $\\Theta$ is the parameter space and $N$ is the space of neural network architectures. As such, $g$ determines how the computational graph representing the neural network is wired. The given parameters $\\theta \\in \\Theta$ specifies meta-information about the network such as the number of layers, activation types, etc. The output of $g$ is symbolic, so it doesn't return the weights of the networks (which can be learned from standard differentiable training processes) but instead a representation of the network (e.g. flow of data and types of operations).\n\nThe network generator $g$ can be extended to include an additional argument $s$, which acts as a stochastic seed. Then, the generator $g(s, \\theta)$ can be repeatedly called to generate a pseudo-random family of architectures.\n\n### Graphs to Neural Networks\nThe neural network generator generates a *general graph*, which is a set of nodes followed by a set of edges that connect the nodes. This general representation does not specify how the graph corresponds to a neural network, which is a later post-processing step. The non-restrictivness of the general graph allows the use of classical graph generation techniques from graph theory. In particular, the authors experiment with Erdos-Renyi (ER), Barabasi-Albert (BA), and Watts-Strogatz (WS) models of graph generation.\n\nThe generated edges are defined to be data flow (i.e. sending a tensor of data from one node to another) and that nodes define operations of either:\n1. **Aggregation** (e.g. weighted sum)\n2. **Transformation** (e.g. non-linearity)\n3. **Distribution** (e.g. copying data)\n\n### Experiments\nFor each generator, the authors sample 5 instances (generated by 5 random seeds), and train them from scratch. Networks are trained for roughly 100 epochs, using a half-period-cosine learning rate decay from an initial learning rate of 0.1 with a momentum of 0.9.\n\nThe authors note that every random generator yields decent accuracy. Furthermore, the variation among the random network instances is rather low with a standard deviation in the range of 0.2% to 0.4%.\n\n## Compositional Attention Networks for Machine Reasoning\n[Hudson et. al, 2018](https://arxiv.org/abs/1803.03067)\n\nThe authors design a novel fully differentiable neural network architecture that is capable of explicit and expressive reasoning. One primary goal of the paper is interpretability, without sacrificing the predictive performance of black box methods. Problems are decomposed into attention-based steps, and are solved using Memory, Attention, and Composition (MAC) sub-units. On the CLEVR dataset for visual reasoning, the model accomplishes a state-of-the-art 98.9% accuracy, using less data than competing models.\n\n## Fine-tuning Transformer Models\nEasy usage can be done through the GPT-2 Simple package by Max Woolf (https://github.com/minimaxir/gpt-2-simple).\n\nInstall using `pip3 install gpt-2-simple` and provide text for fine-tuning:\n\n```python\nimport gpt_2_simple as gpt2\nimport os\nimport requests\n\n# download pre-trained GPT-2 model with 124M parameters\nmodel_name = \"124M\"\ngpt2.download_gpt2(model_name=model_name) \n\n# provide file for fine-tuning\nfile_name = \"shakespeare.txt\"\n    \n# start fine-tuning tensorflow session\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess, file_name, model_name=model_name, steps=1000)\n\n# generate text\ngpt2.generate(sess)\n```\n\nI trained a fine-tuned GPT-2 model on a [corpus of Barack Obama tweets I put together](https://raw.githubusercontent.com/kirubarajan/phraseviz/master/corpus.txt).\n\n```\nWe have a clear goal: Ending the use of force in Afghanistan as quickly \nas possible. That means giving Congress more time to figure out how to \nmake that happen. And doing so is the single most effective way forward.\n\nThe Afghan people deserve better. They and I are foot soldiers for them. \nWe're going to use all our might to get that goal accomplished. \nBut America is not going to give ourselves up for expedience's sake.\n```\n\nWow! Thanks, Obama for the big policy change!\n\n## Generative Adversarial Networks\n\nLately, I've also been playing with Style Transfer GANs by exploring abstract artwork from contemporary artists:\n \n![image](https://i.imgur.com/gFAOIRM.png) \n<small> (this is based off of pieces from Marc Chagall) </small>\n\n![image](https://i.imgur.com/Rv7KRFC.png) <br />\n<small> (this is based off pieces from Jerret Lee) </small>\n\n## Multitask Learning\n\nGeneral background information will go here!","html":"<h1>Independent Study on <em>Modern</em> Deep Learning</h1>\n<blockquote>\n<p>Misc. Machine Learning Methodologies</p>\n</blockquote>\n<p>This page serves as both research notes and a workspace for my independent study on Multitask Learning and other (meta) machine learning methodologies, advised by Pratik Chaudhari at the University of Pennsylvania. I'll be adding interesting code snippets/results here as well as a brief literature review of various research papers. </p>\n<p>Disclaimer: the \"Modern Deep Learning* name is meant to be a joke, since the focus on this independent study is on cool/interesting trends in deep learning research.</p>\n<h2>ZeRO: Memory Optimization Towards Training A Trillion Parameter Models</h2>\n<p><a href=\"https://arxiv.org/pdf/1910.02054.pdf\"><em>Rajbhandari et al. 2020</em></a></p>\n<p>This paper introduces a novel optimizer named the Zero Redundency Optimizer (ZeRO), which aims to make it feasible and efficient to train previously impossible to train model architectures whose training exhibits memory limitations. This is done by partitioning model states as opposed to standard model state replication across clusters. Memory analysis shows that the optimizer can train a <strong>one trillion</strong> parameter model on 1024 GPUs with data parallelism degree <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>N</mi><mi>d</mi></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_d = 1024</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">d</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">2</span><span class=\"mord\">4</span></span></span></span>.</p>\n<p>Thanks, Microsoft!</p>\n<h2>Exploring Randomly Wired Neural Networks for Image Recognition</h2>\n<p><em>Xie et al. (2019)</em></p>\n<p>This paper explores different neural network architectures by generating random neural network wirings. This is done by defining a stochastic network generator to encapsulate Neural Architecture Search, and later using classical random graph algorithms for wiring the networks. The authors show that the generated networks have competitve performance on the ImageNet task.</p>\n<h3>Network Generator</h3>\n<p>Network Generators define a family of possible wiring patterns. Network architectures can thereby be sampled according to a probability distribution, which is differentiably learnable.</p>\n<p>Formally, the generator is a mapping <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo>:</mo><mi mathvariant=\"normal\">Θ</mi><mo>→</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">g: \\Theta \\rightarrow N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Θ</mi></mrow><annotation encoding=\"application/x-tex\">\\Theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Θ</span></span></span></span> is the parameter space and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> is the space of neural network architectures. As such, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> determines how the computational graph representing the neural network is wired. The given parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>∈</mo><mi mathvariant=\"normal\">Θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta \\in \\Theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Θ</span></span></span></span> specifies meta-information about the network such as the number of layers, activation types, etc. The output of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> is symbolic, so it doesn't return the weights of the networks (which can be learned from standard differentiable training processes) but instead a representation of the network (e.g. flow of data and types of operations).</p>\n<p>The network generator <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> can be extended to include an additional argument <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span>, which acts as a stochastic seed. Then, the generator <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">g(s, \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> can be repeatedly called to generate a pseudo-random family of architectures.</p>\n<h3>Graphs to Neural Networks</h3>\n<p>The neural network generator generates a <em>general graph</em>, which is a set of nodes followed by a set of edges that connect the nodes. This general representation does not specify how the graph corresponds to a neural network, which is a later post-processing step. The non-restrictivness of the general graph allows the use of classical graph generation techniques from graph theory. In particular, the authors experiment with Erdos-Renyi (ER), Barabasi-Albert (BA), and Watts-Strogatz (WS) models of graph generation.</p>\n<p>The generated edges are defined to be data flow (i.e. sending a tensor of data from one node to another) and that nodes define operations of either:</p>\n<ol>\n<li><strong>Aggregation</strong> (e.g. weighted sum)</li>\n<li><strong>Transformation</strong> (e.g. non-linearity)</li>\n<li><strong>Distribution</strong> (e.g. copying data)</li>\n</ol>\n<h3>Experiments</h3>\n<p>For each generator, the authors sample 5 instances (generated by 5 random seeds), and train them from scratch. Networks are trained for roughly 100 epochs, using a half-period-cosine learning rate decay from an initial learning rate of 0.1 with a momentum of 0.9.</p>\n<p>The authors note that every random generator yields decent accuracy. Furthermore, the variation among the random network instances is rather low with a standard deviation in the range of 0.2% to 0.4%.</p>\n<h2>Compositional Attention Networks for Machine Reasoning</h2>\n<p><a href=\"https://arxiv.org/abs/1803.03067\">Hudson et. al, 2018</a></p>\n<p>The authors design a novel fully differentiable neural network architecture that is capable of explicit and expressive reasoning. One primary goal of the paper is interpretability, without sacrificing the predictive performance of black box methods. Problems are decomposed into attention-based steps, and are solved using Memory, Attention, and Composition (MAC) sub-units. On the CLEVR dataset for visual reasoning, the model accomplishes a state-of-the-art 98.9% accuracy, using less data than competing models.</p>\n<h2>Fine-tuning Transformer Models</h2>\n<p>Easy usage can be done through the GPT-2 Simple package by Max Woolf (<a href=\"https://github.com/minimaxir/gpt-2-simple\">https://github.com/minimaxir/gpt-2-simple</a>).</p>\n<p>Install using <code class=\"language-text\">pip3 install gpt-2-simple</code> and provide text for fine-tuning:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> gpt_2_simple <span class=\"token keyword\">as</span> gpt2\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> requests\n\n<span class=\"token comment\"># download pre-trained GPT-2 model with 124M parameters</span>\nmodel_name <span class=\"token operator\">=</span> <span class=\"token string\">\"124M\"</span>\ngpt2<span class=\"token punctuation\">.</span>download_gpt2<span class=\"token punctuation\">(</span>model_name<span class=\"token operator\">=</span>model_name<span class=\"token punctuation\">)</span> \n\n<span class=\"token comment\"># provide file for fine-tuning</span>\nfile_name <span class=\"token operator\">=</span> <span class=\"token string\">\"shakespeare.txt\"</span>\n    \n<span class=\"token comment\"># start fine-tuning tensorflow session</span>\nsess <span class=\"token operator\">=</span> gpt2<span class=\"token punctuation\">.</span>start_tf_sess<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ngpt2<span class=\"token punctuation\">.</span>finetune<span class=\"token punctuation\">(</span>sess<span class=\"token punctuation\">,</span> file_name<span class=\"token punctuation\">,</span> model_name<span class=\"token operator\">=</span>model_name<span class=\"token punctuation\">,</span> steps<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># generate text</span>\ngpt2<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>sess<span class=\"token punctuation\">)</span></code></pre></div>\n<p>I trained a fine-tuned GPT-2 model on a <a href=\"https://raw.githubusercontent.com/kirubarajan/phraseviz/master/corpus.txt\">corpus of Barack Obama tweets I put together</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">We have a clear goal: Ending the use of force in Afghanistan as quickly \nas possible. That means giving Congress more time to figure out how to \nmake that happen. And doing so is the single most effective way forward.\n\nThe Afghan people deserve better. They and I are foot soldiers for them. \nWe&#39;re going to use all our might to get that goal accomplished. \nBut America is not going to give ourselves up for expedience&#39;s sake.</code></pre></div>\n<p>Wow! Thanks, Obama for the big policy change!</p>\n<h2>Generative Adversarial Networks</h2>\n<p>Lately, I've also been playing with Style Transfer GANs by exploring abstract artwork from contemporary artists:</p>\n<p><img src=\"https://i.imgur.com/gFAOIRM.png\" alt=\"image\">\n<small> (this is based off of pieces from Marc Chagall) </small></p>\n<p><img src=\"https://i.imgur.com/Rv7KRFC.png\" alt=\"image\"> <br />\n<small> (this is based off pieces from Jerret Lee) </small></p>\n<h2>Multitask Learning</h2>\n<p>General background information will go here!</p>","frontmatter":{"title":"Independent Study on *Modern* Deep Learning","date":"2020-02-22","tags":["machine learning","research"]}}},"pageContext":{"pathSlug":"/blog/study"}}}