{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/decoding","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# Decoding Strategies for Text Generation\n> Finding the Humanity in Approximating an NP-Hard Problem\n\n## Introduction\nRecently, machine learning models have seen incredible progress towards computers being able to generate text that sounds human. This is an area of research that involves both furthering our understanding of machine intelligence as well as language usage. I think it’s an interesting problem since it once against prompts the long-lasting question of “what does it mean to be human”.\n\nSo how do we make computers sound human? In particular, I want to take a look at the different ways we can do this, if we have a trained machine learning model known as a **language model**. In its simplest sense, a language model assigns a probability to a sequence of words. As you can imagine, language is infinite and so we can’t possibly know the probability of the phrase “cat in the hat”. However, our machine learning model by definition is going to **approximate** this for us. And, it turns out that this approximation works incredibly well for practical purposes.\n\nThe language model’s goal is give us always be able to give us $P(w|c)$, where $w$ is a particular target word (i.e. the next word) and $c$ is the context that precede the target word. \n\n## Proving NP-Hardness\nBefore moving on to approaching a solution, it’s worth gaining a little appreciation for how **difficult** this problem truly is. To solve it in its entirety, you would make a million dollars! Literally! The challenges we have with using our machine model to generate text is yet another manifestation of the NP-Complete class of problems (in it’s decision form). If you are unfamiliar, these problems are known to be the toughest problems in computer science. What’s more interesting, is that problems existing in this class can all be **reduced** to one another, implying that they are in essence the same problem.\n\nLet’s show that our issue of text generation is *just as hard* as the other famous NP-hard problems, like the Traveling Salesman Problem and Knapsack Problem. \n\n## Greedy Decoding\n\n## Beam Search\n\n## Random Sampling\n\n## Distribution Changes\n\n### Temperature\n\n### Top-K Sampling\n\n### Top-P Sampling\n\n## Conclusion","html":"<h1>Decoding Strategies for Text Generation</h1>\n<blockquote>\n<p>Finding the Humanity in Approximating an NP-Hard Problem</p>\n</blockquote>\n<h2>Introduction</h2>\n<p>Recently, machine learning models have seen incredible progress towards computers being able to generate text that sounds human. This is an area of research that involves both furthering our understanding of machine intelligence as well as language usage. I think it’s an interesting problem since it once against prompts the long-lasting question of “what does it mean to be human”.</p>\n<p>So how do we make computers sound human? In particular, I want to take a look at the different ways we can do this, if we have a trained machine learning model known as a <strong>language model</strong>. In its simplest sense, a language model assigns a probability to a sequence of words. As you can imagine, language is infinite and so we can’t possibly know the probability of the phrase “cat in the hat”. However, our machine learning model by definition is going to <strong>approximate</strong> this for us. And, it turns out that this approximation works incredibly well for practical purposes.</p>\n<p>The language model’s goal is give us always be able to give us <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mi mathvariant=\"normal\">∣</mi><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w|c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">c</span><span class=\"mclose\">)</span></span></span></span>, where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> is a particular target word (i.e. the next word) and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> is the context that precede the target word. </p>\n<h2>Proving NP-Hardness</h2>\n<p>Before moving on to approaching a solution, it’s worth gaining a little appreciation for how <strong>difficult</strong> this problem truly is. To solve it in its entirety, you would make a million dollars! Literally! The challenges we have with using our machine model to generate text is yet another manifestation of the NP-Complete class of problems (in it’s decision form). If you are unfamiliar, these problems are known to be the toughest problems in computer science. What’s more interesting, is that problems existing in this class can all be <strong>reduced</strong> to one another, implying that they are in essence the same problem.</p>\n<p>Let’s show that our issue of text generation is <em>just as hard</em> as the other famous NP-hard problems, like the Traveling Salesman Problem and Knapsack Problem. </p>\n<h2>Greedy Decoding</h2>\n<h2>Beam Search</h2>\n<h2>Random Sampling</h2>\n<h2>Distribution Changes</h2>\n<h3>Temperature</h3>\n<h3>Top-K Sampling</h3>\n<h3>Top-P Sampling</h3>\n<h2>Conclusion</h2>","frontmatter":{"title":"Decoding Strategies for Text Generation","date":"2020-04-17","tags":["machine learning"]}}},"pageContext":{"pathSlug":"/blog/decoding"}}}